{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Entrainement du modèle d'identification du grade de la route"
      ],
      "metadata": {
        "id": "IIDBcfi_2rTh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rPrdH6kZ2mKA"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Chargement du dataframe contenant les données d'entrainement"
      ],
      "metadata": {
        "id": "Alqnp-S246Dh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Téléchargement et extraction des fichiers\n",
        "\n",
        "!wget -O DataFrameGlobal.zip -q http://62.210.208.36/DataFrameGlobal.zip\n",
        "!unzip /content/DataFrameGlobal.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gzbhxcim3jvx",
        "outputId": "079ef2a8-32cf-468f-e24c-35664fc64584"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/DataFrameGlobal.zip\n",
            "  inflating: DataFrameGlobal.csv     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Création du DataFrame\n",
        "df = pd.read_csv(\"/content/DataFrameGlobal.csv\")"
      ],
      "metadata": {
        "id": "7vd5zc2i4_Tx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "JukmOLNc5Fmw",
        "outputId": "7b59e331-787c-4e24-be07-5b262c4f48ef"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Temps (s)  H_v3.3_p2.0  B_v4.6_p2.0  C_v5.3_p1.5  E_v4.9_p1.9  \\\n",
              "0         0.01     9.817928     9.810112     9.810359     9.811507   \n",
              "1         0.02     9.817914     9.810112     9.810358     9.811503   \n",
              "2         0.03     9.817903     9.810112     9.810357     9.811500   \n",
              "3         0.04     9.817886     9.810111     9.810356     9.811495   \n",
              "4         0.05     9.817870     9.810111     9.810355     9.811491   \n",
              "..         ...          ...          ...          ...          ...   \n",
              "995       9.96     9.880820     9.810467     9.810553     9.813605   \n",
              "996       9.97    21.818193     9.810466     9.810552     9.813594   \n",
              "997       9.98     9.783262     9.810465     9.810550     9.813583   \n",
              "998       9.99     9.783406     9.810463     9.810548     9.813572   \n",
              "999      10.00     9.783435     9.812452     9.396402    14.097687   \n",
              "\n",
              "     F_v5.0_p1.6  D_v4.7_p1.9  H_v1.7_p2.3  G_v3.6_p2.3  A_v3.6_p2.2  ...  \\\n",
              "0       9.812488     9.810424     9.812690     9.813991     9.810062  ...   \n",
              "1       9.812481     9.810423     9.812688     9.813983     9.810062  ...   \n",
              "2       9.812476     9.810422     9.812686     9.813977     9.810062  ...   \n",
              "3       9.812468     9.810421     9.812683     9.813968     9.810062  ...   \n",
              "4       9.812461     9.810420     9.812680     9.813959     9.810062  ...   \n",
              "..           ...          ...          ...          ...          ...  ...   \n",
              "995     9.813951     9.806159     9.795998     9.828026     9.810280  ...   \n",
              "996     9.813940     9.806170     9.796006    29.916127    10.122634  ...   \n",
              "997     9.813927     9.806180     9.796022     9.813416     9.809976  ...   \n",
              "998     9.813915     9.806192     9.796037     9.782474     9.809572  ...   \n",
              "999     9.170562     9.544795     9.796053     9.782545     9.809573  ...   \n",
              "\n",
              "     H_v4.8_p2.4  C_v1.9_p2.4  B_v5.0_p2.2  C_v4.2_p2.3  B_v1.9_p2.2  \\\n",
              "0       9.825546     9.810066     9.810155     9.810306     9.810040   \n",
              "1       9.825504     9.810066     9.810155     9.810305     9.810040   \n",
              "2       9.825475     9.810066     9.810154     9.810304     9.810040   \n",
              "3       9.825428     9.810066     9.810154     9.810304     9.810040   \n",
              "4       9.825381     9.810066     9.810153     9.810303     9.810040   \n",
              "..           ...          ...          ...          ...          ...   \n",
              "995     9.713050     9.810871     9.810246     9.810015     9.810534   \n",
              "996     9.713354     9.810354     9.810246     9.810015     9.810217   \n",
              "997     9.713613     9.810342     9.810245     9.810015     9.810210   \n",
              "998     9.713924     9.810339     9.810244     9.815183     9.810208   \n",
              "999    30.825089     9.810339     9.770152     9.809806     9.810208   \n",
              "\n",
              "     B_v3.4_p1.5  H_v5.5_p1.5  A_v3.5_p2.1  H_v2.3_p1.6  D_v3.9_p1.6  \n",
              "0       9.810088     9.822273     9.810050     9.813930     9.810286  \n",
              "1       9.810088     9.822236     9.810050     9.813925     9.810286  \n",
              "2       9.810088     9.822207     9.810049     9.813921     9.810285  \n",
              "3       9.810088     9.822165     9.810049     9.813915     9.810285  \n",
              "4       9.810087     9.822123     9.810049     9.813910     9.810284  \n",
              "..           ...          ...          ...          ...          ...  \n",
              "995     9.765802     9.832789     9.809665     9.876172     9.811255  \n",
              "996     9.591306     9.832711     9.604913     9.876083     9.811252  \n",
              "997     9.810794     9.832632     9.810097     9.876026     9.451535  \n",
              "998     9.810795     9.832554     9.810116     9.875933     9.811735  \n",
              "999     9.810794     9.832476     9.810116     9.875829     9.811745  \n",
              "\n",
              "[1000 rows x 695 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ad6c2e67-ca84-4077-a49a-b47fd67cc09f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Temps (s)</th>\n",
              "      <th>H_v3.3_p2.0</th>\n",
              "      <th>B_v4.6_p2.0</th>\n",
              "      <th>C_v5.3_p1.5</th>\n",
              "      <th>E_v4.9_p1.9</th>\n",
              "      <th>F_v5.0_p1.6</th>\n",
              "      <th>D_v4.7_p1.9</th>\n",
              "      <th>H_v1.7_p2.3</th>\n",
              "      <th>G_v3.6_p2.3</th>\n",
              "      <th>A_v3.6_p2.2</th>\n",
              "      <th>...</th>\n",
              "      <th>H_v4.8_p2.4</th>\n",
              "      <th>C_v1.9_p2.4</th>\n",
              "      <th>B_v5.0_p2.2</th>\n",
              "      <th>C_v4.2_p2.3</th>\n",
              "      <th>B_v1.9_p2.2</th>\n",
              "      <th>B_v3.4_p1.5</th>\n",
              "      <th>H_v5.5_p1.5</th>\n",
              "      <th>A_v3.5_p2.1</th>\n",
              "      <th>H_v2.3_p1.6</th>\n",
              "      <th>D_v3.9_p1.6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.01</td>\n",
              "      <td>9.817928</td>\n",
              "      <td>9.810112</td>\n",
              "      <td>9.810359</td>\n",
              "      <td>9.811507</td>\n",
              "      <td>9.812488</td>\n",
              "      <td>9.810424</td>\n",
              "      <td>9.812690</td>\n",
              "      <td>9.813991</td>\n",
              "      <td>9.810062</td>\n",
              "      <td>...</td>\n",
              "      <td>9.825546</td>\n",
              "      <td>9.810066</td>\n",
              "      <td>9.810155</td>\n",
              "      <td>9.810306</td>\n",
              "      <td>9.810040</td>\n",
              "      <td>9.810088</td>\n",
              "      <td>9.822273</td>\n",
              "      <td>9.810050</td>\n",
              "      <td>9.813930</td>\n",
              "      <td>9.810286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.02</td>\n",
              "      <td>9.817914</td>\n",
              "      <td>9.810112</td>\n",
              "      <td>9.810358</td>\n",
              "      <td>9.811503</td>\n",
              "      <td>9.812481</td>\n",
              "      <td>9.810423</td>\n",
              "      <td>9.812688</td>\n",
              "      <td>9.813983</td>\n",
              "      <td>9.810062</td>\n",
              "      <td>...</td>\n",
              "      <td>9.825504</td>\n",
              "      <td>9.810066</td>\n",
              "      <td>9.810155</td>\n",
              "      <td>9.810305</td>\n",
              "      <td>9.810040</td>\n",
              "      <td>9.810088</td>\n",
              "      <td>9.822236</td>\n",
              "      <td>9.810050</td>\n",
              "      <td>9.813925</td>\n",
              "      <td>9.810286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.03</td>\n",
              "      <td>9.817903</td>\n",
              "      <td>9.810112</td>\n",
              "      <td>9.810357</td>\n",
              "      <td>9.811500</td>\n",
              "      <td>9.812476</td>\n",
              "      <td>9.810422</td>\n",
              "      <td>9.812686</td>\n",
              "      <td>9.813977</td>\n",
              "      <td>9.810062</td>\n",
              "      <td>...</td>\n",
              "      <td>9.825475</td>\n",
              "      <td>9.810066</td>\n",
              "      <td>9.810154</td>\n",
              "      <td>9.810304</td>\n",
              "      <td>9.810040</td>\n",
              "      <td>9.810088</td>\n",
              "      <td>9.822207</td>\n",
              "      <td>9.810049</td>\n",
              "      <td>9.813921</td>\n",
              "      <td>9.810285</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.04</td>\n",
              "      <td>9.817886</td>\n",
              "      <td>9.810111</td>\n",
              "      <td>9.810356</td>\n",
              "      <td>9.811495</td>\n",
              "      <td>9.812468</td>\n",
              "      <td>9.810421</td>\n",
              "      <td>9.812683</td>\n",
              "      <td>9.813968</td>\n",
              "      <td>9.810062</td>\n",
              "      <td>...</td>\n",
              "      <td>9.825428</td>\n",
              "      <td>9.810066</td>\n",
              "      <td>9.810154</td>\n",
              "      <td>9.810304</td>\n",
              "      <td>9.810040</td>\n",
              "      <td>9.810088</td>\n",
              "      <td>9.822165</td>\n",
              "      <td>9.810049</td>\n",
              "      <td>9.813915</td>\n",
              "      <td>9.810285</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.05</td>\n",
              "      <td>9.817870</td>\n",
              "      <td>9.810111</td>\n",
              "      <td>9.810355</td>\n",
              "      <td>9.811491</td>\n",
              "      <td>9.812461</td>\n",
              "      <td>9.810420</td>\n",
              "      <td>9.812680</td>\n",
              "      <td>9.813959</td>\n",
              "      <td>9.810062</td>\n",
              "      <td>...</td>\n",
              "      <td>9.825381</td>\n",
              "      <td>9.810066</td>\n",
              "      <td>9.810153</td>\n",
              "      <td>9.810303</td>\n",
              "      <td>9.810040</td>\n",
              "      <td>9.810087</td>\n",
              "      <td>9.822123</td>\n",
              "      <td>9.810049</td>\n",
              "      <td>9.813910</td>\n",
              "      <td>9.810284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>9.96</td>\n",
              "      <td>9.880820</td>\n",
              "      <td>9.810467</td>\n",
              "      <td>9.810553</td>\n",
              "      <td>9.813605</td>\n",
              "      <td>9.813951</td>\n",
              "      <td>9.806159</td>\n",
              "      <td>9.795998</td>\n",
              "      <td>9.828026</td>\n",
              "      <td>9.810280</td>\n",
              "      <td>...</td>\n",
              "      <td>9.713050</td>\n",
              "      <td>9.810871</td>\n",
              "      <td>9.810246</td>\n",
              "      <td>9.810015</td>\n",
              "      <td>9.810534</td>\n",
              "      <td>9.765802</td>\n",
              "      <td>9.832789</td>\n",
              "      <td>9.809665</td>\n",
              "      <td>9.876172</td>\n",
              "      <td>9.811255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>9.97</td>\n",
              "      <td>21.818193</td>\n",
              "      <td>9.810466</td>\n",
              "      <td>9.810552</td>\n",
              "      <td>9.813594</td>\n",
              "      <td>9.813940</td>\n",
              "      <td>9.806170</td>\n",
              "      <td>9.796006</td>\n",
              "      <td>29.916127</td>\n",
              "      <td>10.122634</td>\n",
              "      <td>...</td>\n",
              "      <td>9.713354</td>\n",
              "      <td>9.810354</td>\n",
              "      <td>9.810246</td>\n",
              "      <td>9.810015</td>\n",
              "      <td>9.810217</td>\n",
              "      <td>9.591306</td>\n",
              "      <td>9.832711</td>\n",
              "      <td>9.604913</td>\n",
              "      <td>9.876083</td>\n",
              "      <td>9.811252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>9.98</td>\n",
              "      <td>9.783262</td>\n",
              "      <td>9.810465</td>\n",
              "      <td>9.810550</td>\n",
              "      <td>9.813583</td>\n",
              "      <td>9.813927</td>\n",
              "      <td>9.806180</td>\n",
              "      <td>9.796022</td>\n",
              "      <td>9.813416</td>\n",
              "      <td>9.809976</td>\n",
              "      <td>...</td>\n",
              "      <td>9.713613</td>\n",
              "      <td>9.810342</td>\n",
              "      <td>9.810245</td>\n",
              "      <td>9.810015</td>\n",
              "      <td>9.810210</td>\n",
              "      <td>9.810794</td>\n",
              "      <td>9.832632</td>\n",
              "      <td>9.810097</td>\n",
              "      <td>9.876026</td>\n",
              "      <td>9.451535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>9.99</td>\n",
              "      <td>9.783406</td>\n",
              "      <td>9.810463</td>\n",
              "      <td>9.810548</td>\n",
              "      <td>9.813572</td>\n",
              "      <td>9.813915</td>\n",
              "      <td>9.806192</td>\n",
              "      <td>9.796037</td>\n",
              "      <td>9.782474</td>\n",
              "      <td>9.809572</td>\n",
              "      <td>...</td>\n",
              "      <td>9.713924</td>\n",
              "      <td>9.810339</td>\n",
              "      <td>9.810244</td>\n",
              "      <td>9.815183</td>\n",
              "      <td>9.810208</td>\n",
              "      <td>9.810795</td>\n",
              "      <td>9.832554</td>\n",
              "      <td>9.810116</td>\n",
              "      <td>9.875933</td>\n",
              "      <td>9.811735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>10.00</td>\n",
              "      <td>9.783435</td>\n",
              "      <td>9.812452</td>\n",
              "      <td>9.396402</td>\n",
              "      <td>14.097687</td>\n",
              "      <td>9.170562</td>\n",
              "      <td>9.544795</td>\n",
              "      <td>9.796053</td>\n",
              "      <td>9.782545</td>\n",
              "      <td>9.809573</td>\n",
              "      <td>...</td>\n",
              "      <td>30.825089</td>\n",
              "      <td>9.810339</td>\n",
              "      <td>9.770152</td>\n",
              "      <td>9.809806</td>\n",
              "      <td>9.810208</td>\n",
              "      <td>9.810794</td>\n",
              "      <td>9.832476</td>\n",
              "      <td>9.810116</td>\n",
              "      <td>9.875829</td>\n",
              "      <td>9.811745</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 695 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ad6c2e67-ca84-4077-a49a-b47fd67cc09f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ad6c2e67-ca84-4077-a49a-b47fd67cc09f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ad6c2e67-ca84-4077-a49a-b47fd67cc09f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Affichage de quelques séries :"
      ],
      "metadata": {
        "id": "-aH1FhDN_DRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(x=np.linspace(0,len(df),len(df)+1),y=df[df.columns[1]], line=dict(color='blue', width=1),name=df.columns[1]))\n",
        "fig.add_trace(go.Scatter(x=np.linspace(0,len(df),len(df)+1),y=df[df.columns[2]], line=dict(color='red', width=1),name=df.columns[2]))\n",
        "fig.add_trace(go.Scatter(x=np.linspace(0,len(df),len(df)+1),y=df[df.columns[3]], line=dict(color='black', width=1),name=df.columns[3]))\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "vvJUXKVQ_FtW",
        "outputId": "777c8c91-21a8-477d-8fc8-2e8deba745dd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.18.2.min.js\"></script>                <div id=\"c3dc15d2-a470-490f-be8a-fad01275096f\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"c3dc15d2-a470-490f-be8a-fad01275096f\")) {                    Plotly.newPlot(                        \"c3dc15d2-a470-490f-be8a-fad01275096f\",                        [{\"line\":{\"color\":\"blue\",\"width\":1},\"name\":\"H_v3.3_p2.0\",\"x\":[0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0,35.0,36.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,44.0,45.0,46.0,47.0,48.0,49.0,50.0,51.0,52.0,53.0,54.0,55.0,56.0,57.0,58.0,59.0,60.0,61.0,62.0,63.0,64.0,65.0,66.0,67.0,68.0,69.0,70.0,71.0,72.0,73.0,74.0,75.0,76.0,77.0,78.0,79.0,80.0,81.0,82.0,83.0,84.0,85.0,86.0,87.0,88.0,89.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0,100.0,101.0,102.0,103.0,104.0,105.0,106.0,107.0,108.0,109.0,110.0,111.0,112.0,113.0,114.0,115.0,116.0,117.0,118.0,119.0,120.0,121.0,122.0,123.0,124.0,125.0,126.0,127.0,128.0,129.0,130.0,131.0,132.0,133.0,134.0,135.0,136.0,137.0,138.0,139.0,140.0,141.0,142.0,143.0,144.0,145.0,146.0,147.0,148.0,149.0,150.0,151.0,152.0,153.0,154.0,155.0,156.0,157.0,158.0,159.0,160.0,161.0,162.0,163.0,164.0,165.0,166.0,167.0,168.0,169.0,170.0,171.0,172.0,173.0,174.0,175.0,176.0,177.0,178.0,179.0,180.0,181.0,182.0,183.0,184.0,185.0,186.0,187.0,188.0,189.0,190.0,191.0,192.0,193.0,194.0,195.0,196.0,197.0,198.0,199.0,200.0,201.0,202.0,203.0,204.0,205.0,206.0,207.0,208.0,209.0,210.0,211.0,212.0,213.0,214.0,215.0,216.0,217.0,218.0,219.0,220.0,221.0,222.0,223.0,224.0,225.0,226.0,227.0,228.0,229.0,230.0,231.0,232.0,233.0,234.0,235.0,236.0,237.0,238.0,239.0,240.0,241.0,242.0,243.0,244.0,245.0,246.0,247.0,248.0,249.0,250.0,251.0,252.0,253.0,254.0,255.0,256.0,257.0,258.0,259.0,260.0,261.0,262.0,263.0,264.0,265.0,266.0,267.0,268.0,269.0,270.0,271.0,272.0,273.0,274.0,275.0,276.0,277.0,278.0,279.0,280.0,281.0,282.0,283.0,284.0,285.0,286.0,287.0,288.0,289.0,290.0,291.0,292.0,293.0,294.0,295.0,296.0,297.0,298.0,299.0,300.0,301.0,302.0,303.0,304.0,305.0,306.0,307.0,308.0,309.0,310.0,311.0,312.0,313.0,314.0,315.0,316.0,317.0,318.0,319.0,320.0,321.0,322.0,323.0,324.0,325.0,326.0,327.0,328.0,329.0,330.0,331.0,332.0,333.0,334.0,335.0,336.0,337.0,338.0,339.0,340.0,341.0,342.0,343.0,344.0,345.0,346.0,347.0,348.0,349.0,350.0,351.0,352.0,353.0,354.0,355.0,356.0,357.0,358.0,359.0,360.0,361.0,362.0,363.0,364.0,365.0,366.0,367.0,368.0,369.0,370.0,371.0,372.0,373.0,374.0,375.0,376.0,377.0,378.0,379.0,380.0,381.0,382.0,383.0,384.0,385.0,386.0,387.0,388.0,389.0,390.0,391.0,392.0,393.0,394.0,395.0,396.0,397.0,398.0,399.0,400.0,401.0,402.0,403.0,404.0,405.0,406.0,407.0,408.0,409.0,410.0,411.0,412.0,413.0,414.0,415.0,416.0,417.0,418.0,419.0,420.0,421.0,422.0,423.0,424.0,425.0,426.0,427.0,428.0,429.0,430.0,431.0,432.0,433.0,434.0,435.0,436.0,437.0,438.0,439.0,440.0,441.0,442.0,443.0,444.0,445.0,446.0,447.0,448.0,449.0,450.0,451.0,452.0,453.0,454.0,455.0,456.0,457.0,458.0,459.0,460.0,461.0,462.0,463.0,464.0,465.0,466.0,467.0,468.0,469.0,470.0,471.0,472.0,473.0,474.0,475.0,476.0,477.0,478.0,479.0,480.0,481.0,482.0,483.0,484.0,485.0,486.0,487.0,488.0,489.0,490.0,491.0,492.0,493.0,494.0,495.0,496.0,497.0,498.0,499.0,500.0,501.0,502.0,503.0,504.0,505.0,506.0,507.0,508.0,509.0,510.0,511.0,512.0,513.0,514.0,515.0,516.0,517.0,518.0,519.0,520.0,521.0,522.0,523.0,524.0,525.0,526.0,527.0,528.0,529.0,530.0,531.0,532.0,533.0,534.0,535.0,536.0,537.0,538.0,539.0,540.0,541.0,542.0,543.0,544.0,545.0,546.0,547.0,548.0,549.0,550.0,551.0,552.0,553.0,554.0,555.0,556.0,557.0,558.0,559.0,560.0,561.0,562.0,563.0,564.0,565.0,566.0,567.0,568.0,569.0,570.0,571.0,572.0,573.0,574.0,575.0,576.0,577.0,578.0,579.0,580.0,581.0,582.0,583.0,584.0,585.0,586.0,587.0,588.0,589.0,590.0,591.0,592.0,593.0,594.0,595.0,596.0,597.0,598.0,599.0,600.0,601.0,602.0,603.0,604.0,605.0,606.0,607.0,608.0,609.0,610.0,611.0,612.0,613.0,614.0,615.0,616.0,617.0,618.0,619.0,620.0,621.0,622.0,623.0,624.0,625.0,626.0,627.0,628.0,629.0,630.0,631.0,632.0,633.0,634.0,635.0,636.0,637.0,638.0,639.0,640.0,641.0,642.0,643.0,644.0,645.0,646.0,647.0,648.0,649.0,650.0,651.0,652.0,653.0,654.0,655.0,656.0,657.0,658.0,659.0,660.0,661.0,662.0,663.0,664.0,665.0,666.0,667.0,668.0,669.0,670.0,671.0,672.0,673.0,674.0,675.0,676.0,677.0,678.0,679.0,680.0,681.0,682.0,683.0,684.0,685.0,686.0,687.0,688.0,689.0,690.0,691.0,692.0,693.0,694.0,695.0,696.0,697.0,698.0,699.0,700.0,701.0,702.0,703.0,704.0,705.0,706.0,707.0,708.0,709.0,710.0,711.0,712.0,713.0,714.0,715.0,716.0,717.0,718.0,719.0,720.0,721.0,722.0,723.0,724.0,725.0,726.0,727.0,728.0,729.0,730.0,731.0,732.0,733.0,734.0,735.0,736.0,737.0,738.0,739.0,740.0,741.0,742.0,743.0,744.0,745.0,746.0,747.0,748.0,749.0,750.0,751.0,752.0,753.0,754.0,755.0,756.0,757.0,758.0,759.0,760.0,761.0,762.0,763.0,764.0,765.0,766.0,767.0,768.0,769.0,770.0,771.0,772.0,773.0,774.0,775.0,776.0,777.0,778.0,779.0,780.0,781.0,782.0,783.0,784.0,785.0,786.0,787.0,788.0,789.0,790.0,791.0,792.0,793.0,794.0,795.0,796.0,797.0,798.0,799.0,800.0,801.0,802.0,803.0,804.0,805.0,806.0,807.0,808.0,809.0,810.0,811.0,812.0,813.0,814.0,815.0,816.0,817.0,818.0,819.0,820.0,821.0,822.0,823.0,824.0,825.0,826.0,827.0,828.0,829.0,830.0,831.0,832.0,833.0,834.0,835.0,836.0,837.0,838.0,839.0,840.0,841.0,842.0,843.0,844.0,845.0,846.0,847.0,848.0,849.0,850.0,851.0,852.0,853.0,854.0,855.0,856.0,857.0,858.0,859.0,860.0,861.0,862.0,863.0,864.0,865.0,866.0,867.0,868.0,869.0,870.0,871.0,872.0,873.0,874.0,875.0,876.0,877.0,878.0,879.0,880.0,881.0,882.0,883.0,884.0,885.0,886.0,887.0,888.0,889.0,890.0,891.0,892.0,893.0,894.0,895.0,896.0,897.0,898.0,899.0,900.0,901.0,902.0,903.0,904.0,905.0,906.0,907.0,908.0,909.0,910.0,911.0,912.0,913.0,914.0,915.0,916.0,917.0,918.0,919.0,920.0,921.0,922.0,923.0,924.0,925.0,926.0,927.0,928.0,929.0,930.0,931.0,932.0,933.0,934.0,935.0,936.0,937.0,938.0,939.0,940.0,941.0,942.0,943.0,944.0,945.0,946.0,947.0,948.0,949.0,950.0,951.0,952.0,953.0,954.0,955.0,956.0,957.0,958.0,959.0,960.0,961.0,962.0,963.0,964.0,965.0,966.0,967.0,968.0,969.0,970.0,971.0,972.0,973.0,974.0,975.0,976.0,977.0,978.0,979.0,980.0,981.0,982.0,983.0,984.0,985.0,986.0,987.0,988.0,989.0,990.0,991.0,992.0,993.0,994.0,995.0,996.0,997.0,998.0,999.0,1000.0],\"y\":[9.81792782855103,9.81791359820843,9.81790262599249,9.81788625667581,9.81786992227389,9.81785362094036,9.81783735388191,9.81782112026994,9.81780492044736,9.81778875397803,9.84680368892018,9.86278243666989,9.86274645450465,9.86262777758703,9.8625250514667,9.86241206813582,9.86230629881413,9.86219609525086,9.86208922216729,9.86198050230774,9.80280568990337,9.78981822684225,9.78973414913198,9.78980196620173,9.78982657557458,9.78987984545853,9.78991386401513,9.78996057276813,9.7899986776668,9.79004237504328,9.83163075692893,9.83770146313401,9.83766180979669,9.83760439087874,9.83754729573751,9.83749018229453,9.83743327819293,9.83737643126228,9.83731974254204,9.83726314426594,9.82376317746222,9.81297212780933,9.81295077011973,9.81294726401102,9.81293942211022,9.81293449182191,9.81292764149937,9.81292209219684,9.81291569650132,9.81290988597809,9.8026763795548,9.79999825000013,9.79994612998942,9.79996665295704,9.79998580762182,9.80000763501956,9.80002760893407,9.80004874699124,9.8000690375886,9.80008982194229,9.78884149155732,9.78836174066633,9.78836819951969,9.78840597631609,9.78845001193741,9.78849510995011,9.78853934548277,9.78858400210244,9.78862822441653,9.78867258300487,9.80390913289746,9.81221333757826,9.812250704123,9.81224168212841,9.81223994565861,9.81223336822729,9.81223003412573,9.81222455379261,9.81222052023596,9.81221553808398,9.8209300208287,9.82199713345235,9.82192545852545,9.82192311082068,9.82188351207451,9.8218688326198,9.82183762538787,9.82181752158584,9.82179010023218,9.82176764180348,9.81740114177763,9.81447745063659,9.81448967955669,9.81448097607449,9.81447217353824,9.81446259102402,9.81445356049856,9.81444419391775,9.81443508324485,9.81442583377167,9.81786244936939,9.82014335479309,9.82014481826456,9.82011507334134,9.8200999432334,9.82007514232834,9.82005686088443,9.82003430524612,9.8200146710347,9.8199931610201,9.8194755102466,9.81961654007155,9.81961193219189,9.81959037194608,9.81957160804867,9.8195510485566,9.81953175462308,9.81951168533743,9.81949220116519,9.81947239498804,9.80614398416153,9.80533658884666,9.80533556442365,9.80534550711984,9.80535134948473,9.80536097916334,9.80537058737723,9.80538017670208,9.80538974548953,9.80539929490712,9.83085580879917,9.83621072008228,9.836284954797,9.83622393678366,9.83617400163933,9.83611686560386,9.83606471745615,9.83600943096548,9.83595642322803,9.83590208246052,9.80039516759395,9.79638020443234,9.79652573045427,9.79651357471858,9.79656821923287,9.79657823384516,9.79661790552169,9.79663770982421,9.79667066323939,9.7966947549766,9.76686192016585,9.76123775170833,9.76117973392305,9.76129003999047,9.76138481013895,9.7614895889504,9.7615873474233,9.76168943895009,9.76178829531988,9.76188896272451,9.77252036495744,9.77484280598759,9.77497565528871,9.77502944593004,9.77510397030548,9.77517485742716,9.7752479196475,9.77531928258218,9.77539152967028,9.77546293913421,9.82713297323263,9.85015095696845,9.8501391189497,9.85004185481882,9.84996833185569,9.84987926804493,9.84980085080197,9.8497156212591,9.84963521806948,9.84955188154147,9.81175335959683,9.79968179076109,9.79959922959117,9.79964011269226,9.79964867786699,9.79967871413138,9.79969436229661,9.79971952863812,9.79973827579399,9.7997612288242,9.78850341997107,9.78806446868523,9.78808090749597,9.7881168019746,9.78816164525399,9.78820720143491,9.78825212606475,9.78829731576896,9.78834217312029,9.78838709672593,9.79018183472818,9.79115900761367,9.79121782035751,9.79124514362212,9.79129171085033,9.79132531458477,9.7913674267416,9.79140373298738,9.7914437764879,9.79148119540883,9.79368659476313,9.79361304709239,9.79364195025523,9.79367464767994,9.79370925248756,9.79374246887864,9.79377649429479,9.79380986401643,9.79384355476427,9.79387691563164,9.79697520751276,9.79977231715419,9.79982602908964,9.79983018768635,9.79986252898213,9.79987600908074,9.79990199073032,9.79991956558244,9.79994267268608,9.79996201949164,9.75516507081955,9.80208407501483,9.79971135700502,9.79989266658048,9.79985174114624,9.79991400180123,9.79990739928201,9.7999466332476,9.7999552374084,9.79998418944026,9.80616816891637,9.80619210010017,9.80619750402482,9.80620584748278,9.80621339476944,9.80622144568301,9.80622913374761,9.80623703666995,9.80624476936822,9.80625258861954,9.83332456229715,9.84405145543116,9.84407206242089,9.84398971034627,9.84392715965284,9.84385165131753,9.84378502418594,9.84371271852251,9.84364444031731,9.84357371841105,9.82013276300304,9.81202769582164,9.81194110715269,9.81194722157911,9.81193643159236,9.81193692512051,9.81192991018271,9.81192791473745,9.81192258678371,9.81191949429355,9.83357221813983,9.83856973124796,9.8386303992847,9.83856729232498,9.83851065725482,9.8384499120425,9.83839211095237,9.83833255071892,9.83827436643077,9.83821546760798,9.83227317774255,9.82676315301133,9.82675529548348,9.82672082486039,9.82668603449686,9.82665157695446,9.82661701694428,9.82658264441159,9.82654826581272,9.82651400993047,9.81765731780697,9.81676326827358,9.81680025959606,9.81676556963675,9.81676529245741,9.81674212190118,9.81673426197222,9.81671624323422,9.81670504521139,9.81668934812115,9.80540399042372,9.7962029836008,9.79620864788521,9.79623871205274,9.79626621713526,9.79629532978839,9.7963232724457,9.79635189702173,9.79637996910994,9.79640831185074,9.79942252071081,9.79927557765005,9.79930151857311,9.79932021587212,9.79934464528666,9.79936517687157,9.7993882307486,9.79940952699453,9.79943191904915,9.79945350477657,9.80451806656394,9.80448723515314,9.80449861214013,9.80450918147879,9.80452110510313,9.80453208657803,9.80454365694163,9.80455479558304,9.80456618299057,9.80457736558623,9.78799300380636,9.78192428254135,9.78189471202609,9.78195653133625,9.7820122147291,9.78207178803189,5.00510725824344,9.79614468334179,9.79609476283324,9.79612363377438,9.84566520266272,9.87177562873302,9.87171166567599,9.87157525533918,9.87145343876726,-25.509470812372,7.12362225259646,9.94999760759662,9.94971318425034,9.94951782704178,9.92643038947025,9.92324443031105,9.92289169827359,9.92266254910563,9.9225140487494,66.0949664280959,15.0478004307144,9.79546538130461,9.79557719987299,9.79559649365793,9.76726754540607,9.75876655407034,9.75875024167801,9.75886130820077,9.75896395682979,-27.3227144435684,6.77057714526485,9.84260700004299,9.84260459804979,9.84256154725685,9.83670590660884,9.83574832327576,9.83572914545904,9.83566414512552,9.83561879185633,28.4465074624927,11.5823797870001,9.79307198910811,9.79309904561004,9.79312615730164,9.80851274189994,9.81143784261677,9.81154198095847,9.81153189808661,9.81153097109313,19.7735849381044,10.796550647973,9.78881143111471,9.78883857294073,9.78887131815007,9.80118362410002,9.8090557330845,9.80908517581595,9.80907997191786,9.80908661009342,18.8371762500188,10.7011896900638,9.7883825441732,9.78843893279717,9.78847694552061,9.77192837436136,9.76591797475027,9.76591128349252,9.76599708494734,9.76609190813216,-8.55232435412554,8.27170837916599,9.8075950003124,9.80754105231261,9.80754620190304,9.790702710416,9.78503649126171,9.78499719111328,9.78505288439163,9.78510195420303,2.19311783907618,9.18147502605266,9.8023713472773,9.80237333057192,9.80238884757956,9.80952537917428,9.81077553082828,9.81073480630443,9.81074967457351,9.81073719491896,15.3013425376713,10.3482234066413,9.79811409409528,9.79811444569442,9.79813455131568,9.81150498558662,9.81778357884362,9.81781858985024,9.81780200343493,9.81778610427647,5.27961895968728,9.48067977110786,9.8277952132213,9.82777410523938,9.82774691970328,9.8496978820073,9.85397961249169,9.85402863385742,9.85393964965006,9.8538471623089,9.91387163624244,10.040052552125,9.89519628081191,9.90138962519686,9.85258364302011,9.83273163157012,9.82579608559803,9.82569483933263,9.82567377114577,9.82563367894258,20.8380958802606,10.7908709491209,9.80081630367461,9.80083364298099,9.80084648276368,9.81899572461974,9.82235160154486,9.82243876392066,9.82241742005743,9.82239013840709,-14.3931577417303,7.84950777175991,9.876464567433,9.87633908478489,9.87622513033274,9.85374301511242,9.8438976912118,9.84378018637481,9.84372953668203,9.84364679300868,40.1903326235476,12.7750542285456,9.77457580532954,9.77462182433171,9.77467398343859,9.78154529473355,9.78584935901267,9.78590834777693,9.78593993136441,9.78598369498154,37.5478080087204,12.0705953448325,9.72365699406173,9.72388912431079,9.72398801751956,9.72615655559155,9.72800962596699,9.72819699697305,9.72834855242948,9.72852959965648,-0.114908212490873,8.87653677344716,9.75153675217143,9.75161978451002,9.75171111801924,9.71655542097848,9.71241654442155,9.71266302672076,9.71283262141771,9.71305523105412,-48.0673120268585,4.83918976113877,9.84411820101316,9.84430451972635,9.84426612344895,9.83080237594491,9.83007064019407,9.83004050425852,9.83001258178562,9.8299620713166,48.7667617975676,13.2756569908131,9.74254958056649,9.74274067511514,9.74283921124483,9.78130086159956,9.80240841454299,9.80246333030551,9.80246584550305,9.80249018138242,18.8009440456723,10.6863100693606,9.78189414669557,9.78196699400493,9.78201257725329,9.77108021836371,9.76980599480112,9.76994221278627,9.76999667995449,9.77009854210161,8.67228724986355,9.55800907096812,9.77509428484864,9.77517678615575,9.77521065818822,9.80203071103345,9.80472253813024,9.80467867775501,9.80469878221161,9.80470370772289,9.42125654824645,8.79101640086192,9.8083366753655,9.80833812823903,9.80833985799353,9.78885986634496,9.78108411255556,9.78105631771202,9.78111659975167,9.78117620545958,5.17824025301533,9.42113512052815,9.791674135715,9.79167491087031,9.7917016683875,9.78435513495884,9.78440170557185,9.78444823657725,9.7844863732032,9.78453733490666,9.8202984597642,9.89535086511463,9.8094183627325,9.76771070738732,9.76888958307965,9.78133200073726,9.78151613689162,9.78156003960779,9.78162299313852,9.78167907338448,5.00137525436287,9.42521100688944,9.79271497371662,9.79271990860576,9.79273699634947,9.81489375254918,9.81905454129899,9.81915090190161,9.81913001950017,9.81911239385429,-19.6216696488372,6.96794818644226,9.88530188309148,9.88521570025537,9.88510836906323,9.88171311457079,9.88143014086801,9.88131817810376,9.88116953778403,9.88102273652355,34.5173137450665,12.1445691670906,9.82534673703223,9.82536095302375,9.8253366525716,9.83277611291888,9.83324531459056,9.83317755195896,9.83314145833282,9.83308558259294,-10.8527506647574,7.87439527940122,9.87983259956582,9.87969829551825,9.87958505711172,9.89223904846055,9.90360589814136,9.90348288626638,9.90329710559579,9.90309862509192,18.547407478173,10.8104888310188,9.88264025185936,9.88250272486078,9.88239954259927,9.8559107331107,9.85222342039114,9.85207074088438,9.8519937874029,9.85190001364376,17.3710899030973,10.5384010792748,9.83469749555562,9.83466643325605,9.83463664221292,9.79994568222551,9.78304028778457,9.78302170847343,9.78309273492346,9.7831383705687,25.6307485858728,11.0355690000443,9.74741975141934,9.74758847964933,9.74767225755714,9.78237428986004,9.78881186221922,9.78897003840216,9.78899433375091,9.78905068559353,8.66205367932174,9.60225050828104,9.79410706151515,9.79415068502082,9.7941710577081,9.82018928557394,9.82445952223405,9.8245626075716,9.82452581015347,9.8245001453495,6.04787025295131,9.52776445341442,9.83315471085227,9.83308984542187,9.83304115601901,9.78844845542836,9.77763437610952,9.7775560330706,9.77764524328063,9.77769758721127,27.3651898876939,11.366593846755,9.73802311862476,9.73821655102845,9.73831365714722,9.75665791974043,9.7601441793581,9.76033770991648,9.76041026174923,9.7605141887629,-48.7148260894367,5.33277903309277,9.89331717549239,9.89318180570951,9.89306506402665,9.87190074620783,9.86002195266228,9.85987611802916,9.85977936817656,9.85966643657565,29.9313286676705,11.7447867494727,9.81426609993781,9.81427493710237,9.81427252957545,9.84816617834598,9.8557492460168,9.85578858756939,9.85567923086479,9.85559340078983,38.3822581823955,12.3685558107336,9.79045296884073,9.79050374204719,9.79052801702313,9.77615209585204,9.77165511471879,9.77164332383192,9.77172238094316,9.77180319874096,14.8477143067024,10.2535899208957,9.76035385584433,9.76051320824708,9.76058521713342,9.74647440346175,9.73697601556538,9.73706070066841,9.73721510064343,9.73736609508734,-4.60129718240218,8.64701279237588,9.77005069762509,9.77007266455669,9.77013163526795,9.81208697220745,9.81738591849814,9.81727720426806,9.81729390973245,9.81725809960969,-5.83595672444658,8.59447879322521,9.85249944897468,9.85236214644202,9.85231023850528,9.88261434303676,9.88689602423369,9.88684547369287,9.88667974185226,9.88652267481291,27.4926592598442,11.4153274458406,9.84610654956333,9.84609058225701,9.84603587996903,9.82563636707042,9.81292199842678,9.81288181754328,9.81288366452507,9.81287349013128,27.3424129384738,11.4446550846414,9.77322533559977,9.77334140261912,9.77338807130948,9.75804873937827,9.75547497298495,9.75547478459385,9.7555642355181,9.75567800250935,3.25843590733679,9.18851527323121,9.77064293330614,9.77072256268615,9.77076935444164,9.82284661382663,9.84338170658969,9.84343764677815,9.84334688692875,9.8432945708594,-5.10768812375413,8.55472273125767,9.87751495378946,9.87732835597569,9.87724243407119,9.81058928516409,9.79354698902471,9.79344193023191,9.79350521444396,9.79352005355421,-10.698297643925,7.74663367920598,9.84000408795498,9.83989840629125,9.83985849502447,9.87302693821845,9.87911293024565,9.87917019990103,9.87903140367753,9.87888766672708,9.87874884251021,17.7011054418541,9.83136972461333,9.83121695349494,9.83118437903393,9.8231529722824,9.82338014336107,9.82335873298838,9.82332922797155,9.8233029836655,-7.08440261245906,8.39740713566193,9.86104872062697,9.86090719493734,9.86083348140988,9.90448010252672,9.87135795524259,9.86363111049535,9.86003710941641,9.86025662446669,34.9579754567643,11.8305767106154,9.80351068924835,9.80359846531702,9.80360031681034,9.78900785036237,9.78856029793308,9.78860934986512,9.78863544006133,9.78868065980918,1.10319664933626,9.10214765727791,9.80820007520479,9.80813702710562,9.80812860840522,9.77832135869653,9.77623549977236,9.7763667833118,9.7764147622987,9.77649887779035,8.29010617703981,9.59620158297713,9.78316215117033,9.78321441293849,9.78326303097196,9.82268732016809,9.82840884459529,9.82847077342999,9.82842258870827,9.82839049632931,40.4071138319294,12.1663709909526,9.75913181437816,9.75918686942683,9.7592552377851,9.75020989316935,9.74953100416186,9.74968578148791,9.74980275975081,9.74993500131634,20.719366694304,10.6197500740147,9.72579552009561,9.72593829158686,9.72605922470442,9.70319008728131,9.69014915356524,9.69028738295081,9.69054449564576,9.6907896852327,-36.052470854319,5.39097150014939,9.79555575635242,9.79553430080877,9.79555935305564,9.866639509764,9.88925801042778,9.88925090253003,9.88905602464246,9.88891107376616,19.3534282973837,10.9335454609496,9.86681028157714,9.86673858251248,9.86665366110375,9.84260833776102,9.83921249563193,9.83899419819203,9.8389651053166,9.83890291117052,-12.8419704808625,8.1143903442545,9.89013412174265,9.88987960273381,9.88977904203107,9.86079061053291,9.85562751496198,9.85542918568597,9.85534609287085,9.85524067344654,30.7743154508824,11.8085066939653,9.80696873424162,9.80704307691651,9.80704253068764,9.79187965092242,9.78082999370822,9.78084426375595,9.78090718560885,9.78096591456999,9.78102705761915,11.7481968108651,9.76818113382302,9.76831951881812,9.76838067270475,9.77577539521345,9.78022751514316,9.78027748686739,9.7803230654149,9.78038362271601,9.78044469682055,10.547090542881,9.77510306527718,9.77527431920578,9.77533459153282,9.78121408757188,9.782473954618,9.78249996304078,9.78256903094555,9.78262025712229,-10.7894515731367,7.78173575383753,9.82904194810923,9.82892785475469,9.82891137052213,9.83510815127617,9.83545584588062,9.83538875153538,9.83533987042205,9.83528378415643,11.2365385121593,10.1250531263624,9.82922619346095,9.82921094807061,9.82919972307263,9.81216457172546,9.81022846711978,9.81033375389239,9.81028081737064,9.81031331638487,3.60877649575131,9.357361416467,9.8242878704703,9.82421366235505,9.82418847265282,9.82342568701209,9.82344871793321,9.8234213162085,9.82338882029152,9.82336371540684,-9.10394873040551,7.7667971431918,9.86598542791445,9.86585362862551,9.86577775946332,9.87322565921251,9.87401444041757,9.87386336825556,9.87373410971884,9.87359337585983,32.9090563634874,11.6691690740799,9.82063633436367,9.82069530869216,9.8206863206965,9.81328784609742,9.81197781638846,9.81200471661059,9.81198302988723,9.81198747853638,50.2547492054954,13.6819968621989,9.72157798073554,9.72187433987325,9.72197492559497,9.76115682601828,9.76800044115733,9.76824110369781,9.76831544987454,9.76841522137725,-22.4712232404278,6.73482807649141,9.8404493458397,9.84041887255715,9.84037378209477,9.80159320822183,9.79435624753476,9.79421733686373,9.79426204255855,9.79428601558198,-14.0737540998868,7.89871609609309,9.84760683100974,9.84747681489565,9.84742717407879,9.8758109808966,9.88122710508882,9.88126354862903,9.88111586491637,9.88095767148966,9.88082016263679,21.8181932048507,9.78326230319221,9.78340622343702,9.78343539416022],\"type\":\"scatter\"},{\"line\":{\"color\":\"red\",\"width\":1},\"name\":\"B_v4.6_p2.0\",\"x\":[0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0,35.0,36.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,44.0,45.0,46.0,47.0,48.0,49.0,50.0,51.0,52.0,53.0,54.0,55.0,56.0,57.0,58.0,59.0,60.0,61.0,62.0,63.0,64.0,65.0,66.0,67.0,68.0,69.0,70.0,71.0,72.0,73.0,74.0,75.0,76.0,77.0,78.0,79.0,80.0,81.0,82.0,83.0,84.0,85.0,86.0,87.0,88.0,89.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0,100.0,101.0,102.0,103.0,104.0,105.0,106.0,107.0,108.0,109.0,110.0,111.0,112.0,113.0,114.0,115.0,116.0,117.0,118.0,119.0,120.0,121.0,122.0,123.0,124.0,125.0,126.0,127.0,128.0,129.0,130.0,131.0,132.0,133.0,134.0,135.0,136.0,137.0,138.0,139.0,140.0,141.0,142.0,143.0,144.0,145.0,146.0,147.0,148.0,149.0,150.0,151.0,152.0,153.0,154.0,155.0,156.0,157.0,158.0,159.0,160.0,161.0,162.0,163.0,164.0,165.0,166.0,167.0,168.0,169.0,170.0,171.0,172.0,173.0,174.0,175.0,176.0,177.0,178.0,179.0,180.0,181.0,182.0,183.0,184.0,185.0,186.0,187.0,188.0,189.0,190.0,191.0,192.0,193.0,194.0,195.0,196.0,197.0,198.0,199.0,200.0,201.0,202.0,203.0,204.0,205.0,206.0,207.0,208.0,209.0,210.0,211.0,212.0,213.0,214.0,215.0,216.0,217.0,218.0,219.0,220.0,221.0,222.0,223.0,224.0,225.0,226.0,227.0,228.0,229.0,230.0,231.0,232.0,233.0,234.0,235.0,236.0,237.0,238.0,239.0,240.0,241.0,242.0,243.0,244.0,245.0,246.0,247.0,248.0,249.0,250.0,251.0,252.0,253.0,254.0,255.0,256.0,257.0,258.0,259.0,260.0,261.0,262.0,263.0,264.0,265.0,266.0,267.0,268.0,269.0,270.0,271.0,272.0,273.0,274.0,275.0,276.0,277.0,278.0,279.0,280.0,281.0,282.0,283.0,284.0,285.0,286.0,287.0,288.0,289.0,290.0,291.0,292.0,293.0,294.0,295.0,296.0,297.0,298.0,299.0,300.0,301.0,302.0,303.0,304.0,305.0,306.0,307.0,308.0,309.0,310.0,311.0,312.0,313.0,314.0,315.0,316.0,317.0,318.0,319.0,320.0,321.0,322.0,323.0,324.0,325.0,326.0,327.0,328.0,329.0,330.0,331.0,332.0,333.0,334.0,335.0,336.0,337.0,338.0,339.0,340.0,341.0,342.0,343.0,344.0,345.0,346.0,347.0,348.0,349.0,350.0,351.0,352.0,353.0,354.0,355.0,356.0,357.0,358.0,359.0,360.0,361.0,362.0,363.0,364.0,365.0,366.0,367.0,368.0,369.0,370.0,371.0,372.0,373.0,374.0,375.0,376.0,377.0,378.0,379.0,380.0,381.0,382.0,383.0,384.0,385.0,386.0,387.0,388.0,389.0,390.0,391.0,392.0,393.0,394.0,395.0,396.0,397.0,398.0,399.0,400.0,401.0,402.0,403.0,404.0,405.0,406.0,407.0,408.0,409.0,410.0,411.0,412.0,413.0,414.0,415.0,416.0,417.0,418.0,419.0,420.0,421.0,422.0,423.0,424.0,425.0,426.0,427.0,428.0,429.0,430.0,431.0,432.0,433.0,434.0,435.0,436.0,437.0,438.0,439.0,440.0,441.0,442.0,443.0,444.0,445.0,446.0,447.0,448.0,449.0,450.0,451.0,452.0,453.0,454.0,455.0,456.0,457.0,458.0,459.0,460.0,461.0,462.0,463.0,464.0,465.0,466.0,467.0,468.0,469.0,470.0,471.0,472.0,473.0,474.0,475.0,476.0,477.0,478.0,479.0,480.0,481.0,482.0,483.0,484.0,485.0,486.0,487.0,488.0,489.0,490.0,491.0,492.0,493.0,494.0,495.0,496.0,497.0,498.0,499.0,500.0,501.0,502.0,503.0,504.0,505.0,506.0,507.0,508.0,509.0,510.0,511.0,512.0,513.0,514.0,515.0,516.0,517.0,518.0,519.0,520.0,521.0,522.0,523.0,524.0,525.0,526.0,527.0,528.0,529.0,530.0,531.0,532.0,533.0,534.0,535.0,536.0,537.0,538.0,539.0,540.0,541.0,542.0,543.0,544.0,545.0,546.0,547.0,548.0,549.0,550.0,551.0,552.0,553.0,554.0,555.0,556.0,557.0,558.0,559.0,560.0,561.0,562.0,563.0,564.0,565.0,566.0,567.0,568.0,569.0,570.0,571.0,572.0,573.0,574.0,575.0,576.0,577.0,578.0,579.0,580.0,581.0,582.0,583.0,584.0,585.0,586.0,587.0,588.0,589.0,590.0,591.0,592.0,593.0,594.0,595.0,596.0,597.0,598.0,599.0,600.0,601.0,602.0,603.0,604.0,605.0,606.0,607.0,608.0,609.0,610.0,611.0,612.0,613.0,614.0,615.0,616.0,617.0,618.0,619.0,620.0,621.0,622.0,623.0,624.0,625.0,626.0,627.0,628.0,629.0,630.0,631.0,632.0,633.0,634.0,635.0,636.0,637.0,638.0,639.0,640.0,641.0,642.0,643.0,644.0,645.0,646.0,647.0,648.0,649.0,650.0,651.0,652.0,653.0,654.0,655.0,656.0,657.0,658.0,659.0,660.0,661.0,662.0,663.0,664.0,665.0,666.0,667.0,668.0,669.0,670.0,671.0,672.0,673.0,674.0,675.0,676.0,677.0,678.0,679.0,680.0,681.0,682.0,683.0,684.0,685.0,686.0,687.0,688.0,689.0,690.0,691.0,692.0,693.0,694.0,695.0,696.0,697.0,698.0,699.0,700.0,701.0,702.0,703.0,704.0,705.0,706.0,707.0,708.0,709.0,710.0,711.0,712.0,713.0,714.0,715.0,716.0,717.0,718.0,719.0,720.0,721.0,722.0,723.0,724.0,725.0,726.0,727.0,728.0,729.0,730.0,731.0,732.0,733.0,734.0,735.0,736.0,737.0,738.0,739.0,740.0,741.0,742.0,743.0,744.0,745.0,746.0,747.0,748.0,749.0,750.0,751.0,752.0,753.0,754.0,755.0,756.0,757.0,758.0,759.0,760.0,761.0,762.0,763.0,764.0,765.0,766.0,767.0,768.0,769.0,770.0,771.0,772.0,773.0,774.0,775.0,776.0,777.0,778.0,779.0,780.0,781.0,782.0,783.0,784.0,785.0,786.0,787.0,788.0,789.0,790.0,791.0,792.0,793.0,794.0,795.0,796.0,797.0,798.0,799.0,800.0,801.0,802.0,803.0,804.0,805.0,806.0,807.0,808.0,809.0,810.0,811.0,812.0,813.0,814.0,815.0,816.0,817.0,818.0,819.0,820.0,821.0,822.0,823.0,824.0,825.0,826.0,827.0,828.0,829.0,830.0,831.0,832.0,833.0,834.0,835.0,836.0,837.0,838.0,839.0,840.0,841.0,842.0,843.0,844.0,845.0,846.0,847.0,848.0,849.0,850.0,851.0,852.0,853.0,854.0,855.0,856.0,857.0,858.0,859.0,860.0,861.0,862.0,863.0,864.0,865.0,866.0,867.0,868.0,869.0,870.0,871.0,872.0,873.0,874.0,875.0,876.0,877.0,878.0,879.0,880.0,881.0,882.0,883.0,884.0,885.0,886.0,887.0,888.0,889.0,890.0,891.0,892.0,893.0,894.0,895.0,896.0,897.0,898.0,899.0,900.0,901.0,902.0,903.0,904.0,905.0,906.0,907.0,908.0,909.0,910.0,911.0,912.0,913.0,914.0,915.0,916.0,917.0,918.0,919.0,920.0,921.0,922.0,923.0,924.0,925.0,926.0,927.0,928.0,929.0,930.0,931.0,932.0,933.0,934.0,935.0,936.0,937.0,938.0,939.0,940.0,941.0,942.0,943.0,944.0,945.0,946.0,947.0,948.0,949.0,950.0,951.0,952.0,953.0,954.0,955.0,956.0,957.0,958.0,959.0,960.0,961.0,962.0,963.0,964.0,965.0,966.0,967.0,968.0,969.0,970.0,971.0,972.0,973.0,974.0,975.0,976.0,977.0,978.0,979.0,980.0,981.0,982.0,983.0,984.0,985.0,986.0,987.0,988.0,989.0,990.0,991.0,992.0,993.0,994.0,995.0,996.0,997.0,998.0,999.0,1000.0],\"y\":[9.81011231515091,9.81011203088729,9.81011181661267,9.8101114938914,9.81011117211482,9.81011085125804,9.8101105313332,9.81011021232777,9.81010989424563,9.81010957707975,9.8105812572388,9.81074653211497,9.81074540884642,9.81074316673192,9.81074108241196,9.81073890322516,9.81073679758265,9.81073465318058,9.81073254485855,9.81073042269393,9.80984586791447,9.80970698944158,9.80970650930106,9.80970760785196,9.80970828405064,9.80970923775221,9.80971000239832,9.80971088903959,9.8097116903206,9.80971254448991,9.81032518653737,9.81038769529058,9.81038708184001,9.81038592775109,9.81038483854993,9.81038371145434,9.8103826149703,9.81038150341171,9.8103804072206,9.81037930608692,9.8101460246469,9.81003465639037,9.81003442898923,9.8100343543348,9.81003423871102,9.81003415087748,9.81003404499308,9.81003395161718,9.8100338503755,9.81003375484952,9.80987831083131,9.80985067658976,9.80985034598565,9.80985071442016,9.8098511297833,9.8098515697629,9.80985199126252,9.80985242301909,9.80985284588061,9.80985327262102,9.80969220006297,9.80968753323505,9.80968785022461,9.80968861773805,9.80968950892756,9.80969041002686,9.80969130020427,9.80969219336003,9.80969308023974,9.80969396702537,9.80994224864589,9.81002799941783,9.81002834103397,9.81002821551934,9.81002816323317,9.81002806252043,9.81002799448334,9.81002790505262,9.81002783027333,9.81002774611432,9.8101550409962,9.81016605704333,9.81016514117382,9.81016489371385,9.81016426504632,9.81016389280768,9.81016335190035,9.81016292571614,9.8101624253206,9.81016197666347,9.810088952264,9.81005853654022,9.81005862586003,9.81005847572143,9.81005831246499,9.81005814049637,9.8100579751463,9.81005780619195,9.81005764044622,9.81005747336485,9.81011486313152,9.81013839128913,9.81013826914872,9.81013777306495,9.81013744012284,9.81013700033502,9.81013663368754,9.8101362201842,9.81013583981696,9.81013543925236,9.81012859043847,9.81012990834792,9.81012974853739,9.81012935601246,9.81012899471772,9.81012861440082,9.81012824855815,9.81012787485361,9.81012750817276,9.81012713858676,9.80993520151299,9.80992682181286,9.80992686978181,9.80992700705524,9.80992721754871,9.80992742771978,9.80992763709421,9.80992784599108,9.80992805420057,9.80992826186547,9.81030991872267,9.81036504923118,9.81036547405779,9.81036434939738,9.81036334442922,9.81036226473055,9.81036123990178,9.81036018352834,9.81035915320529,9.81035811052146,9.80983989467142,9.80979845483489,9.80980001258445,9.80980024002889,9.80980104971378,9.80980146846393,9.80980214507081,9.80980264701475,9.80980326265016,9.80980379974869,9.80935926604958,9.8093017748478,9.80930182603898,9.80930393751595,9.80930588209737,9.80930792826763,9.80930989706506,9.80931190782529,9.80931388101898,9.80931586969337,9.8094760380363,9.80950094075139,9.80950280148542,9.80950389855615,9.8095053513717,9.80950676496108,9.80950819782617,9.80950961098558,9.80951103044667,9.80951243889093,9.8103337918994,9.81057092594624,9.81057019526963,9.81056841259283,9.81056686345998,9.81056516653208,9.81056357601237,9.81056192240924,9.81056031869445,9.81055868953086,9.80997601002182,9.80985125657728,9.80985058697552,9.80985121240374,9.80985151234671,9.80985202721082,9.80985239673113,9.8098528610902,9.80985326017226,9.80985370072708,9.8096926627894,9.80968839850762,9.80968883978742,9.80968956327101,9.80969045295061,9.80969135051271,9.80969223851757,9.80969312860408,9.80969401302538,9.80969489695838,9.80972427359317,9.80973475574533,9.80973566324135,9.80973630267364,9.8097371460414,9.80973784978784,9.80973864296139,9.80973937287364,9.80974014132737,9.80974088046456,9.80977196465694,9.80977153659694,9.80977206098937,9.80977270779363,9.80977337114658,9.80977402030807,9.80977467578066,9.80977532390507,9.80977597379655,9.8097766193873,9.80983142049738,9.80986055119778,9.80986127443663,9.80986149282941,9.80986201389572,9.80986233125563,9.80986278250131,9.80986314257685,9.80986356152505,9.80986393932287,9.8094095014281,9.80988516359239,9.80986119155339,9.8098631959028,9.80986297065841,9.80986377952065,9.80986389707741,9.80986447360891,9.80986474226713,9.80986521428955,9.80995289620826,9.80995333601656,9.80995342526931,9.80995356855648,9.80995369664994,9.80995383422716,9.80995396483836,9.80995409945191,9.80995423075741,9.80995436363031,9.810379506795,9.81048979698648,9.81048951861658,9.81048798773761,9.81048665808025,9.81048520105817,9.81048383570935,9.81048241598895,9.81048103924073,9.81047964054945,9.8101157225426,9.8100317814111,9.8100308885648,9.81003090267327,9.81003074464804,9.81003070180755,9.81003058260492,9.81003051473684,9.81003041307096,9.81003033436094,9.81035659801378,9.810407929721,9.81040817564,9.81040695582085,9.81040580916505,9.81040461939004,9.81040346400122,9.81040229131203,9.81040113576395,9.81039997437982,9.81029435713241,9.81023710594623,9.81023684381787,9.81023616238119,9.81023547938295,9.81023480070808,9.81023412242381,9.81023344714255,9.81023277311327,9.81023210149393,9.81010326709458,9.81009377868052,9.81009405639368,9.81009357262491,9.81009344410757,9.81009308005765,9.81009287432741,9.81009256434493,9.81009232515442,9.81009204005602,9.80989648926868,9.80980179176116,9.80980204648063,9.80980263161007,9.80980319202262,9.80980376616987,9.80980432842535,9.80980489588126,9.80980545715066,9.80980601983271,9.80984830553154,9.80984698354,9.80984743149274,9.80984783548736,9.80984829887018,9.80984872054619,9.80984916791817,9.80984959605702,9.80985003492153,9.80985046454522,9.80992214892352,9.80992200790938,9.80992221144093,9.80992242929461,9.80992265760529,9.80992287786655,9.80992310241911,9.80992332303893,9.80992354521179,9.80992376528359,9.80966522818658,9.80960295032881,9.80960301165627,9.80960419539164,9.80960531239473,9.80960646838465,9.80960759289797,9.80960873292599,9.80960985715702,9.80961098648125,9.81041481491272,9.81068377500726,9.81068262306375,9.81068056153446,9.81067865831269,9.81067665901196,9.81067473319597,9.81067276779502,9.81067083816176,9.81066889403403,9.81033395274843,9.81030173258637,9.81029955454948,9.81029875250431,9.81029794116028,9.81029739424517,9.8102965359131,9.81029568009857,9.81029482672713,9.81029397583651,9.80985744436696,9.80976926408759,9.80976880830165,9.80976960284971,9.80977018292633,9.80977090278099,9.80977152625662,9.80977221080048,9.80977285145692,9.80977351820554,9.80968971560003,9.80968056611906,9.80968157453114,9.80968238050732,9.80968337254037,9.80968423612166,9.80968518093635,9.80968606720703,9.8096869881313,9.80968788159007,9.80991793570336,9.80994849959921,9.80994970136827,9.80994972326878,9.80994988127981,9.80995001732208,9.80995016731354,9.80995030731089,9.80995045327834,9.80995059457509,9.81015463947143,9.81023533372893,9.81023515702079,9.8102344546601,9.81023379378535,9.81023310851204,9.81023244275376,9.81023176722541,9.81023110144105,9.81023043238197,9.8099704092816,9.80990775585289,9.80990726432358,9.80990756943165,9.8099078112189,9.80990809393516,9.80990834808426,9.80990862000067,9.80990887879843,9.80990914507185,9.8096487044311,9.80959064478587,9.80959066957352,9.8095918833091,9.80959303959288,9.80959422850541,9.80959539000937,9.80959656414595,9.80959772423731,9.8095988880849,9.80970593224688,9.80971952297071,9.80971979167084,9.80972077053253,9.80972146297916,9.8097223424873,9.80972309341711,9.80972392620633,9.80972470057426,9.80972551005248,9.80993986902829,9.81000491375102,9.81000535215941,9.81000531319574,9.81000531353924,9.81000528775194,9.81000527945894,9.81000525957662,9.81000524749386,9.81000523028452,9.81033399741669,9.81037824276762,9.81037863663982,9.81037752566369,9.81037644821898,9.81037535366718,9.81037427575218,9.81037319196292,9.81037211729171,9.8103710417287,9.81004436022371,9.80998856253437,9.80998765235857,9.80998773813742,9.80998774009805,9.80998779776679,9.80998781812659,9.80998786318929,9.80998789161416,9.80998793096218,9.81025786395692,9.81029223208347,9.81029251895486,9.81029195685974,9.81029111309254,9.81029027368074,9.81028943541143,9.81028860041486,9.81028776725923,9.80837272218967,9.81013606663209,9.81002716303774,9.81002662167346,9.81002663720218,9.81002649875044,9.81002646332205,9.81002635958007,9.81002630174805,9.42290604416674,9.81014815944789,9.81125188567346,9.81129545947406,9.81129227597485,9.81129005949732,9.81128631609747,9.81128261711191,9.81127890639498,9.81127522132553,10.4295561288951,9.81152743609537,9.80951171656802,9.80953028828538,9.8095316615453,9.80953272673053,9.80953426631451,9.80953548313981,9.80953690866338,9.80953818859935,9.4009886879137,9.80980630011106,9.81020941446871,9.81016096069531,9.81016108154242,9.81016059131441,9.81016014472038,9.81015967126915,9.81015921794839,9.81015875342663,10.0179646951838,9.81026376251492,9.8093673409653,9.80936060930727,9.80936227853654,9.80936340222252,9.80936523904637,9.80936707133141,9.80936889782001,9.80937071937606,9.920807364465,9.80935582916062,9.80967956807731,9.80989682389084,9.8098974908206,9.80989764885414,9.8098979721394,9.80989824800799,9.80989855406948,9.80989883858826,9.91135386259174,9.80997286203297,9.80944615861489,9.80943363712781,9.8094357710579,9.80943694086157,9.80943878719926,9.80944025938185,9.80944197319986,9.80944351814877,9.60759095926033,9.80955546296129,9.81041788523603,9.81044611154829,9.81044498261779,9.81044375267027,9.81044243574859,9.81044118297693,9.81043989358807,9.81043863474238,9.72673176930656,9.81045354977844,9.81036498711673,9.81028797245892,9.81028681936617,9.8102860627182,9.81028518968763,9.81028439822095,9.81028355634236,9.81028275202428,9.87231701717022,9.81021731336012,9.81002836411189,9.8099961104342,9.80999638570201,9.80999638902681,9.80999643219348,9.80999642066047,9.80999644554414,9.8099964461004,9.76056163370726,9.81000250331858,9.81010188592515,9.81009525885387,9.81009500546909,9.81009482438745,9.8100944886399,9.81009425732037,9.81009395769576,9.8100937049177,9.81150432773691,9.81042252034905,9.81042384494911,9.81045608364653,9.81045612037182,9.81045489540207,9.81045398916765,9.81045270305207,9.81045138041495,9.81045008841734,9.93170404085568,9.81058872607621,9.81005569932745,9.8100485880932,9.81004848449069,9.81004831779079,9.81004817606823,9.81004803854278,9.81004789888885,9.81004776132157,9.54470877646172,9.81000382624173,9.81091931131846,9.81092422464026,9.8109218893393,9.81091952843021,9.81091667475057,9.81091416236315,9.81091143518923,9.81090886391118,10.1482637146654,9.81099698604382,9.81025572985381,9.8102714046974,9.81027008075333,9.81026945689871,9.81026857547135,9.81026786950337,9.81026705029589,9.81026631030404,10.1166697986752,9.81016805271148,9.8089974828238,9.80895998890879,9.80895994974862,9.80896297841108,9.8089659533672,9.80896894971421,9.80897191742888,9.80897488990151,9.69903211081827,9.80903522805829,9.80874206040791,9.80857495755758,9.80857731856052,9.80858142037769,9.80858535483829,9.80858954410061,9.80859354383439,9.80859765031651,9.16763396838407,9.80911373794055,9.81096687368018,9.81103722254709,9.81103687136346,9.81103372500891,9.8110308441448,9.8110278006553,9.81102487990949,9.81102189161996,10.2387333628225,9.81092590412755,9.81015822256763,9.81020679655052,9.81020747442973,9.81020683174834,9.81020626407274,9.81020564926822,9.81020506874965,9.8102044682321,9.91134438718155,9.81028679032999,9.80923748597857,9.80912743245253,9.80912790956066,9.80913058180942,9.80913287990505,9.80913552324714,9.80913792437431,9.80914047496225,9.79052735138887,9.80910748431761,9.80949319125033,9.80952251799985,9.8095248007604,9.80952562796737,9.80952698149035,9.80952835703945,9.80952971133025,9.80953107323799,9.79505091542522,9.80960002605048,9.80921324612197,9.8091251778619,9.80912594834446,9.80912857403804,9.80913102032398,9.80913357410281,9.80913604414192,9.80913855796534,9.75888343913985,9.80920141463002,9.80981163940919,9.80988161440199,9.80988299255585,9.8098831896277,9.80988362050192,9.80988389388518,9.80988427064437,9.80988457687291,9.81072298823803,9.8106929105893,9.80963456458225,9.80961325504692,9.80961294004895,9.80961380733706,9.80961486175038,9.80961601341254,9.80961709488957,9.80961821781957,9.75670364315092,9.80963529939738,9.80952686597392,9.80943803649392,9.80943869366962,9.80944035769518,9.80944194351408,9.80944357369224,9.8094451665415,9.80944677654302,9.48424186772757,9.8094351313793,9.81099817228116,9.81105467080155,9.81105257189911,9.81104963645931,9.8110465388546,9.81104356394778,9.81104052178822,9.81103753896769,10.0811853751938,9.81112195847872,9.81069177830157,9.8107384465598,9.81073836522266,9.81073616163188,9.81073408539918,9.81073193449314,9.81072984357193,9.81072772283387,9.58093460339796,9.81059754794772,9.81105402032673,9.81091365818589,9.81091186376072,9.81090929434922,9.81090662849755,9.81090403957693,9.81090141197008,9.81089882271883,9.90789389053807,9.81080896209546,9.8103729550568,9.81035144642014,9.81034969232653,9.81034895625759,9.81034804749219,9.81034697742404,9.8103460197279,9.81034499193996,9.893092880892,9.81033888651887,9.81092419813625,9.81112818862695,9.81112696220919,9.81112347930656,9.811120390428,9.81111705448284,9.81111389881979,9.8111106384948,9.98699601891796,9.8111994360777,9.8096346095499,9.80940976855417,9.80940980003154,9.8094117542912,9.80941328476601,9.80941508958684,9.80941670335671,9.8094184363638,9.78261978913128,9.80942563145707,9.80998660733522,9.81005378719401,9.81005505320821,9.81005482889316,9.81005471426929,9.81005452728037,9.81005438929511,9.81005421939872,9.7671700641128,9.81000886168571,9.81010753958713,9.81006822194976,9.81006750863206,9.81006753421983,9.81006719236032,9.81006709640307,9.81006683744568,9.81006668808706,10.0053590653832,9.81020245245994,9.80950181145373,9.80950052530742,9.80950173265935,9.80950299793377,9.80950454751013,9.80950590064518,9.80950737785434,9.80950876548055,9.15958749040325,9.80941100996857,9.81123549048911,9.81117056629409,9.8111676169075,9.81116416527962,9.8111610648714,9.81115885194989,9.81115558848072,9.81115219928521,10.0319558118792,9.81118614069048,9.8100756617134,9.81005114440141,9.81005161707517,9.81005139240593,9.81005119981688,9.81005103278187,9.81005100300612,9.81005077746681,10.1278417074728,9.81000921927806,9.80970672745963,9.80977258143833,9.80977449940771,9.80977506946455,9.80977577247478,9.80977638372364,9.80977705302996,9.80977768052355,9.86697851596498,9.80971533021777,9.80947977638951,9.8094755649008,9.80947704777732,9.80947825757795,9.80947996303511,9.80948133080474,9.80948291647047,9.80948434966408,9.65196028900862,9.80950412273989,9.8095698419918,9.80943028547341,9.80943087815872,9.80943260030674,9.80943416288785,9.80943584593921,9.80943744081309,9.80943908663072,9.6366004408363,9.80941752122101,9.81102831621311,9.81126913229216,9.81126773531158,9.81126381293456,9.8112603411665,9.81125658655923,9.81125303802748,9.81124936957755,10.0063980472335,9.81140381288914,9.81032414464249,9.81028653512385,9.81028408360575,9.81028365382534,9.81028280732466,9.81028200968197,9.81028118339808,9.81028038012778,10.004287020625,9.810333879652,9.80929439832156,9.8092399981649,9.80923957892939,9.80924179844836,9.80924397010836,9.80924616313623,9.80924833141116,9.80925050571068,9.73784255612616,9.80930704473607,9.8091961092723,9.80909452772737,9.80909563423181,9.80909828379563,9.80910085991458,9.80910347246393,9.80910604822964,9.80910863605717,9.64027849661938,9.809111727132,9.80972184364071,9.80977029277312,9.80977077145045,9.80977124206054,9.80977190177403,9.80977256042274,9.80977321661105,9.80977387127852,9.58286439875468,9.80989891402548,9.81051406725781,9.8105291035264,9.81052783747156,9.81052655411489,9.81052487440856,9.81052346625239,9.81052188435871,9.81052042556491,10.0415566033278,9.81069361803657,9.80993490371547,9.80994221149884,9.80994209847313,9.80994235872699,9.80994246298876,9.8099426704436,9.80994280830372,9.80994299176328,9.62452316800121,9.81012752785892,9.81023288997725,9.81021007419593,9.81021065610912,9.81020966527639,9.81020938572799,9.81020856437742,9.81020810713663,9.81020741005599,9.8102070190683,9.81556048038803,9.80940007649157,9.80940227635448,9.80940325227063,9.80940497728641,9.80940669274785,9.80940840630883,9.80941011289062,9.80941173319003,9.80941348587724,9.80796279301982,9.80979990372413,9.80981105897372,9.80981115718961,9.80981184144723,9.80981228970941,9.80981289464649,9.80981339252524,9.80981395511121,9.80981447604992,9.80936456871057,9.80979434939081,9.80978453824343,9.80978539729253,9.80978589189995,9.80978659300699,9.8097871534725,9.8097878047325,9.80978841405242,9.80978902672455,9.81566485800174,9.8093808941907,9.80946187883259,9.80946404430165,9.80946547380101,9.80946709470699,9.80946858058004,9.80947014906803,9.8094714024891,9.80947311251564,9.8112473439141,9.80855690493594,9.8084816642439,9.80848273025064,9.80848724806847,9.8084913309671,9.80849587382595,9.80850008907783,9.80850415994133,9.80850864934918,9.80238864737432,9.81040838284508,9.81047063221081,9.81047121745409,9.8104698135411,9.81046848690477,9.81046711528068,9.81046578015963,9.81046450501809,9.81046311819181,9.81245154724983],\"type\":\"scatter\"},{\"line\":{\"color\":\"black\",\"width\":1},\"name\":\"C_v5.3_p1.5\",\"x\":[0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0,35.0,36.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,44.0,45.0,46.0,47.0,48.0,49.0,50.0,51.0,52.0,53.0,54.0,55.0,56.0,57.0,58.0,59.0,60.0,61.0,62.0,63.0,64.0,65.0,66.0,67.0,68.0,69.0,70.0,71.0,72.0,73.0,74.0,75.0,76.0,77.0,78.0,79.0,80.0,81.0,82.0,83.0,84.0,85.0,86.0,87.0,88.0,89.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0,100.0,101.0,102.0,103.0,104.0,105.0,106.0,107.0,108.0,109.0,110.0,111.0,112.0,113.0,114.0,115.0,116.0,117.0,118.0,119.0,120.0,121.0,122.0,123.0,124.0,125.0,126.0,127.0,128.0,129.0,130.0,131.0,132.0,133.0,134.0,135.0,136.0,137.0,138.0,139.0,140.0,141.0,142.0,143.0,144.0,145.0,146.0,147.0,148.0,149.0,150.0,151.0,152.0,153.0,154.0,155.0,156.0,157.0,158.0,159.0,160.0,161.0,162.0,163.0,164.0,165.0,166.0,167.0,168.0,169.0,170.0,171.0,172.0,173.0,174.0,175.0,176.0,177.0,178.0,179.0,180.0,181.0,182.0,183.0,184.0,185.0,186.0,187.0,188.0,189.0,190.0,191.0,192.0,193.0,194.0,195.0,196.0,197.0,198.0,199.0,200.0,201.0,202.0,203.0,204.0,205.0,206.0,207.0,208.0,209.0,210.0,211.0,212.0,213.0,214.0,215.0,216.0,217.0,218.0,219.0,220.0,221.0,222.0,223.0,224.0,225.0,226.0,227.0,228.0,229.0,230.0,231.0,232.0,233.0,234.0,235.0,236.0,237.0,238.0,239.0,240.0,241.0,242.0,243.0,244.0,245.0,246.0,247.0,248.0,249.0,250.0,251.0,252.0,253.0,254.0,255.0,256.0,257.0,258.0,259.0,260.0,261.0,262.0,263.0,264.0,265.0,266.0,267.0,268.0,269.0,270.0,271.0,272.0,273.0,274.0,275.0,276.0,277.0,278.0,279.0,280.0,281.0,282.0,283.0,284.0,285.0,286.0,287.0,288.0,289.0,290.0,291.0,292.0,293.0,294.0,295.0,296.0,297.0,298.0,299.0,300.0,301.0,302.0,303.0,304.0,305.0,306.0,307.0,308.0,309.0,310.0,311.0,312.0,313.0,314.0,315.0,316.0,317.0,318.0,319.0,320.0,321.0,322.0,323.0,324.0,325.0,326.0,327.0,328.0,329.0,330.0,331.0,332.0,333.0,334.0,335.0,336.0,337.0,338.0,339.0,340.0,341.0,342.0,343.0,344.0,345.0,346.0,347.0,348.0,349.0,350.0,351.0,352.0,353.0,354.0,355.0,356.0,357.0,358.0,359.0,360.0,361.0,362.0,363.0,364.0,365.0,366.0,367.0,368.0,369.0,370.0,371.0,372.0,373.0,374.0,375.0,376.0,377.0,378.0,379.0,380.0,381.0,382.0,383.0,384.0,385.0,386.0,387.0,388.0,389.0,390.0,391.0,392.0,393.0,394.0,395.0,396.0,397.0,398.0,399.0,400.0,401.0,402.0,403.0,404.0,405.0,406.0,407.0,408.0,409.0,410.0,411.0,412.0,413.0,414.0,415.0,416.0,417.0,418.0,419.0,420.0,421.0,422.0,423.0,424.0,425.0,426.0,427.0,428.0,429.0,430.0,431.0,432.0,433.0,434.0,435.0,436.0,437.0,438.0,439.0,440.0,441.0,442.0,443.0,444.0,445.0,446.0,447.0,448.0,449.0,450.0,451.0,452.0,453.0,454.0,455.0,456.0,457.0,458.0,459.0,460.0,461.0,462.0,463.0,464.0,465.0,466.0,467.0,468.0,469.0,470.0,471.0,472.0,473.0,474.0,475.0,476.0,477.0,478.0,479.0,480.0,481.0,482.0,483.0,484.0,485.0,486.0,487.0,488.0,489.0,490.0,491.0,492.0,493.0,494.0,495.0,496.0,497.0,498.0,499.0,500.0,501.0,502.0,503.0,504.0,505.0,506.0,507.0,508.0,509.0,510.0,511.0,512.0,513.0,514.0,515.0,516.0,517.0,518.0,519.0,520.0,521.0,522.0,523.0,524.0,525.0,526.0,527.0,528.0,529.0,530.0,531.0,532.0,533.0,534.0,535.0,536.0,537.0,538.0,539.0,540.0,541.0,542.0,543.0,544.0,545.0,546.0,547.0,548.0,549.0,550.0,551.0,552.0,553.0,554.0,555.0,556.0,557.0,558.0,559.0,560.0,561.0,562.0,563.0,564.0,565.0,566.0,567.0,568.0,569.0,570.0,571.0,572.0,573.0,574.0,575.0,576.0,577.0,578.0,579.0,580.0,581.0,582.0,583.0,584.0,585.0,586.0,587.0,588.0,589.0,590.0,591.0,592.0,593.0,594.0,595.0,596.0,597.0,598.0,599.0,600.0,601.0,602.0,603.0,604.0,605.0,606.0,607.0,608.0,609.0,610.0,611.0,612.0,613.0,614.0,615.0,616.0,617.0,618.0,619.0,620.0,621.0,622.0,623.0,624.0,625.0,626.0,627.0,628.0,629.0,630.0,631.0,632.0,633.0,634.0,635.0,636.0,637.0,638.0,639.0,640.0,641.0,642.0,643.0,644.0,645.0,646.0,647.0,648.0,649.0,650.0,651.0,652.0,653.0,654.0,655.0,656.0,657.0,658.0,659.0,660.0,661.0,662.0,663.0,664.0,665.0,666.0,667.0,668.0,669.0,670.0,671.0,672.0,673.0,674.0,675.0,676.0,677.0,678.0,679.0,680.0,681.0,682.0,683.0,684.0,685.0,686.0,687.0,688.0,689.0,690.0,691.0,692.0,693.0,694.0,695.0,696.0,697.0,698.0,699.0,700.0,701.0,702.0,703.0,704.0,705.0,706.0,707.0,708.0,709.0,710.0,711.0,712.0,713.0,714.0,715.0,716.0,717.0,718.0,719.0,720.0,721.0,722.0,723.0,724.0,725.0,726.0,727.0,728.0,729.0,730.0,731.0,732.0,733.0,734.0,735.0,736.0,737.0,738.0,739.0,740.0,741.0,742.0,743.0,744.0,745.0,746.0,747.0,748.0,749.0,750.0,751.0,752.0,753.0,754.0,755.0,756.0,757.0,758.0,759.0,760.0,761.0,762.0,763.0,764.0,765.0,766.0,767.0,768.0,769.0,770.0,771.0,772.0,773.0,774.0,775.0,776.0,777.0,778.0,779.0,780.0,781.0,782.0,783.0,784.0,785.0,786.0,787.0,788.0,789.0,790.0,791.0,792.0,793.0,794.0,795.0,796.0,797.0,798.0,799.0,800.0,801.0,802.0,803.0,804.0,805.0,806.0,807.0,808.0,809.0,810.0,811.0,812.0,813.0,814.0,815.0,816.0,817.0,818.0,819.0,820.0,821.0,822.0,823.0,824.0,825.0,826.0,827.0,828.0,829.0,830.0,831.0,832.0,833.0,834.0,835.0,836.0,837.0,838.0,839.0,840.0,841.0,842.0,843.0,844.0,845.0,846.0,847.0,848.0,849.0,850.0,851.0,852.0,853.0,854.0,855.0,856.0,857.0,858.0,859.0,860.0,861.0,862.0,863.0,864.0,865.0,866.0,867.0,868.0,869.0,870.0,871.0,872.0,873.0,874.0,875.0,876.0,877.0,878.0,879.0,880.0,881.0,882.0,883.0,884.0,885.0,886.0,887.0,888.0,889.0,890.0,891.0,892.0,893.0,894.0,895.0,896.0,897.0,898.0,899.0,900.0,901.0,902.0,903.0,904.0,905.0,906.0,907.0,908.0,909.0,910.0,911.0,912.0,913.0,914.0,915.0,916.0,917.0,918.0,919.0,920.0,921.0,922.0,923.0,924.0,925.0,926.0,927.0,928.0,929.0,930.0,931.0,932.0,933.0,934.0,935.0,936.0,937.0,938.0,939.0,940.0,941.0,942.0,943.0,944.0,945.0,946.0,947.0,948.0,949.0,950.0,951.0,952.0,953.0,954.0,955.0,956.0,957.0,958.0,959.0,960.0,961.0,962.0,963.0,964.0,965.0,966.0,967.0,968.0,969.0,970.0,971.0,972.0,973.0,974.0,975.0,976.0,977.0,978.0,979.0,980.0,981.0,982.0,983.0,984.0,985.0,986.0,987.0,988.0,989.0,990.0,991.0,992.0,993.0,994.0,995.0,996.0,997.0,998.0,999.0,1000.0],\"y\":[9.81035921436549,9.81035818342548,9.81035735299562,9.81035616495232,9.8103549808661,9.81035380071158,9.81035262448379,9.81035145216421,9.81035028374355,9.81034911920643,9.81192245635055,9.81238521538996,9.81238068982315,9.81237253050337,9.81236480596138,9.81235683533223,9.81234907238044,9.81234121445126,9.81233346316999,9.81232568397572,9.80943975514037,9.80905120061687,9.80905047484629,9.80905432363241,9.8090570062235,9.80906044882921,9.80906336736777,9.80906661795151,9.8090696298965,9.80907278371613,9.81105751537076,9.81123198908454,9.81122983577981,9.81122559803216,9.81122162286281,9.81121749524934,9.81121349179816,9.81120942803013,9.81120542685666,9.81120140626241,9.8104099406506,9.81009839423902,9.81009775050068,9.81009749154745,9.81009712341603,9.81009682986788,9.81009648838937,9.81009618065031,9.81009585219822,9.81009553932884,9.80958717023003,9.80950999015789,9.80950945834808,9.80951079134411,9.80951237627826,9.80951402505289,9.80951562227296,9.80951724489839,9.80951884165239,9.80952044674918,9.80900361571056,9.80898868903994,9.80899226296037,9.80899526395968,9.80899883710796,9.80900201029315,9.80900543165157,9.80900866915374,9.80901201088051,9.80901526483896,9.80984439607383,9.81008434324491,9.81008523247689,9.81008482870885,9.81008462696344,9.81008429210189,9.81008404754487,9.81008374434004,9.8100834817838,9.81008319367339,9.81049441629047,9.81052517094614,9.81052228652959,9.81052118883171,9.81051903033117,9.81051758864134,9.81051567865476,9.81051409041115,9.81051229718565,9.81051065009657,9.8102650051836,9.81017955346493,9.81017973392701,9.81017921327291,9.81017863363471,9.8101780289806,9.81017744429866,9.81017684958589,9.81017626483336,9.81017567670281,9.81036855600007,9.81043430552179,9.81043371279236,9.81043198963313,9.81043074096631,9.81042918394163,9.81042784043276,9.81042636249753,9.81042498207105,9.8104235445037,9.81040209130602,9.81040552704958,9.81040487985769,9.81040348181643,9.81040217508443,9.81040081492247,9.81039949779861,9.81039815937615,9.81039684252165,9.8103955186329,9.80977827256658,9.80975483099209,9.80975510055935,9.80975563800021,9.80975644859063,9.80975725949273,9.8097580656946,9.80975887055244,9.80975967184366,9.80976047106483,9.81100371094991,9.81115756012244,9.81115813737274,9.81115408900452,9.81115038422802,9.81114647167974,9.81114271886426,9.81113888070852,9.81113512052404,9.81113132936643,9.80945747704512,9.80934166532691,9.80934621142021,9.80934749211434,9.80935025665148,9.80935201993061,9.80935443872574,9.8093564085607,9.80935866579268,9.80936071955649,9.80791572115658,9.80775607761261,9.80775742473176,9.80776515234444,9.80777240084458,9.80777992752782,9.80778722766734,9.80779463788446,9.80780193390106,9.80780926537158,9.8083322919631,9.80840359044603,9.80840973070743,9.80841379392938,9.80841912517272,9.80842434232925,9.80842960638294,9.80843481010809,9.80844002508513,9.80844520369007,9.81116831842253,9.81183080175267,9.81182763376925,9.81182118652662,9.81181537934794,9.81180917905568,9.81180327432327,9.81179720592364,9.81179127989729,9.81178529210661,9.80987069930369,9.80952154531874,9.80951999475415,9.80952212613127,9.80952335779853,9.80952518045371,9.80952660032759,9.80952827996222,9.80952977769202,9.80953138798974,9.80901481902632,9.80900346272391,9.80900518722358,9.80900788536523,9.80901116469555,9.80901446480198,9.80901773281809,9.80902100404881,9.80902425501795,9.80902750143654,9.80912591130679,9.80915592248679,9.80915900875043,9.80916145361795,9.80916447544441,9.80916709716655,9.80916997021138,9.80917266034553,9.80917545710887,9.80917816752577,9.80927758032029,9.80927695347546,9.80927883180473,9.80928120007966,9.80928360929235,9.80928597795468,9.80928836043614,9.80929072053433,9.80929308242797,9.80929543004155,9.80948255243193,9.80956446664722,9.80956678092159,9.8095677015798,9.80956948517964,9.80957068551487,9.80957226674948,9.80957358613388,9.80957507219413,9.80957643927242,9.80831622902488,9.80963495313221,9.80956867350696,9.80957450652899,9.80957421404992,9.8095767676341,9.80957741600149,9.8095793266982,9.80958038805279,9.80958200787212,9.80986256852413,9.80986407356792,9.80986436925723,9.80986485080064,9.80986527969004,9.80986574118896,9.80986617846335,9.80986662941083,9.80986706877456,9.80986751340064,9.81127187315875,9.81157952760292,9.81157783359324,9.81157226266078,9.81156725248509,9.81156189747539,9.81155680126241,9.81155156132964,9.81154644592559,9.81154127612238,9.81034283997551,9.81010757153574,9.81010505327896,9.81010499082627,9.81010445057709,9.81010423079341,9.81010379929413,9.81010351086025,9.81010312896472,9.81010281128586,9.81116716023078,9.81131016917391,9.81131015696477,9.81130568104866,9.81130142039672,9.81129704032038,9.81129276386528,9.81128844225452,9.81128417459315,9.81127989473186,9.8109187885676,9.81075793815494,9.81075686151243,9.8107543506625,9.81075183919994,9.81074934206027,9.81074684924021,9.81074436736249,9.81074189196602,9.81073942597907,9.81032361601611,9.81029666426129,9.81029725229076,9.81029567078843,9.8102950833296,9.81029383863333,9.81029303753544,9.8102919461273,9.8102910536633,9.81029003395587,9.80962625398149,9.80936183188473,9.80936290603301,9.80936506036818,9.80936714706997,9.80936926714853,9.80937135330269,9.80937345043871,9.80937552865777,9.80937760793096,9.80950870776641,9.80950947761485,9.80951072604763,9.80951234276457,9.8095139706068,9.80951558203709,9.80951719544349,9.8095187985971,9.80952039968023,9.80952199326762,9.80975128792442,9.80975117913375,9.80975190680353,9.80975271595002,9.80975354849381,9.80975436087821,9.80975518215616,9.80975599297414,9.80975680624958,9.80975761338587,9.80890549126067,9.80873199682952,9.80873283612442,9.80873715610035,9.8087412830094,9.80874551533491,9.80874965416569,9.80875383218602,9.80875796101676,9.808762099654,9.81144192592707,9.81219303560096,9.81218847096632,9.81218095026428,9.81217386292357,9.81216652690618,9.81215939677038,9.81215216934402,9.812145046609,9.8121378937783,9.81105270786216,9.81096139304117,9.81095457946256,9.81095164524849,9.81094975562083,9.81094659791828,9.81094345103026,9.81094031439318,9.81093718832472,9.81093407255524,9.80950188995957,9.80925538033427,9.80925455585115,9.80925738721269,9.80925962067256,9.80926223903296,9.80926458713795,9.80926710180356,9.80926949186436,9.80927195147573,9.80900078640312,9.8089755911498,9.80897907139968,9.80898215244637,9.80898574505701,9.80898897785245,9.80899243181455,9.80899571968427,9.80899909969629,9.80900239975595,9.80975106033346,9.80983701385714,9.80984044316882,9.8098405902942,9.80984115361032,9.80984165947008,9.8098422007038,9.80984271543435,9.80984324492285,9.80984376167158,9.81052903144181,9.81075428303686,9.81075337341402,9.81075080406661,9.81074835112414,9.81074583442793,9.81074337403772,9.81074088986781,9.81073843526292,9.81073597461431,9.80987868137015,9.80970340396232,9.80970221659214,9.80970331242481,9.80970422822093,9.80970525856803,9.80970620709262,9.80970720472851,9.80970816420455,9.80970914371954,9.80885264457374,9.80869096822601,9.80869172438395,9.80869616611369,9.80870043924588,9.80870480072567,9.80870907933607,9.80871338930172,9.80871765455219,9.80872192588025,9.80906987357775,9.80910866894582,9.80911000103776,9.8091134316576,9.8091160645865,9.80911921295253,9.80912200139135,9.8091250135334,9.80912786034573,9.80913080123748,9.80984293722431,9.81002504136232,9.81002620834241,9.81002605500941,9.81002601252281,9.8100258966197,9.81002583014043,9.81002573118983,9.81002565436309,9.81002556326184,9.81109510045687,9.81121836390454,9.81121881567535,9.81121470743485,9.81121070655836,9.81120665651236,9.81120266157807,9.81119865216039,9.81119467458205,9.81119069788751,9.81012280236965,9.80996633272672,9.80996384132184,9.80996410431546,9.8099641284644,9.80996431117995,9.80996438752351,9.80996453412293,9.80996463322853,9.80996476334228,9.8108422581486,9.81093800866393,9.81093799456286,9.81093458965107,9.81093204787321,9.81092977606497,9.8109266851494,9.81092360421984,9.81092053372628,9.81091747331186,9.80974290085367,9.80946067604023,9.80946008218824,9.8094621521812,9.80946375694611,9.80946566194007,9.80946735689132,9.80946918201392,9.80947091053127,9.80947269366042,9.8098513736028,9.8099748714185,9.80997514465742,9.80997518387841,9.80997526697543,9.80997534880532,9.80997543102375,9.80997551252845,9.80997559405578,9.80997567511633,9.8100915160846,9.81014149184032,9.81014236187476,9.81014132116931,9.8101412296169,9.81014050790676,9.81014020890405,9.81013963069,9.81013924120185,9.81013872847285,9.8084686552208,9.8083491740889,9.80835514804192,9.80836005859997,9.8083658825642,9.80837106734526,9.80837664810331,9.80838193483184,9.808387387635,9.80839269987623,9.80778609764797,9.80777104381262,9.80777529305004,9.80778268608816,9.80779005974331,9.80779740542064,9.80780472898602,9.8078120266644,9.80781930110754,9.80782655068222,9.80991718234678,9.81052550286764,9.81052532652803,9.81052328126654,9.81052174078092,9.81051987343253,9.81051823361503,9.81051645170066,9.81051477407605,9.81051303645166,9.80999066455054,9.80995219727453,9.80995422436181,9.80995354245541,9.8099542529659,9.80995403434769,9.80995443430069,9.80995442102774,9.80995468239645,9.8099547598312,9.51479189553239,9.8119048433464,9.81190418263074,9.81189903576802,9.81189292525542,9.81188649683777,9.81188031527158,9.81187400393643,9.81186781379692,8.90803466344648,9.63181410598091,9.81417788745921,9.81417572303776,9.81416551617753,9.81415224593993,9.81413805608594,9.81412455589619,9.81411067226232,9.81409712033614,11.2555505331838,10.0997043513819,9.8080020706433,9.80800571184655,9.80801060522584,9.80801689307847,9.80802370335198,9.8080301287676,9.80803677429944,9.80804323675262,8.85922683573849,9.59013970558217,9.81170365739299,9.81169732139706,9.81169360915739,9.81168830352381,9.81168247409693,9.81167702500506,9.81167135339247,9.8116658610598,10.2921303706517,9.91320483275334,9.81091765450311,9.81091683885573,9.81091459065431,9.81091161426396,9.81090854074466,9.81090554879286,9.81090251922168,9.81089953143331,10.0630323187115,9.86285527293291,9.80970848088248,9.80970992894639,9.80971080616762,9.8097117118759,9.80971260514788,9.80971360404526,9.809714527225,9.80971549560008,10.0388632832639,9.86396791657577,9.80914100169257,9.80914508918608,9.80914644984317,9.80914868372356,9.80915125242374,9.80915424849353,9.80915694399647,9.80915982427557,9.34741962053375,9.7110404672363,9.81213714489654,9.81212923827408,9.8121243509158,9.81211729007134,9.81211024991423,9.81210323502034,9.81209624220928,9.81208927347191,9.62011700537396,9.77000312248025,9.81148758486086,9.8114815193843,9.81147826819029,9.81147344175311,9.81146848445617,9.81146364157703,9.81145874950696,9.81145391722839,9.95441854058882,9.84415264402482,9.80853217528814,9.80853815000935,9.80854113388661,9.80854604210137,9.80855088652228,9.80855569706887,9.8085605034684,9.80856528599157,9.69440585764715,9.78554292889276,9.81089110636688,9.81088842545201,9.81088639979307,9.81088346458643,9.81088051967775,9.8108775975331,9.81087467645426,9.8108717708527,9.81204417594056,9.81427802063947,9.81221165902147,9.81220432374127,9.81219717770455,9.8121898164215,9.81218257402811,9.81217529276801,9.81216807767773,9.81216085859917,10.0811679055734,9.8763869588008,9.80850834246042,9.80851389386699,9.80851720609638,9.80852207765663,9.80852700322562,9.8085318921888,9.80853677838537,9.80854163934618,9.19781716709394,9.6826750207874,9.81198268033946,9.81198187406757,9.8119771031705,9.81197070659629,9.81196403727209,9.81195758613089,9.8119510257632,9.81194457432157,10.5994108149698,9.97523520542896,9.80732524957839,9.80732979692874,9.8073365818796,9.80734525656355,9.80735420239125,9.80736291848928,9.80737173893604,9.80738044117486,10.4998066920553,9.9730479161119,9.80646459076929,9.80647254648016,9.80648099503231,9.80649215799698,9.8065041774377,9.80651556119388,9.80652730425435,9.8065387435076,9.55926777989839,9.75476225900468,9.80673087582384,9.80674104342641,9.8067497920167,9.80676051539223,9.80677092108971,9.80678189909252,9.80679243599864,9.80680320760742,8.32416626808719,9.5214017417296,9.81166798193991,9.81167407119671,9.81167009445676,9.81166508383202,9.81165918704492,9.81165391174229,9.81164825272415,9.81164288002206,10.8071864049114,10.0268876064932,9.80979746170296,9.80979980139271,9.8098000782786,9.8098006650451,9.80980137966719,9.80980200537593,9.80980268669617,9.80980332729019,10.0379630427238,9.8676627043349,9.81044657937961,9.8104462689985,9.8104452087226,9.81044381059887,9.81044201239541,9.81044075805641,9.81043914929669,9.81043778494594,9.80057440379773,9.79615097220942,9.80915509872876,9.8091580004591,9.80915952327683,9.80916203835084,9.80916501028858,9.80916766219581,9.8091705120534,9.80917321459197,9.80167886762268,9.79970723576595,9.80852370270444,9.80852778819495,9.80853036647614,9.80853487087201,9.80853999608979,9.80854468040384,9.8085496317211,9.80855437819313,9.69344529850709,9.78525206701946,9.81231660098915,9.81230963370588,9.81230381948319,9.81229615186865,9.81228852382097,9.8122809117495,9.81227333124053,9.81226577176285,9.81295635616861,9.81059806046674,9.80844825198772,9.80845094343266,9.80845615593358,9.80846124334356,9.80846628777602,9.80847143417985,9.80847648431296,9.80848157043079,9.68992206198766,9.78376686983729,9.81077790320509,9.81077548963699,9.81077355163287,9.81077096908815,9.81076841318184,9.8107658537377,9.81076331082595,9.81076077101933,9.07393272099284,9.63646223922773,9.81340319837977,9.81339818938348,9.81338974864083,9.81337889656015,9.81336738502899,9.81335637545443,9.81334509335781,9.81333405485173,10.4308140749997,9.96086197174075,9.81078908713683,9.81078765329181,9.81078572267655,9.81078316576228,9.810780525245,9.81077795490866,9.81077535218261,9.81077278539941,9.2764198682401,9.69922999296825,9.81220676921172,9.81219991687689,9.8121948361681,9.81218788954268,9.81218038230658,9.81217328916542,9.81216596018129,9.81215882851242,10.0312063078379,9.85416585699995,9.80979636349855,9.80979690002474,9.8097984670905,9.80979862382486,9.80979939505099,9.80979999414576,9.80980070429373,9.80980133671749,10.0001335151379,9.85138931467797,9.8110772911985,9.81107558590831,9.81107320390674,9.81106971248468,9.81106610516859,9.81106259484616,9.81105903952669,9.81105553380527,10.2117481409916,9.89033516122133,9.80896908773763,9.80897425228534,9.80897627302791,9.80897947686864,9.80898300271391,9.80898629506674,9.80898972432102,9.80899304360986,9.79870511798839,9.79799790085441,9.80758405508799,9.8075899755141,9.8075967210878,9.80760455009482,9.8076126211078,9.807620486599,9.80762844506714,9.80763629765948,9.74719782894989,9.78883665108764,9.81229985085631,9.81229233382587,9.81228678052027,9.81227917253782,9.81227159894872,9.81226404447161,9.8122565191543,9.81224901615894,10.2663058577724,9.89904481865229,9.80918034707237,9.8091850039879,9.80918636829316,9.80918859487099,9.80919161135252,9.80919408626821,9.80919690731418,9.80919948274259,8.31474376993788,9.47209684673714,9.81366970438381,9.81366626384343,9.81365710179946,9.81364554531401,9.8136330243451,9.81362121360522,9.81360899639583,9.81359711695982,10.3066970705532,9.90934654866918,9.81032570762392,9.81032714328216,9.8103263586724,9.81032530508373,9.81032419250447,9.81032321535825,9.81032207371254,9.81032104769433,10.5407638020377,9.96518983359464,9.80790833748122,9.80791396833008,9.80791896369359,9.80792562175178,9.80793269176933,9.80793944888594,9.80794637646903,9.80795315240042,9.93337117875474,9.83560332909738,9.8077638152307,9.8077703637801,9.80777558537621,9.80778289124721,9.80779032168381,9.8077976281813,9.80780497654544,9.80781225637456,9.45214980118295,9.72823358995712,9.80958743277418,9.80959213896997,9.80959335149786,9.80959463218065,9.80959600381268,9.80959733094238,9.80959868028815,9.80960000739583,9.41142090272088,9.71847927122069,9.81035193149616,9.81034878017786,9.81034820848696,9.8103471038712,9.81034591455829,9.81034478811232,9.81034362613594,9.81034249420617,10.2457670024461,9.90676933977049,9.80850133870438,9.80850760343712,9.80851100117476,9.80851590478046,9.80852086983277,9.8085257665459,9.80853068153472,9.80853555714668,10.2432546385914,9.89811272953931,9.80714453532814,9.80714835410936,9.80715524484148,9.8071645434782,9.80717407603725,9.80718340034738,9.8071928113633,9.80720211262184,9.64835804769639,9.77473164944077,9.80750394271891,9.80751074151305,9.80751648262888,9.80752466530424,9.80753294395751,9.80754111296686,9.80754930956531,9.80755744241547,9.43169518828079,9.72960185962227,9.81121416578591,9.81121223518868,9.81120918757636,9.81120516238431,9.81120115921464,9.81119716359343,9.81119318509606,9.81118921726529,9.28671531254551,9.68966090722551,9.81118977505313,9.81118796911697,9.81118530251426,9.8111815968117,9.81117751193211,9.81117370163081,9.81116972999846,9.81116588756908,10.3485322007519,9.9270898694856,9.81056057872737,9.81055847500475,9.81055705110878,9.8105551889976,9.81055335001947,9.81055150586022,9.8105496753619,9.81054784592922,9.39640182020824],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"rangeslider\":{\"visible\":true}},\"yaxis\":{\"autorange\":true,\"fixedrange\":false}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('c3dc15d2-a470-490f-be8a-fad01275096f');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Création du dataset d'entrainement et de test"
      ],
      "metadata": {
        "id": "7F1y38Hx7NR2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1. Création des datasets global (avec les données d'entrainement et de test)**"
      ],
      "metadata": {
        "id": "h1Oklb3NbWOE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le dataset d'entrainement contient des milliers de séquences de 30 secondes, chacune étant identifiée comme appartenant à un grade spécifique de la route (A,B,C,D,E,F,G ou H) : "
      ],
      "metadata": {
        "id": "QkbtvlAz7UEM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://github.com/AlexandreBourrieau/FICHIERS/raw/main/VibrationRoute/ExtractionAccel.jpg\" width=800></center>"
      ],
      "metadata": {
        "id": "oWMZgNRg696X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://github.com/AlexandreBourrieau/FICHIERS/raw/main/VibrationRoute/GradesRoutes.jpg\" width=600></center>"
      ],
      "metadata": {
        "id": "_XOKlxe88H3H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour coder les labels, on utilise le format one_hot avec la fonction [to_categorical](https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical):"
      ],
      "metadata": {
        "id": "qzObEX_zelC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "grades = [0,1,2,3,4,5,6,7]\n",
        "\n",
        "codes_one_hot = to_categorical(grades)\n",
        "print(codes_one_hot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ypJxOoveuWN",
        "outputId": "46a9a7df-f243-4dad-8ed6-6c033fa2af05"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La structure du dataset est la suivante (si batch_size = 1) :"
      ],
      "metadata": {
        "id": "M9n01VSOXMkP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://github.com/AlexandreBourrieau/FICHIERS/raw/main/VibrationRoute/ContenuDataset2.jpg\" width=900></center>"
      ],
      "metadata": {
        "id": "5FSv_gGjXO8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extraction des quantités annexes à partir des données d'accélération\n",
        "def get_annexes(serie):\n",
        "  # Variance\n",
        "  variance = tf.math.reduce_variance(serie)\n",
        "  # Ecart type\n",
        "  std = tf.math.reduce_std(serie)\n",
        "  # max\n",
        "  max = tf.math.reduce_max(serie)\n",
        "  # min\n",
        "  min = tf.math.reduce_min(serie)\n",
        "\n",
        "  return tf.convert_to_tensor([variance, std, max, min], dtype=tf.float32)"
      ],
      "metadata": {
        "id": "8RzRn4r1cO9J"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonction permettant de créer un dataset à partir des données de la série temporelle\n",
        "# au format X(X1,X2,...X300), Y, Z\n",
        "# X sont les échantillons aux pas de 0.1s (100 échantillons = 10 secondes)\n",
        "# Y sont les entrées annexes (vitesse et quantités extraites des données d'accélération)\n",
        "# Z sont les labels (grades de la route) au format one_hot : A=0, B=1, ..., H=7\n",
        "\n",
        "def prepare_dataset_XY(serie, label, vitesse, taille_fenetre, buffer_melange):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(serie)\n",
        "  dataset = dataset.window(taille_fenetre, shift=taille_fenetre, drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda x: x.batch(taille_fenetre))\n",
        "  dataset = dataset.map(lambda x: (x/tf.math.reduce_mean(x), tf.concat([vitesse,get_annexes(x)],0), codes_one_hot[label]))\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "Jhwag9Vz854O"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_label(grade_route):\n",
        "    if grade_route == 'A':\n",
        "      label = 0\n",
        "    elif grade_route == 'B':\n",
        "      label = 1\n",
        "    elif grade_route == 'C':\n",
        "      label = 2\n",
        "    elif grade_route == 'D':\n",
        "      label = 3\n",
        "    elif grade_route == 'E':\n",
        "      label = 4\n",
        "    elif grade_route == 'F':\n",
        "      label = 5\n",
        "    elif grade_route == 'G':\n",
        "      label = 6\n",
        "    elif grade_route == 'H':\n",
        "      label = 7\n",
        "    return tf.convert_to_tensor(label)"
      ],
      "metadata": {
        "id": "uodHIqVtIt3X"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "taille_fenetre = 100*10\n",
        "batch_size = 32\n",
        "ratio_entrainement = 0.8\n",
        "\n",
        "# Création du dataset initial\n",
        "serie = df[df.columns[1]].values\n",
        "\n",
        "grade_route = df.columns[1].split(\"_\")[0]\n",
        "vitesse = tf.convert_to_tensor([float(df.columns[1].split(\"_\")[1].split(\"v\")[1])])\n",
        "label = get_label(grade_route)\n",
        "dataset = prepare_dataset_XY(serie,label,vitesse, taille_fenetre,10000)\n",
        "\n",
        "# Concaténation avec les datasets suivants\n",
        "from tqdm import tqdm\n",
        "for i in tqdm(range(len(df.columns))):\n",
        "  colonne = df.columns[i]\n",
        "  if i > 1:\n",
        "    # Extraction des labels\n",
        "    grade_route = colonne.split(\"_\")[0]\n",
        "    vitesse = tf.convert_to_tensor([float(colonne.split(\"_\")[1].split(\"v\")[1])])\n",
        "    label = get_label(grade_route)\n",
        "    \n",
        "    # Création de la série\n",
        "    serie = df[colonne].values\n",
        "\n",
        "    # Création du dataset\n",
        "    dataset_ = prepare_dataset_XY(serie,label,vitesse,taille_fenetre,10000)\n",
        "    dataset = dataset.concatenate(dataset_)\n",
        "\n",
        "# Création des batchs\n",
        "dataset = dataset.batch(batch_size,drop_remainder=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3NR_n7JGEgN",
        "outputId": "0acc90f7-c9e4-4ff3-9112-b82b22fa84e4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 695/695 [00:33<00:00, 21.01it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2. Séparation des données d'entrainement et de tests**"
      ],
      "metadata": {
        "id": "M6b0F1HAbeNf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Format des dataset : [(serie,annexes,label), ...]"
      ],
      "metadata": {
        "id": "u0AsmQenN950"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ratio_entrainement = 0.8\n",
        "dataset_ent, dataset_test = tf.keras.utils.split_dataset(dataset, left_size=ratio_entrainement)"
      ],
      "metadata": {
        "id": "zgt6WQHJMPq7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.3. Réorganisation des datasets**"
      ],
      "metadata": {
        "id": "vCe9qaGnMWN6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Format final des dataset : [((serie,annexes),label), ...]"
      ],
      "metadata": {
        "id": "CI4lDofoOD-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_ent = dataset_ent.map(lambda x,y,z : ((x,y),z))\n",
        "dataset_test = dataset_test.map(lambda x,y,z : ((x,y),z))"
      ],
      "metadata": {
        "id": "BZxvSshlMcpa"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "for element in dataset_ent:\n",
        "  i=i+1\n",
        "  if i > 1:\n",
        "    break\n",
        "  print(element)"
      ],
      "metadata": {
        "id": "BO4CpuNUxxWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Mise en place du modèle"
      ],
      "metadata": {
        "id": "xvc64tvUXqNn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.1. Création du modèle à base de réseaux de convolution**"
      ],
      "metadata": {
        "id": "v2feDVufaf8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://github.com/AlexandreBourrieau/FICHIERS/raw/main/VibrationRoute/StructureModele.jpg\" width=600></center>"
      ],
      "metadata": {
        "id": "vN1ZYnq_vovm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers.merging.concatenate import concatenate\n",
        "\n",
        "entree_serie = tf.keras.layers.Input(shape=(taille_fenetre), name=\"First\")\n",
        "entree_annexes = tf.keras.layers.Input(shape=(5), name=\"Second\")\n",
        "\n",
        "# Réseau de convolution\n",
        "conv1 = keras.layers.Conv1D(filters=64, kernel_size=7, padding=\"same\")(tf.expand_dims(entree_serie,axis=-1))\n",
        "conv1 = keras.layers.BatchNormalization()(conv1)\n",
        "conv1 = keras.layers.ReLU()(conv1)\n",
        "conv1 = keras.layers.MaxPool1D()(conv1)\n",
        "\n",
        "conv2 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(conv1)\n",
        "conv2 = keras.layers.BatchNormalization()(conv2)\n",
        "conv2 = keras.layers.ReLU()(conv2)\n",
        "\n",
        "conv3 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(conv2)\n",
        "conv3 = keras.layers.BatchNormalization()(conv3)\n",
        "\n",
        "conv4 = keras.layers.Add()([conv1,conv3])\n",
        "conv4 = keras.layers.ReLU()(conv4)\n",
        "conv4 = keras.layers.MaxPool1D()(conv4)\n",
        "conv4 = keras.layers.Flatten()(conv4)\n",
        "\n",
        "# Prise en compte des entrées annexes\n",
        "concat = keras.layers.concatenate([conv4,entree_annexes])\n",
        "\n",
        "# Construction du modèle\n",
        "output_layer = keras.layers.Dense(1024, activation=\"relu\")(concat)\n",
        "output_layer = keras.layers.Dense(512, activation=\"relu\")(concat)\n",
        "output_layer = keras.layers.Dense(8, activation=\"softmax\")(output_layer)\n",
        "\n",
        "\n",
        "model = tf.keras.Model(inputs=[entree_serie,entree_annexes],outputs=output_layer)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElFAE8xcuzbA",
        "outputId": "a013365e-1ee7-4dfe-a5cf-1cc7f506747c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " First (InputLayer)             [(None, 1000)]       0           []                               \n",
            "                                                                                                  \n",
            " tf.expand_dims_2 (TFOpLambda)  (None, 1000, 1)      0           ['First[0][0]']                  \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, 1000, 64)     512         ['tf.expand_dims_2[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 1000, 64)    256         ['conv1d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " re_lu_3 (ReLU)                 (None, 1000, 64)     0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " max_pooling1d_2 (MaxPooling1D)  (None, 500, 64)     0           ['re_lu_3[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)              (None, 500, 64)      12352       ['max_pooling1d_2[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 500, 64)     256         ['conv1d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " re_lu_4 (ReLU)                 (None, 500, 64)      0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_5 (Conv1D)              (None, 500, 64)      12352       ['re_lu_4[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 500, 64)     256         ['conv1d_5[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 500, 64)      0           ['max_pooling1d_2[0][0]',        \n",
            "                                                                  'batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " re_lu_5 (ReLU)                 (None, 500, 64)      0           ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " max_pooling1d_3 (MaxPooling1D)  (None, 250, 64)     0           ['re_lu_5[0][0]']                \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)            (None, 16000)        0           ['max_pooling1d_3[0][0]']        \n",
            "                                                                                                  \n",
            " Second (InputLayer)            [(None, 5)]          0           []                               \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 16005)        0           ['flatten_1[0][0]',              \n",
            "                                                                  'Second[0][0]']                 \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 512)          8195072     ['concatenate_2[0][0]']          \n",
            "                                                                                                  \n",
            " dense_8 (Dense)                (None, 8)            4104        ['dense_7[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 8,225,160\n",
            "Trainable params: 8,224,776\n",
            "Non-trainable params: 384\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.2. Définition de l'optimiseur et de la fonction d'erreur**"
      ],
      "metadata": {
        "id": "C0OXocVCaoA_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On utilise l'optimiseur Adam ainsi qu'une fonction d'erreur de type [CategoricalCrossentropy](https://keras.io/api/losses/probabilistic_losses/#categoricalcrossentropy-class).\n",
        "\n",
        "L'ensemble des fonctions d'erreur utilisables avec Keras se trouve ici : https://keras.io/api/losses/"
      ],
      "metadata": {
        "id": "IaGCpmr4Z0Rs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La fonction d'erreur que nous utilisons permet de gérer les labels multiples qui ont été codés en représentation \"one_hot\"."
      ],
      "metadata": {
        "id": "iE2Cy-SyaNpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=optimiseur, metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "N26CL-8ZYkEo"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.3. Entrainement du modèle**"
      ],
      "metadata": {
        "id": "MjmSeurSa4pk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3EKJU0By3HA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "historique = model.fit(dataset_ent, validation_data = dataset_test, epochs=300, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "8wp4KC-Ca-Hw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "abab317f-b805-4a77-b7af-aad70138e6d7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "17/17 [==============================] - 10s 382ms/step - loss: 6.4932 - accuracy: 0.3996 - val_loss: 2.4869 - val_accuracy: 0.3507\n",
            "Epoch 2/300\n",
            "17/17 [==============================] - 5s 271ms/step - loss: 1.0465 - accuracy: 0.6729 - val_loss: 1.8543 - val_accuracy: 0.3881\n",
            "Epoch 3/300\n",
            "17/17 [==============================] - 5s 311ms/step - loss: 0.5877 - accuracy: 0.7695 - val_loss: 2.8295 - val_accuracy: 0.2090\n",
            "Epoch 4/300\n",
            "17/17 [==============================] - 5s 307ms/step - loss: 0.4918 - accuracy: 0.8197 - val_loss: 3.9587 - val_accuracy: 0.1343\n",
            "Epoch 5/300\n",
            "17/17 [==============================] - 6s 374ms/step - loss: 0.3618 - accuracy: 0.8792 - val_loss: 4.7667 - val_accuracy: 0.1269\n",
            "Epoch 6/300\n",
            "17/17 [==============================] - 5s 278ms/step - loss: 0.3111 - accuracy: 0.8699 - val_loss: 5.4715 - val_accuracy: 0.1343\n",
            "Epoch 7/300\n",
            "17/17 [==============================] - 6s 349ms/step - loss: 0.2710 - accuracy: 0.9182 - val_loss: 5.8120 - val_accuracy: 0.1269\n",
            "Epoch 8/300\n",
            "17/17 [==============================] - 5s 286ms/step - loss: 0.1807 - accuracy: 0.9405 - val_loss: 5.8860 - val_accuracy: 0.1269\n",
            "Epoch 9/300\n",
            "17/17 [==============================] - 5s 271ms/step - loss: 0.1713 - accuracy: 0.9349 - val_loss: 5.8118 - val_accuracy: 0.1493\n",
            "Epoch 10/300\n",
            "17/17 [==============================] - 6s 337ms/step - loss: 0.1417 - accuracy: 0.9591 - val_loss: 5.9633 - val_accuracy: 0.1791\n",
            "Epoch 11/300\n",
            "17/17 [==============================] - 6s 348ms/step - loss: 0.1384 - accuracy: 0.9535 - val_loss: 7.0191 - val_accuracy: 0.2239\n",
            "Epoch 12/300\n",
            "17/17 [==============================] - 5s 279ms/step - loss: 0.1865 - accuracy: 0.9461 - val_loss: 6.8806 - val_accuracy: 0.2090\n",
            "Epoch 13/300\n",
            "17/17 [==============================] - 8s 484ms/step - loss: 0.1394 - accuracy: 0.9591 - val_loss: 5.8832 - val_accuracy: 0.2090\n",
            "Epoch 14/300\n",
            "17/17 [==============================] - 5s 320ms/step - loss: 0.1231 - accuracy: 0.9610 - val_loss: 5.4075 - val_accuracy: 0.2239\n",
            "Epoch 15/300\n",
            "17/17 [==============================] - 11s 639ms/step - loss: 0.0869 - accuracy: 0.9740 - val_loss: 5.7430 - val_accuracy: 0.2239\n",
            "Epoch 16/300\n",
            "17/17 [==============================] - 5s 313ms/step - loss: 0.0701 - accuracy: 0.9814 - val_loss: 6.1555 - val_accuracy: 0.2239\n",
            "Epoch 17/300\n",
            "17/17 [==============================] - 6s 351ms/step - loss: 0.0901 - accuracy: 0.9796 - val_loss: 7.0402 - val_accuracy: 0.2537\n",
            "Epoch 18/300\n",
            "17/17 [==============================] - 5s 280ms/step - loss: 0.1803 - accuracy: 0.9758 - val_loss: 6.2152 - val_accuracy: 0.1791\n",
            "Epoch 19/300\n",
            "17/17 [==============================] - 5s 274ms/step - loss: 0.1546 - accuracy: 0.9610 - val_loss: 5.3149 - val_accuracy: 0.3507\n",
            "Epoch 20/300\n",
            "17/17 [==============================] - 6s 346ms/step - loss: 0.0586 - accuracy: 0.9796 - val_loss: 4.6233 - val_accuracy: 0.2910\n",
            "Epoch 21/300\n",
            "17/17 [==============================] - 5s 283ms/step - loss: 0.0969 - accuracy: 0.9870 - val_loss: 3.4540 - val_accuracy: 0.3955\n",
            "Epoch 22/300\n",
            "17/17 [==============================] - 6s 355ms/step - loss: 0.0316 - accuracy: 0.9944 - val_loss: 2.3579 - val_accuracy: 0.4254\n",
            "Epoch 23/300\n",
            "17/17 [==============================] - 5s 275ms/step - loss: 0.0273 - accuracy: 0.9944 - val_loss: 2.0159 - val_accuracy: 0.5970\n",
            "Epoch 24/300\n",
            "17/17 [==============================] - 6s 324ms/step - loss: 0.0479 - accuracy: 0.9796 - val_loss: 1.6078 - val_accuracy: 0.6119\n",
            "Epoch 25/300\n",
            "17/17 [==============================] - 5s 269ms/step - loss: 0.0347 - accuracy: 0.9888 - val_loss: 1.0915 - val_accuracy: 0.6418\n",
            "Epoch 26/300\n",
            "17/17 [==============================] - 6s 353ms/step - loss: 0.0243 - accuracy: 0.9963 - val_loss: 0.8731 - val_accuracy: 0.7537\n",
            "Epoch 27/300\n",
            "17/17 [==============================] - 5s 270ms/step - loss: 0.0374 - accuracy: 0.9907 - val_loss: 1.0049 - val_accuracy: 0.7463\n",
            "Epoch 28/300\n",
            "17/17 [==============================] - 5s 274ms/step - loss: 0.0470 - accuracy: 0.9851 - val_loss: 0.6561 - val_accuracy: 0.8209\n",
            "Epoch 29/300\n",
            "17/17 [==============================] - 6s 342ms/step - loss: 0.0366 - accuracy: 0.9870 - val_loss: 0.6344 - val_accuracy: 0.8507\n",
            "Epoch 30/300\n",
            "17/17 [==============================] - 5s 276ms/step - loss: 0.0418 - accuracy: 0.9870 - val_loss: 0.8605 - val_accuracy: 0.8433\n",
            "Epoch 31/300\n",
            "17/17 [==============================] - 6s 346ms/step - loss: 0.0517 - accuracy: 0.9777 - val_loss: 2.7659 - val_accuracy: 0.5821\n",
            "Epoch 32/300\n",
            "17/17 [==============================] - 5s 270ms/step - loss: 0.0346 - accuracy: 0.9907 - val_loss: 0.5498 - val_accuracy: 0.8433\n",
            "Epoch 33/300\n",
            "17/17 [==============================] - 6s 353ms/step - loss: 0.0336 - accuracy: 0.9981 - val_loss: 1.1653 - val_accuracy: 0.6716\n",
            "Epoch 34/300\n",
            "17/17 [==============================] - 5s 271ms/step - loss: 0.0394 - accuracy: 0.9851 - val_loss: 0.6311 - val_accuracy: 0.7612\n",
            "Epoch 35/300\n",
            "17/17 [==============================] - 6s 356ms/step - loss: 0.0290 - accuracy: 0.9888 - val_loss: 1.3428 - val_accuracy: 0.6418\n",
            "Epoch 36/300\n",
            "17/17 [==============================] - 5s 276ms/step - loss: 0.0169 - accuracy: 0.9944 - val_loss: 3.1132 - val_accuracy: 0.6343\n",
            "Epoch 37/300\n",
            "17/17 [==============================] - 6s 355ms/step - loss: 0.0112 - accuracy: 0.9981 - val_loss: 1.8487 - val_accuracy: 0.6418\n",
            "Epoch 38/300\n",
            "17/17 [==============================] - 5s 276ms/step - loss: 0.0105 - accuracy: 0.9963 - val_loss: 1.2830 - val_accuracy: 0.6567\n",
            "Epoch 39/300\n",
            "17/17 [==============================] - 5s 314ms/step - loss: 0.0132 - accuracy: 0.9944 - val_loss: 0.3374 - val_accuracy: 0.8731\n",
            "Epoch 40/300\n",
            "17/17 [==============================] - 5s 303ms/step - loss: 0.0378 - accuracy: 0.9907 - val_loss: 1.7041 - val_accuracy: 0.7687\n",
            "Epoch 41/300\n",
            "17/17 [==============================] - 5s 288ms/step - loss: 0.0524 - accuracy: 0.9814 - val_loss: 0.9103 - val_accuracy: 0.7836\n",
            "Epoch 42/300\n",
            "17/17 [==============================] - 6s 352ms/step - loss: 0.0366 - accuracy: 0.9870 - val_loss: 2.3171 - val_accuracy: 0.7836\n",
            "Epoch 43/300\n",
            "17/17 [==============================] - 6s 330ms/step - loss: 0.0257 - accuracy: 0.9907 - val_loss: 6.3720 - val_accuracy: 0.5522\n",
            "Epoch 44/300\n",
            "17/17 [==============================] - 8s 464ms/step - loss: 0.0172 - accuracy: 0.9963 - val_loss: 8.9883 - val_accuracy: 0.5149\n",
            "Epoch 45/300\n",
            "17/17 [==============================] - 5s 312ms/step - loss: 0.0302 - accuracy: 0.9926 - val_loss: 8.6375 - val_accuracy: 0.5224\n",
            "Epoch 46/300\n",
            "17/17 [==============================] - 6s 359ms/step - loss: 0.2557 - accuracy: 0.9777 - val_loss: 9.7374 - val_accuracy: 0.5075\n",
            "Epoch 47/300\n",
            "17/17 [==============================] - 5s 314ms/step - loss: 0.3433 - accuracy: 0.9610 - val_loss: 6.3943 - val_accuracy: 0.5373\n",
            "Epoch 48/300\n",
            "17/17 [==============================] - 6s 316ms/step - loss: 1.1523 - accuracy: 0.9089 - val_loss: 2.3281 - val_accuracy: 0.4030\n",
            "Epoch 49/300\n",
            "17/17 [==============================] - 6s 351ms/step - loss: 0.2915 - accuracy: 0.9126 - val_loss: 8.6351 - val_accuracy: 0.6119\n",
            "Epoch 50/300\n",
            "17/17 [==============================] - 5s 275ms/step - loss: 0.2554 - accuracy: 0.9331 - val_loss: 20.4423 - val_accuracy: 0.4179\n",
            "Epoch 51/300\n",
            "17/17 [==============================] - 5s 302ms/step - loss: 0.3210 - accuracy: 0.9591 - val_loss: 13.4412 - val_accuracy: 0.4925\n",
            "Epoch 52/300\n",
            "17/17 [==============================] - 5s 303ms/step - loss: 0.1313 - accuracy: 0.9665 - val_loss: 8.2642 - val_accuracy: 0.5299\n",
            "Epoch 53/300\n",
            "17/17 [==============================] - 6s 360ms/step - loss: 0.0444 - accuracy: 0.9814 - val_loss: 9.8503 - val_accuracy: 0.5299\n",
            "Epoch 54/300\n",
            "17/17 [==============================] - 5s 301ms/step - loss: 0.0594 - accuracy: 0.9833 - val_loss: 9.4918 - val_accuracy: 0.5373\n",
            "Epoch 55/300\n",
            "17/17 [==============================] - 5s 270ms/step - loss: 0.0264 - accuracy: 0.9907 - val_loss: 10.7083 - val_accuracy: 0.5299\n",
            "Epoch 56/300\n",
            "17/17 [==============================] - 6s 355ms/step - loss: 0.0230 - accuracy: 0.9907 - val_loss: 9.2273 - val_accuracy: 0.5373\n",
            "Epoch 57/300\n",
            "17/17 [==============================] - 5s 313ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 8.2284 - val_accuracy: 0.5373\n",
            "Epoch 58/300\n",
            "17/17 [==============================] - 5s 279ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 6.3484 - val_accuracy: 0.5373\n",
            "Epoch 59/300\n",
            "17/17 [==============================] - 6s 355ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 5.6534 - val_accuracy: 0.5373\n",
            "Epoch 60/300\n",
            "17/17 [==============================] - 5s 318ms/step - loss: 0.0082 - accuracy: 0.9963 - val_loss: 3.9496 - val_accuracy: 0.5373\n",
            "Epoch 61/300\n",
            "17/17 [==============================] - 5s 311ms/step - loss: 0.0065 - accuracy: 0.9981 - val_loss: 3.1020 - val_accuracy: 0.5970\n",
            "Epoch 62/300\n",
            "17/17 [==============================] - 5s 276ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 2.4072 - val_accuracy: 0.6791\n",
            "Epoch 63/300\n",
            "17/17 [==============================] - 6s 362ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 1.2598 - val_accuracy: 0.7537\n",
            "Epoch 64/300\n",
            "17/17 [==============================] - 5s 293ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.9987 - val_accuracy: 0.7985\n",
            "Epoch 65/300\n",
            "17/17 [==============================] - 5s 323ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.6812 - val_accuracy: 0.8134\n",
            "Epoch 66/300\n",
            "17/17 [==============================] - 5s 285ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.4566 - val_accuracy: 0.8507\n",
            "Epoch 67/300\n",
            "17/17 [==============================] - 6s 360ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.4365 - val_accuracy: 0.8657\n",
            "Epoch 68/300\n",
            "17/17 [==============================] - 6s 332ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.3265 - val_accuracy: 0.8806\n",
            "Epoch 69/300\n",
            "17/17 [==============================] - 5s 291ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.1503 - val_accuracy: 0.9552\n",
            "Epoch 70/300\n",
            "17/17 [==============================] - 6s 361ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.1562 - val_accuracy: 0.9328\n",
            "Epoch 71/300\n",
            "17/17 [==============================] - 5s 290ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.3144 - val_accuracy: 0.8806\n",
            "Epoch 72/300\n",
            "17/17 [==============================] - 6s 375ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.2828 - val_accuracy: 0.8806\n",
            "Epoch 73/300\n",
            "17/17 [==============================] - 5s 288ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.3123 - val_accuracy: 0.8806\n",
            "Epoch 74/300\n",
            "17/17 [==============================] - 6s 359ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.1442 - val_accuracy: 0.9478\n",
            "Epoch 75/300\n",
            "17/17 [==============================] - 6s 325ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0787 - val_accuracy: 0.9701\n",
            "Epoch 76/300\n",
            "17/17 [==============================] - 5s 293ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0549 - val_accuracy: 0.9701\n",
            "Epoch 77/300\n",
            "17/17 [==============================] - 5s 277ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0125 - val_accuracy: 1.0000\n",
            "Epoch 78/300\n",
            "17/17 [==============================] - 6s 360ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0269 - val_accuracy: 0.9925\n",
            "Epoch 79/300\n",
            "17/17 [==============================] - 5s 271ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0123 - val_accuracy: 1.0000\n",
            "Epoch 80/300\n",
            "17/17 [==============================] - 6s 329ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0503 - val_accuracy: 0.9701\n",
            "Epoch 81/300\n",
            "17/17 [==============================] - 5s 291ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.2179 - val_accuracy: 0.9478\n",
            "Epoch 82/300\n",
            "17/17 [==============================] - 5s 279ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0789 - val_accuracy: 0.9552\n",
            "Epoch 83/300\n",
            "17/17 [==============================] - 6s 350ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0119 - val_accuracy: 1.0000\n",
            "Epoch 84/300\n",
            "17/17 [==============================] - 5s 277ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.2180 - val_accuracy: 0.9478\n",
            "Epoch 85/300\n",
            "17/17 [==============================] - 6s 331ms/step - loss: 0.0060 - accuracy: 0.9963 - val_loss: 1.0038 - val_accuracy: 0.7985\n",
            "Epoch 86/300\n",
            "17/17 [==============================] - 5s 281ms/step - loss: 0.0211 - accuracy: 0.9963 - val_loss: 8.4927 - val_accuracy: 0.5373\n",
            "Epoch 87/300\n",
            "17/17 [==============================] - 5s 275ms/step - loss: 0.0464 - accuracy: 0.9888 - val_loss: 15.9591 - val_accuracy: 0.5299\n",
            "Epoch 88/300\n",
            "17/17 [==============================] - 6s 354ms/step - loss: 0.0555 - accuracy: 0.9870 - val_loss: 18.0494 - val_accuracy: 0.5299\n",
            "Epoch 89/300\n",
            "17/17 [==============================] - 6s 328ms/step - loss: 0.0654 - accuracy: 0.9814 - val_loss: 15.6113 - val_accuracy: 0.5299\n",
            "Epoch 90/300\n",
            "17/17 [==============================] - 5s 294ms/step - loss: 0.0306 - accuracy: 0.9870 - val_loss: 17.2819 - val_accuracy: 0.5299\n",
            "Epoch 91/300\n",
            "17/17 [==============================] - 6s 347ms/step - loss: 0.0297 - accuracy: 0.9870 - val_loss: 17.7404 - val_accuracy: 0.5299\n",
            "Epoch 92/300\n",
            "17/17 [==============================] - 6s 329ms/step - loss: 0.0194 - accuracy: 0.9926 - val_loss: 19.6658 - val_accuracy: 0.4478\n",
            "Epoch 93/300\n",
            "17/17 [==============================] - 5s 268ms/step - loss: 0.0364 - accuracy: 0.9926 - val_loss: 12.2230 - val_accuracy: 0.5299\n",
            "Epoch 94/300\n",
            "17/17 [==============================] - 6s 348ms/step - loss: 0.0161 - accuracy: 0.9944 - val_loss: 9.9533 - val_accuracy: 0.4776\n",
            "Epoch 95/300\n",
            "17/17 [==============================] - 5s 276ms/step - loss: 0.0444 - accuracy: 0.9926 - val_loss: 7.4854 - val_accuracy: 0.5373\n",
            "Epoch 96/300\n",
            "17/17 [==============================] - 6s 327ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 3.2440 - val_accuracy: 0.5373\n",
            "Epoch 97/300\n",
            "17/17 [==============================] - 5s 270ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 1.7883 - val_accuracy: 0.6866\n",
            "Epoch 98/300\n",
            "17/17 [==============================] - 6s 352ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 1.0594 - val_accuracy: 0.7836\n",
            "Epoch 99/300\n",
            "17/17 [==============================] - 5s 271ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.3056 - val_accuracy: 0.8806\n",
            "Epoch 100/300\n",
            "17/17 [==============================] - 5s 300ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.2519 - val_accuracy: 0.9179\n",
            "Epoch 101/300\n",
            "17/17 [==============================] - 5s 312ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.4280 - val_accuracy: 0.8507\n",
            "Epoch 102/300\n",
            "17/17 [==============================] - 6s 354ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 1.0463 - val_accuracy: 0.7836\n",
            "Epoch 103/300\n",
            "17/17 [==============================] - 5s 276ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.4703 - val_accuracy: 0.8433\n",
            "Epoch 104/300\n",
            "17/17 [==============================] - 5s 310ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.7523 - val_accuracy: 0.8060\n",
            "Epoch 105/300\n",
            "17/17 [==============================] - 5s 313ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.6851 - val_accuracy: 0.8209\n",
            "Epoch 106/300\n",
            "17/17 [==============================] - 5s 270ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.7026 - val_accuracy: 0.8134\n",
            "Epoch 107/300\n",
            "17/17 [==============================] - 6s 354ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0815 - val_accuracy: 0.9627\n",
            "Epoch 108/300\n",
            "17/17 [==============================] - 5s 280ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 1.1590 - val_accuracy: 0.7836\n",
            "Epoch 109/300\n",
            "17/17 [==============================] - 6s 330ms/step - loss: 0.0040 - accuracy: 0.9981 - val_loss: 4.2374 - val_accuracy: 0.6418\n",
            "Epoch 110/300\n",
            "17/17 [==============================] - 6s 355ms/step - loss: 0.0053 - accuracy: 0.9981 - val_loss: 8.3609 - val_accuracy: 0.5373\n",
            "Epoch 111/300\n",
            "17/17 [==============================] - 5s 275ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 4.6956 - val_accuracy: 0.6119\n",
            "Epoch 112/300\n",
            "17/17 [==============================] - 5s 270ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 3.3374 - val_accuracy: 0.6791\n",
            "Epoch 113/300\n",
            "17/17 [==============================] - 6s 351ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 1.2007 - val_accuracy: 0.7836\n",
            "Epoch 114/300\n",
            "17/17 [==============================] - 6s 355ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 2.2750 - val_accuracy: 0.7761\n",
            "Epoch 115/300\n",
            "17/17 [==============================] - 5s 269ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 4.3264 - val_accuracy: 0.6791\n",
            "Epoch 116/300\n",
            "17/17 [==============================] - 6s 353ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 3.4100 - val_accuracy: 0.7612\n",
            "Epoch 117/300\n",
            "17/17 [==============================] - 5s 277ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 2.5708 - val_accuracy: 0.7761\n",
            "Epoch 118/300\n",
            "17/17 [==============================] - 6s 340ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2724 - val_accuracy: 0.9104\n",
            "Epoch 119/300\n",
            "17/17 [==============================] - 5s 277ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.3028 - val_accuracy: 0.9179\n",
            "Epoch 120/300\n",
            "17/17 [==============================] - 6s 347ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.5616 - val_accuracy: 0.8433\n",
            "Epoch 121/300\n",
            "17/17 [==============================] - 5s 326ms/step - loss: 9.8941e-04 - accuracy: 1.0000 - val_loss: 0.5085 - val_accuracy: 0.8806\n",
            "Epoch 122/300\n",
            "17/17 [==============================] - 5s 296ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0496 - val_accuracy: 0.9701\n",
            "Epoch 123/300\n",
            "17/17 [==============================] - 6s 356ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.2772 - val_accuracy: 0.9179\n",
            "Epoch 124/300\n",
            "17/17 [==============================] - 5s 268ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 1.2875 - val_accuracy: 0.7761\n",
            "Epoch 125/300\n",
            "17/17 [==============================] - 6s 334ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 3.1093 - val_accuracy: 0.7761\n",
            "Epoch 126/300\n",
            "17/17 [==============================] - 5s 273ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 2.3567 - val_accuracy: 0.7761\n",
            "Epoch 127/300\n",
            "17/17 [==============================] - 6s 356ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 3.7877 - val_accuracy: 0.7687\n",
            "Epoch 128/300\n",
            "17/17 [==============================] - 5s 279ms/step - loss: 0.0077 - accuracy: 0.9981 - val_loss: 15.9832 - val_accuracy: 0.5224\n",
            "Epoch 129/300\n",
            "17/17 [==============================] - 6s 341ms/step - loss: 0.0355 - accuracy: 0.9926 - val_loss: 11.7662 - val_accuracy: 0.5373\n",
            "Epoch 130/300\n",
            "17/17 [==============================] - 5s 273ms/step - loss: 0.0474 - accuracy: 0.9851 - val_loss: 15.1941 - val_accuracy: 0.5373\n",
            "Epoch 131/300\n",
            "17/17 [==============================] - 5s 276ms/step - loss: 0.0901 - accuracy: 0.9796 - val_loss: 11.1435 - val_accuracy: 0.4776\n",
            "Epoch 132/300\n",
            "17/17 [==============================] - 6s 371ms/step - loss: 0.0712 - accuracy: 0.9870 - val_loss: 11.0543 - val_accuracy: 0.4328\n",
            "Epoch 133/300\n",
            "17/17 [==============================] - 7s 412ms/step - loss: 0.0372 - accuracy: 0.9888 - val_loss: 14.4912 - val_accuracy: 0.4776\n",
            "Epoch 134/300\n",
            "17/17 [==============================] - 5s 279ms/step - loss: 0.0433 - accuracy: 0.9907 - val_loss: 16.3105 - val_accuracy: 0.5373\n",
            "Epoch 135/300\n",
            "17/17 [==============================] - 6s 357ms/step - loss: 0.0280 - accuracy: 0.9907 - val_loss: 19.4027 - val_accuracy: 0.5373\n",
            "Epoch 136/300\n",
            "17/17 [==============================] - 5s 288ms/step - loss: 0.0144 - accuracy: 0.9926 - val_loss: 20.1829 - val_accuracy: 0.5299\n",
            "Epoch 137/300\n",
            "17/17 [==============================] - 6s 347ms/step - loss: 0.0068 - accuracy: 0.9981 - val_loss: 18.5535 - val_accuracy: 0.5299\n",
            "Epoch 138/300\n",
            "17/17 [==============================] - 5s 277ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 18.1214 - val_accuracy: 0.5299\n",
            "Epoch 139/300\n",
            "17/17 [==============================] - 6s 352ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 16.6083 - val_accuracy: 0.5299\n",
            "Epoch 140/300\n",
            "17/17 [==============================] - 6s 331ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 14.4229 - val_accuracy: 0.5299\n",
            "Epoch 141/300\n",
            "17/17 [==============================] - 5s 311ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 12.6672 - val_accuracy: 0.5299\n",
            "Epoch 142/300\n",
            "17/17 [==============================] - 6s 363ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 9.0590 - val_accuracy: 0.5299\n",
            "Epoch 143/300\n",
            "17/17 [==============================] - 5s 325ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 6.2325 - val_accuracy: 0.5597\n",
            "Epoch 144/300\n",
            "17/17 [==============================] - 6s 321ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 4.1582 - val_accuracy: 0.7164\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-8cfa3169166d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistorique\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_ent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1671\u001b[0m             \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1672\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1673\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1674\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1675\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mreset_metrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2426\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2428\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2429\u001b[0m         \"\"\"Resets the state of all the metrics in the model.\n\u001b[1;32m   2430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "metadata": {
        "id": "3BTh60u8qg1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(dataset_test)"
      ],
      "metadata": {
        "id": "kbahDFURBtpz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}