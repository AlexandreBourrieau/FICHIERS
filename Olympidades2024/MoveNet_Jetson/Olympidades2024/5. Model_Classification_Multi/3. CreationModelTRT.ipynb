{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DmDvMG66uol7",
    "outputId": "b26297ee-1aa4-41cd-c78d-c89ea7ee3034"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&&&& RUNNING TensorRT.trtexec [TensorRT v8205] # /home/jetson/TensorRT/build/trtexec --fp16 --onnx=modele_classification_multi_assises.onnx --saveEngine=model_classification_multi_assises.engine\n",
      "[05/19/2024-18:22:15] [I] === Model Options ===\n",
      "[05/19/2024-18:22:15] [I] Format: ONNX\n",
      "[05/19/2024-18:22:15] [I] Model: modele_classification_multi_assises.onnx\n",
      "[05/19/2024-18:22:15] [I] Output:\n",
      "[05/19/2024-18:22:15] [I] === Build Options ===\n",
      "[05/19/2024-18:22:15] [I] Max batch: explicit batch\n",
      "[05/19/2024-18:22:15] [I] Workspace: 16 MiB\n",
      "[05/19/2024-18:22:15] [I] minTiming: 1\n",
      "[05/19/2024-18:22:15] [I] avgTiming: 8\n",
      "[05/19/2024-18:22:15] [I] Precision: FP32+FP16\n",
      "[05/19/2024-18:22:15] [I] Calibration: \n",
      "[05/19/2024-18:22:15] [I] Refit: Disabled\n",
      "[05/19/2024-18:22:15] [I] Sparsity: Disabled\n",
      "[05/19/2024-18:22:15] [I] Safe mode: Disabled\n",
      "[05/19/2024-18:22:15] [I] DirectIO mode: Disabled\n",
      "[05/19/2024-18:22:15] [I] Restricted mode: Disabled\n",
      "[05/19/2024-18:22:15] [I] Save engine: model_classification_multi_assises.engine\n",
      "[05/19/2024-18:22:15] [I] Load engine: \n",
      "[05/19/2024-18:22:15] [I] Profiling verbosity: 0\n",
      "[05/19/2024-18:22:15] [I] Tactic sources: Using default tactic sources\n",
      "[05/19/2024-18:22:15] [I] timingCacheMode: local\n",
      "[05/19/2024-18:22:15] [I] timingCacheFile: \n",
      "[05/19/2024-18:22:15] [I] Input(s)s format: fp32:CHW\n",
      "[05/19/2024-18:22:15] [I] Output(s)s format: fp32:CHW\n",
      "[05/19/2024-18:22:15] [I] Input build shapes: model\n",
      "[05/19/2024-18:22:15] [I] Input calibration shapes: model\n",
      "[05/19/2024-18:22:15] [I] === System Options ===\n",
      "[05/19/2024-18:22:15] [I] Device: 0\n",
      "[05/19/2024-18:22:15] [I] DLACore: \n",
      "[05/19/2024-18:22:15] [I] Plugins:\n",
      "[05/19/2024-18:22:15] [I] === Inference Options ===\n",
      "[05/19/2024-18:22:15] [I] Batch: Explicit\n",
      "[05/19/2024-18:22:15] [I] Input inference shapes: model\n",
      "[05/19/2024-18:22:15] [I] Iterations: 10\n",
      "[05/19/2024-18:22:15] [I] Duration: 3s (+ 200ms warm up)\n",
      "[05/19/2024-18:22:15] [I] Sleep time: 0ms\n",
      "[05/19/2024-18:22:15] [I] Idle time: 0ms\n",
      "[05/19/2024-18:22:15] [I] Streams: 1\n",
      "[05/19/2024-18:22:15] [I] ExposeDMA: Disabled\n",
      "[05/19/2024-18:22:15] [I] Data transfers: Enabled\n",
      "[05/19/2024-18:22:15] [I] Spin-wait: Disabled\n",
      "[05/19/2024-18:22:15] [I] Multithreading: Disabled\n",
      "[05/19/2024-18:22:15] [I] CUDA Graph: Disabled\n",
      "[05/19/2024-18:22:15] [I] Separate profiling: Disabled\n",
      "[05/19/2024-18:22:15] [I] Time Deserialize: Disabled\n",
      "[05/19/2024-18:22:15] [I] Time Refit: Disabled\n",
      "[05/19/2024-18:22:15] [I] Skip inference: Disabled\n",
      "[05/19/2024-18:22:15] [I] Inputs:\n",
      "[05/19/2024-18:22:15] [I] === Reporting Options ===\n",
      "[05/19/2024-18:22:15] [I] Verbose: Disabled\n",
      "[05/19/2024-18:22:15] [I] Averages: 10 inferences\n",
      "[05/19/2024-18:22:15] [I] Percentile: 99\n",
      "[05/19/2024-18:22:15] [I] Dump refittable layers:Disabled\n",
      "[05/19/2024-18:22:15] [I] Dump output: Disabled\n",
      "[05/19/2024-18:22:15] [I] Profile: Disabled\n",
      "[05/19/2024-18:22:15] [I] Export timing to JSON file: \n",
      "[05/19/2024-18:22:15] [I] Export output to JSON file: \n",
      "[05/19/2024-18:22:15] [I] Export profile to JSON file: \n",
      "[05/19/2024-18:22:15] [I] \n",
      "[05/19/2024-18:22:15] [I] === Device Information ===\n",
      "[05/19/2024-18:22:15] [I] Selected Device: NVIDIA Tegra X1\n",
      "[05/19/2024-18:22:15] [I] Compute Capability: 5.3\n",
      "[05/19/2024-18:22:15] [I] SMs: 1\n",
      "[05/19/2024-18:22:15] [I] Compute Clock Rate: 0.9216 GHz\n",
      "[05/19/2024-18:22:15] [I] Device Global Memory: 3963 MiB\n",
      "[05/19/2024-18:22:15] [I] Shared Memory per SM: 64 KiB\n",
      "[05/19/2024-18:22:15] [I] Memory Bus Width: 64 bits (ECC disabled)\n",
      "[05/19/2024-18:22:15] [I] Memory Clock Rate: 0.01275 GHz\n",
      "[05/19/2024-18:22:15] [I] \n",
      "[05/19/2024-18:22:15] [I] TensorRT version: 8.2.5\n",
      "[05/19/2024-18:22:18] [I] [TRT] [MemUsageChange] Init CUDA: CPU +229, GPU +0, now: CPU 248, GPU 2171 (MiB)\n",
      "[05/19/2024-18:22:19] [I] [TRT] [MemUsageSnapshot] Begin constructing builder kernel library: CPU 248 MiB, GPU 2201 MiB\n",
      "[05/19/2024-18:22:19] [I] [TRT] [MemUsageSnapshot] End constructing builder kernel library: CPU 278 MiB, GPU 2230 MiB\n",
      "[05/19/2024-18:22:19] [I] Start parsing network model\n",
      "[05/19/2024-18:22:19] [I] [TRT] ----------------------------------------------------------------\n",
      "[05/19/2024-18:22:19] [I] [TRT] Input filename:   modele_classification_multi_assises.onnx\n",
      "[05/19/2024-18:22:19] [I] [TRT] ONNX IR version:  0.0.7\n",
      "[05/19/2024-18:22:19] [I] [TRT] Opset version:    13\n",
      "[05/19/2024-18:22:19] [I] [TRT] Producer name:    tf2onnx\n",
      "[05/19/2024-18:22:19] [I] [TRT] Producer version: 1.12.0 f4902a\n",
      "[05/19/2024-18:22:19] [I] [TRT] Domain:           \n",
      "[05/19/2024-18:22:19] [I] [TRT] Model version:    0\n",
      "[05/19/2024-18:22:19] [I] [TRT] Doc string:       \n",
      "[05/19/2024-18:22:19] [I] [TRT] ----------------------------------------------------------------\n",
      "[05/19/2024-18:22:19] [W] [TRT] parsers/onnx/onnx2trt_utils.cpp:366: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[05/19/2024-18:22:19] [W] [TRT] parsers/onnx/ShapedWeights.cpp:173: Weights StatefulPartitionedCall/model/dense_6/MatMul/ReadVariableOp:0 has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[05/19/2024-18:22:19] [W] [TRT] parsers/onnx/ShapedWeights.cpp:173: Weights StatefulPartitionedCall/model/dense_7/MatMul/ReadVariableOp:0 has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[05/19/2024-18:22:19] [W] [TRT] parsers/onnx/ShapedWeights.cpp:173: Weights StatefulPartitionedCall/model/dense_8/MatMul/ReadVariableOp:0 has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[05/19/2024-18:22:19] [W] [TRT] parsers/onnx/ShapedWeights.cpp:173: Weights StatefulPartitionedCall/model/dense_3/MatMul/ReadVariableOp:0 has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[05/19/2024-18:22:19] [W] [TRT] parsers/onnx/ShapedWeights.cpp:173: Weights StatefulPartitionedCall/model/dense_4/MatMul/ReadVariableOp:0 has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[05/19/2024-18:22:19] [W] [TRT] parsers/onnx/ShapedWeights.cpp:173: Weights StatefulPartitionedCall/model/dense_5/MatMul/ReadVariableOp:0 has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[05/19/2024-18:22:19] [W] [TRT] parsers/onnx/ShapedWeights.cpp:173: Weights StatefulPartitionedCall/model/dense_9/MatMul/ReadVariableOp:0 has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[05/19/2024-18:22:19] [W] [TRT] parsers/onnx/ShapedWeights.cpp:173: Weights StatefulPartitionedCall/model/dense_10/MatMul/ReadVariableOp:0 has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[05/19/2024-18:22:19] [W] [TRT] parsers/onnx/ShapedWeights.cpp:173: Weights StatefulPartitionedCall/model/dense_14/MatMul/ReadVariableOp:0 has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[05/19/2024-18:22:19] [W] [TRT] parsers/onnx/ShapedWeights.cpp:173: Weights StatefulPartitionedCall/model/dense_11/MatMul/ReadVariableOp:0 has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[05/19/2024-18:22:19] [W] [TRT] parsers/onnx/ShapedWeights.cpp:173: Weights StatefulPartitionedCall/model/dense/MatMul/ReadVariableOp:0 has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[05/19/2024-18:22:19] [W] [TRT] parsers/onnx/ShapedWeights.cpp:173: Weights StatefulPartitionedCall/model/dense_1/MatMul/ReadVariableOp:0 has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[05/19/2024-18:22:19] [W] [TRT] parsers/onnx/ShapedWeights.cpp:173: Weights StatefulPartitionedCall/model/dense_2/MatMul/ReadVariableOp:0 has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[05/19/2024-18:22:19] [I] Finish parsing network model\n",
      "[05/19/2024-18:22:19] [W] Dynamic dimensions required for input: input_1, but no shapes were provided. Automatically overriding shape to: 1x17x3\n",
      "[05/19/2024-18:22:20] [I] [TRT] ---------- Layers Running on DLA ----------\n",
      "[05/19/2024-18:22:20] [I] [TRT] ---------- Layers Running on GPU ----------\n",
      "[05/19/2024-18:22:20] [I] [TRT] [GpuLayer] {ForeignNode[StatefulPartitionedCall/model/tf.compat.v1.gather_3/GatherV2/indices:0...StatefulPartitionedCall/model/dense_6/BiasAdd]}\n",
      "[05/19/2024-18:22:20] [I] [TRT] [GpuLayer] PWN(StatefulPartitionedCall/model/dense_6/Relu6)\n",
      "[05/19/2024-18:22:20] [I] [TRT] [GpuLayer] PWN(StatefulPartitionedCall/model/dense_3/Relu6)\n",
      "[05/19/2024-18:22:20] [I] [TRT] [GpuLayer] PWN(StatefulPartitionedCall/model/dense_9/Relu6)\n",
      "[05/19/2024-18:22:20] [I] [TRT] [GpuLayer] PWN(StatefulPartitionedCall/model/dense/Relu6)\n",
      "[05/19/2024-18:22:20] [I] [TRT] [GpuLayer] (Unnamed Layer* 240) [Shuffle]\n",
      "[05/19/2024-18:22:20] [I] [TRT] [GpuLayer] StatefulPartitionedCall/model/dense_7/MatMul + StatefulPartitionedCall/model/dense_7/BiasAdd/ReadVariableOp:0 + (Unnamed Layer* 247) [Shuffle] + unsqueeze_node_after_StatefulPartitionedCall/model/dense_7/BiasAdd/ReadVariableOp:0 + (Unnamed Layer* 247) [Shuffle]_(Unnamed Layer* 247) [Shuffle]_output + StatefulPartitionedCall/model/dense_7/BiasAdd + PWN(StatefulPartitionedCall/model/dense_7/Relu6)\n",
      "[05/19/2024-18:22:20] [I] [TRT] [GpuLayer] (Unnamed Layer* 349) [Shuffle]\n",
      "[05/19/2024-18:22:20] [I] [TRT] [GpuLayer] StatefulPartitionedCall/model/dense_4/MatMul + StatefulPartitionedCall/model/dense_4/BiasAdd/ReadVariableOp:0 + (Unnamed Layer* 356) [Shuffle] + unsqueeze_node_after_StatefulPartitionedCall/model/dense_4/BiasAdd/ReadVariableOp:0 + (Unnamed Layer* 356) [Shuffle]_(Unnamed Layer* 356) [Shuffle]_output + StatefulPartitionedCall/model/dense_4/BiasAdd + PWN(StatefulPartitionedCall/model/dense_4/Relu6)\n",
      "[05/19/2024-18:22:20] [I] [TRT] [GpuLayer] (Unnamed Layer* 424) [Shuffle]\n",
      "[05/19/2024-18:22:20] [I] [TRT] [GpuLayer] StatefulPartitionedCall/model/dense_10/MatMul + StatefulPartitionedCall/model/dense_10/BiasAdd/ReadVariableOp:0 + (Unnamed Layer* 431) [Shuffle] + unsqueeze_node_after_StatefulPartitionedCall/model/dense_10/BiasAdd/ReadVariableOp:0 + (Unnamed Layer* 431) [Shuffle]_(Unnamed Layer* 431) [Shuffle]_output + StatefulPartitionedCall/model/dense_10/BiasAdd + PWN(StatefulPartitionedCall/model/dense_10/Relu6)\n",
      "[05/19/2024-18:22:20] [I] [TRT] [GpuLayer] (Unnamed Layer* 522) [Shuffle]\n",
      "[05/19/2024-18:22:20] [I] [TRT] [GpuLayer] StatefulPartitionedCall/model/dense_1/MatMul + StatefulPartitionedCall/model/dense_1/BiasAdd/ReadVariableOp:0 + (Unnamed Layer* 529) [Shuffle] + unsqueeze_node_after_StatefulPartitionedCall/model/dense_1/BiasAdd/ReadVariableOp:0 + (Unnamed Layer* 529) [Shuffle]_(Unnamed Layer* 529) [Shuffle]_output + StatefulPartitionedCall/model/dense_1/BiasAdd + PWN(StatefulPartitionedCall/model/dense_1/Relu6)\n",
      "[05/19/2024-18:22:20] [I] [TRT] [GpuLayer] squeeze_after_StatefulPartitionedCall/model/dense_10/Relu6\n",
      "[05/19/2024-18:22:20] [I] [TRT] [GpuLayer] (Unnamed Layer* 464) [Shuffle]\n",
      "[05/19/2024-18:22:20] [I] [TRT] [GpuLayer] StatefulPartitionedCall/model/dense_2/MatMul + StatefulPartitionedCall/model/dense_2/BiasAdd\n",
      "[05/19/2024-18:22:20] [I] [TRT] [GpuLayer] copied_squeeze_after_StatefulPartitionedCall/model/dense_2/BiasAdd\n",
      "[05/19/2024-18:22:20] [I] [TRT] [GpuLayer] StatefulPartitionedCall/model/dense_2/Softmax\n",
      "[05/19/2024-18:22:20] [I] [TRT] [GpuLayer] (Unnamed Layer* 548) [Shuffle] + StatefulPartitionedCall/model/tf.expand_dims_2/ExpandDims\n",
      "[05/19/2024-18:22:20] [I] [TRT] [GpuLayer] StatefulPartitionedCall/model/dense_11/MatMul + StatefulPartitionedCall/model/dense_11/BiasAdd || StatefulPartitionedCall/model/dense_14/MatMul + StatefulPartitionedCall/model/dense_14/BiasAdd\n",
      "[05/19/2024-18:22:20] [I] [TRT] [GpuLayer] copied_squeeze_after_StatefulPartitionedCall/model/dense_11/BiasAdd\n",
      "[05/19/2024-18:22:20] [I] [TRT] [GpuLayer] StatefulPartitionedCall/model/dense_11/Softmax\n",
      "[05/19/2024-18:22:20] [I] [TRT] [GpuLayer] (Unnamed Layer* 474) [Shuffle] + StatefulPartitionedCall/model/tf.expand_dims_5/ExpandDims\n",
      "[05/19/2024-18:22:20] [I] [TRT] [GpuLayer] copied_squeeze_after_StatefulPartitionedCall/model/dense_14/BiasAdd\n",
      "[05/19/2024-18:22:20] [I] [TRT] [GpuLayer] StatefulPartitionedCall/model/dense_14/Softmax\n",
      "[05/19/2024-18:22:20] [I] [TRT] [GpuLayer] (Unnamed Layer* 450) [Shuffle] + StatefulPartitionedCall/model/tf.expand_dims_6/ExpandDims\n",
      "[05/19/2024-18:22:20] [I] [TRT] [GpuLayer] StatefulPartitionedCall/model/dense_5/MatMul + StatefulPartitionedCall/model/dense_5/BiasAdd\n",
      "[05/19/2024-18:22:20] [I] [TRT] [GpuLayer] copied_squeeze_after_StatefulPartitionedCall/model/dense_5/BiasAdd\n",
      "[05/19/2024-18:22:20] [I] [TRT] [GpuLayer] StatefulPartitionedCall/model/dense_5/Softmax\n",
      "[05/19/2024-18:22:20] [I] [TRT] [GpuLayer] (Unnamed Layer* 375) [Shuffle] + StatefulPartitionedCall/model/tf.expand_dims_3/ExpandDims\n",
      "[05/19/2024-18:22:20] [I] [TRT] [GpuLayer] StatefulPartitionedCall/model/dense_8/MatMul + StatefulPartitionedCall/model/dense_8/BiasAdd\n",
      "[05/19/2024-18:22:20] [I] [TRT] [GpuLayer] copied_squeeze_after_StatefulPartitionedCall/model/dense_8/BiasAdd\n",
      "[05/19/2024-18:22:20] [I] [TRT] [GpuLayer] StatefulPartitionedCall/model/dense_8/Softmax\n",
      "[05/19/2024-18:22:20] [I] [TRT] [GpuLayer] (Unnamed Layer* 266) [Shuffle] + StatefulPartitionedCall/model/tf.expand_dims_4/ExpandDims\n",
      "[05/19/2024-18:22:20] [I] [TRT] [GpuLayer] StatefulPartitionedCall/model/tf.expand_dims_2/ExpandDims:0 copy\n",
      "[05/19/2024-18:22:20] [I] [TRT] [GpuLayer] StatefulPartitionedCall/model/tf.expand_dims_3/ExpandDims:0 copy\n",
      "[05/19/2024-18:22:20] [I] [TRT] [GpuLayer] StatefulPartitionedCall/model/tf.expand_dims_4/ExpandDims:0 copy\n",
      "[05/19/2024-18:22:20] [I] [TRT] [GpuLayer] StatefulPartitionedCall/model/tf.expand_dims_5/ExpandDims:0 copy\n",
      "[05/19/2024-18:22:20] [I] [TRT] [GpuLayer] StatefulPartitionedCall/model/tf.expand_dims_6/ExpandDims:0 copy\n",
      "[05/19/2024-18:22:22] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +158, GPU +235, now: CPU 449, GPU 2482 (MiB)\n",
      "[05/19/2024-18:22:25] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +241, GPU +352, now: CPU 690, GPU 2834 (MiB)\n",
      "[05/19/2024-18:22:25] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[05/19/2024-18:22:52] [I] [TRT] Some tactics do not have sufficient workspace memory to run. Increasing workspace size may increase performance, please check verbose output.\n",
      "[05/19/2024-18:23:00] [I] [TRT] Detected 1 inputs and 6 output network tensors.\n",
      "[05/19/2024-18:23:00] [I] [TRT] Total Host Persistent Memory: 18592\n",
      "[05/19/2024-18:23:00] [I] [TRT] Total Device Persistent Memory: 0\n",
      "[05/19/2024-18:23:00] [I] [TRT] Total Scratch Memory: 1544\n",
      "[05/19/2024-18:23:00] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 5 MiB\n",
      "[05/19/2024-18:23:00] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 14.0969ms to assign 11 blocks to 48 nodes requiring 5124 bytes.\n",
      "[05/19/2024-18:23:00] [I] [TRT] Total Activation Memory: 5124\n",
      "[05/19/2024-18:23:00] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 932, GPU 3330 (MiB)\n",
      "[05/19/2024-18:23:00] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +1, GPU +0, now: CPU 933, GPU 3330 (MiB)\n",
      "[05/19/2024-18:23:00] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +4, now: CPU 0, GPU 4 (MiB)\n",
      "[05/19/2024-18:23:00] [I] [TRT] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 931, GPU 3330 (MiB)\n",
      "[05/19/2024-18:23:00] [I] [TRT] Loaded engine size: 1 MiB\n",
      "[05/19/2024-18:23:00] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 933, GPU 3330 (MiB)\n",
      "[05/19/2024-18:23:00] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +0, now: CPU 933, GPU 3330 (MiB)\n",
      "[05/19/2024-18:23:00] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)\n",
      "[05/19/2024-18:23:00] [I] Engine built in 44.9459 sec.\n",
      "[05/19/2024-18:23:00] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 901, GPU 3332 (MiB)\n",
      "[05/19/2024-18:23:00] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +0, now: CPU 901, GPU 3332 (MiB)\n",
      "[05/19/2024-18:23:00] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)\n",
      "[05/19/2024-18:23:00] [I] Using random values for input input_1\n",
      "[05/19/2024-18:23:00] [I] Created input binding for input_1 with dimensions 1x17x3\n",
      "[05/19/2024-18:23:00] [I] Using random values for output concatenate\n",
      "[05/19/2024-18:23:00] [I] Created output binding for concatenate with dimensions 1x5x2\n",
      "[05/19/2024-18:23:00] [I] Starting inference\n",
      "[05/19/2024-18:23:03] [I] Warmup completed 59 queries over 200 ms\n",
      "[05/19/2024-18:23:03] [I] Timing trace has 868 queries over 3.00292 s\n",
      "[05/19/2024-18:23:03] [I] \n",
      "[05/19/2024-18:23:03] [I] === Trace details ===\n",
      "[05/19/2024-18:23:03] [I] Trace averages of 10 runs:\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.84194 ms - Host latency: 1.86585 ms (end to end 3.45886 ms, enqueue 1.47436 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.82999 ms - Host latency: 1.85291 ms (end to end 3.45681 ms, enqueue 1.61333 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.83877 ms - Host latency: 1.86119 ms (end to end 3.45882 ms, enqueue 1.79169 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.81943 ms - Host latency: 1.84177 ms (end to end 3.45766 ms, enqueue 1.99491 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.81423 ms - Host latency: 1.83675 ms (end to end 3.45766 ms, enqueue 2.17927 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.82613 ms - Host latency: 1.84843 ms (end to end 3.45946 ms, enqueue 2.29524 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.81025 ms - Host latency: 1.83258 ms (end to end 3.45864 ms, enqueue 2.08905 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.81696 ms - Host latency: 1.83918 ms (end to end 3.45834 ms, enqueue 2.18515 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.82267 ms - Host latency: 1.84513 ms (end to end 3.45952 ms, enqueue 2.11562 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.83153 ms - Host latency: 1.8541 ms (end to end 3.45886 ms, enqueue 1.97244 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.82384 ms - Host latency: 1.84646 ms (end to end 3.46005 ms, enqueue 1.89463 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.82009 ms - Host latency: 1.84228 ms (end to end 3.45759 ms, enqueue 1.74053 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.82677 ms - Host latency: 1.84922 ms (end to end 3.45864 ms, enqueue 1.86718 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.82338 ms - Host latency: 1.84603 ms (end to end 3.45813 ms, enqueue 1.73867 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.82532 ms - Host latency: 1.84773 ms (end to end 3.45906 ms, enqueue 2.02264 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.84329 ms - Host latency: 1.86616 ms (end to end 3.45784 ms, enqueue 1.88284 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.84131 ms - Host latency: 1.86469 ms (end to end 3.45725 ms, enqueue 1.48004 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.81071 ms - Host latency: 1.8332 ms (end to end 3.45825 ms, enqueue 1.69349 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.83228 ms - Host latency: 1.85459 ms (end to end 3.45803 ms, enqueue 1.99913 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.81536 ms - Host latency: 1.8377 ms (end to end 3.45696 ms, enqueue 2.20064 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.81744 ms - Host latency: 1.8396 ms (end to end 3.45937 ms, enqueue 2.09885 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.81994 ms - Host latency: 1.84214 ms (end to end 3.45932 ms, enqueue 1.89155 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.80263 ms - Host latency: 1.82487 ms (end to end 3.28557 ms, enqueue 1.79334 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.82386 ms - Host latency: 1.84656 ms (end to end 3.45848 ms, enqueue 1.82282 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.81873 ms - Host latency: 1.84133 ms (end to end 3.45967 ms, enqueue 2.05223 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.82361 ms - Host latency: 1.846 ms (end to end 3.45907 ms, enqueue 2.02675 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.82364 ms - Host latency: 1.84641 ms (end to end 3.45856 ms, enqueue 1.64935 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.83232 ms - Host latency: 1.85455 ms (end to end 3.45896 ms, enqueue 1.71688 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.8255 ms - Host latency: 1.84795 ms (end to end 3.45876 ms, enqueue 1.60072 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.81285 ms - Host latency: 1.8354 ms (end to end 3.45847 ms, enqueue 1.60461 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.87986 ms - Host latency: 1.904 ms (end to end 3.45867 ms, enqueue 1.18737 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.82675 ms - Host latency: 1.84968 ms (end to end 3.45758 ms, enqueue 1.5343 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.82126 ms - Host latency: 1.84365 ms (end to end 3.45958 ms, enqueue 2.02926 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.80922 ms - Host latency: 1.83154 ms (end to end 3.45858 ms, enqueue 1.95752 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.82227 ms - Host latency: 1.84475 ms (end to end 3.4583 ms, enqueue 1.93322 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.81664 ms - Host latency: 1.83904 ms (end to end 3.45833 ms, enqueue 2.2335 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.82068 ms - Host latency: 1.8432 ms (end to end 3.45842 ms, enqueue 1.7752 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.82909 ms - Host latency: 1.8517 ms (end to end 3.45741 ms, enqueue 1.83798 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.82676 ms - Host latency: 1.84911 ms (end to end 3.45773 ms, enqueue 2.12045 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.83237 ms - Host latency: 1.8548 ms (end to end 3.45828 ms, enqueue 2.07264 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.8137 ms - Host latency: 1.83611 ms (end to end 3.45826 ms, enqueue 1.94756 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.81805 ms - Host latency: 1.84061 ms (end to end 3.45824 ms, enqueue 2.02919 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.82341 ms - Host latency: 1.84596 ms (end to end 3.45847 ms, enqueue 1.89296 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.81929 ms - Host latency: 1.84178 ms (end to end 3.45911 ms, enqueue 1.71084 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.81625 ms - Host latency: 1.83861 ms (end to end 3.45765 ms, enqueue 2.08678 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.82023 ms - Host latency: 1.8426 ms (end to end 3.45775 ms, enqueue 2.08427 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.81482 ms - Host latency: 1.83689 ms (end to end 3.45869 ms, enqueue 1.9782 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.82371 ms - Host latency: 1.84607 ms (end to end 3.4585 ms, enqueue 2.06927 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.81665 ms - Host latency: 1.839 ms (end to end 3.45759 ms, enqueue 1.97964 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.81815 ms - Host latency: 1.84063 ms (end to end 3.45773 ms, enqueue 1.91218 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.82777 ms - Host latency: 1.8506 ms (end to end 3.45739 ms, enqueue 1.79293 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.84038 ms - Host latency: 1.86337 ms (end to end 3.46001 ms, enqueue 1.69188 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.82446 ms - Host latency: 1.84716 ms (end to end 3.45831 ms, enqueue 2.08069 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.81721 ms - Host latency: 1.83951 ms (end to end 3.4584 ms, enqueue 2.14982 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.82822 ms - Host latency: 1.85088 ms (end to end 3.45801 ms, enqueue 2.01814 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.83357 ms - Host latency: 1.85579 ms (end to end 3.45789 ms, enqueue 2.12407 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.82754 ms - Host latency: 1.85 ms (end to end 3.46035 ms, enqueue 2.04424 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.83313 ms - Host latency: 1.85547 ms (end to end 3.46104 ms, enqueue 2.00183 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.81497 ms - Host latency: 1.83752 ms (end to end 3.45825 ms, enqueue 1.93865 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.82656 ms - Host latency: 1.84954 ms (end to end 3.45842 ms, enqueue 1.75217 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.81184 ms - Host latency: 1.83428 ms (end to end 3.45857 ms, enqueue 1.98833 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.81467 ms - Host latency: 1.83696 ms (end to end 3.45828 ms, enqueue 1.92449 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.83374 ms - Host latency: 1.85681 ms (end to end 3.45798 ms, enqueue 1.79343 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.81838 ms - Host latency: 1.84094 ms (end to end 3.45833 ms, enqueue 1.97925 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.80425 ms - Host latency: 1.8269 ms (end to end 3.45969 ms, enqueue 2.11108 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.82271 ms - Host latency: 1.84524 ms (end to end 3.4582 ms, enqueue 1.85884 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.81238 ms - Host latency: 1.83462 ms (end to end 3.45798 ms, enqueue 1.77793 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.81404 ms - Host latency: 1.83645 ms (end to end 3.45881 ms, enqueue 1.7874 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.84141 ms - Host latency: 1.86465 ms (end to end 3.45842 ms, enqueue 1.61382 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.8199 ms - Host latency: 1.8425 ms (end to end 3.45764 ms, enqueue 1.87712 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.81987 ms - Host latency: 1.84233 ms (end to end 3.45667 ms, enqueue 2.03335 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.80374 ms - Host latency: 1.82598 ms (end to end 3.45854 ms, enqueue 1.89797 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.83496 ms - Host latency: 1.85754 ms (end to end 3.45762 ms, enqueue 1.84375 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.81653 ms - Host latency: 1.83901 ms (end to end 3.45889 ms, enqueue 1.92205 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.82437 ms - Host latency: 1.84712 ms (end to end 3.45928 ms, enqueue 2.05737 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.82043 ms - Host latency: 1.8426 ms (end to end 3.45803 ms, enqueue 2.23103 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.83174 ms - Host latency: 1.85435 ms (end to end 3.45869 ms, enqueue 1.78052 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.82515 ms - Host latency: 1.84792 ms (end to end 3.45903 ms, enqueue 2.03733 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.81963 ms - Host latency: 1.84216 ms (end to end 3.45742 ms, enqueue 1.7709 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.8251 ms - Host latency: 1.84749 ms (end to end 3.4595 ms, enqueue 1.81001 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.82229 ms - Host latency: 1.84507 ms (end to end 3.45842 ms, enqueue 1.91826 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.83069 ms - Host latency: 1.85305 ms (end to end 3.45664 ms, enqueue 1.66387 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.8199 ms - Host latency: 1.84277 ms (end to end 3.45837 ms, enqueue 1.8844 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.8311 ms - Host latency: 1.85376 ms (end to end 3.45896 ms, enqueue 1.93694 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.87322 ms - Host latency: 1.89558 ms (end to end 3.4584 ms, enqueue 1.945 ms)\n",
      "[05/19/2024-18:23:03] [I] Average on 10 runs - GPU latency: 1.83435 ms - Host latency: 1.85693 ms (end to end 3.45977 ms, enqueue 2.0467 ms)\n",
      "[05/19/2024-18:23:03] [I] \n",
      "[05/19/2024-18:23:03] [I] === Performance summary ===\n",
      "[05/19/2024-18:23:03] [I] Throughput: 289.052 qps\n",
      "[05/19/2024-18:23:03] [I] Latency: min = 1.65717 ms, max = 2.18042 ms, mean = 1.84688 ms, median = 1.84778 ms, percentile(99%) = 1.92285 ms\n",
      "[05/19/2024-18:23:03] [I] End-to-End Host Latency: min = 1.72766 ms, max = 3.46997 ms, mean = 3.45465 ms, median = 3.4585 ms, percentile(99%) = 3.46558 ms\n",
      "[05/19/2024-18:23:03] [I] Enqueue Time: min = 1.09937 ms, max = 3.28137 ms, mean = 1.90693 ms, median = 1.86897 ms, percentile(99%) = 3.13721 ms\n",
      "[05/19/2024-18:23:03] [I] H2D Latency: min = 0.0102234 ms, max = 0.0116577 ms, mean = 0.0105984 ms, median = 0.010498 ms, percentile(99%) = 0.0113525 ms\n",
      "[05/19/2024-18:23:03] [I] GPU Compute Time: min = 1.63678 ms, max = 2.1582 ms, mean = 1.82433 ms, median = 1.82544 ms, percentile(99%) = 1.89868 ms\n",
      "[05/19/2024-18:23:03] [I] D2H Latency: min = 0.00976562 ms, max = 0.0150146 ms, mean = 0.0119536 ms, median = 0.0118713 ms, percentile(99%) = 0.0144043 ms\n",
      "[05/19/2024-18:23:03] [I] Total Host Walltime: 3.00292 s\n",
      "[05/19/2024-18:23:03] [I] Total GPU Compute Time: 1.58352 s\n",
      "[05/19/2024-18:23:03] [W] * Throughput may be bound by Enqueue Time rather than GPU Compute and the GPU may be under-utilized.\n",
      "[05/19/2024-18:23:03] [W]   If not already in use, --useCudaGraph (utilize CUDA graphs where possible) may increase the throughput.\n",
      "[05/19/2024-18:23:03] [I] Explanations of the performance metrics are printed in the verbose logs.\n",
      "[05/19/2024-18:23:03] [I] \n",
      "&&&& PASSED TensorRT.trtexec [TensorRT v8205] # /home/jetson/TensorRT/build/trtexec --fp16 --onnx=modele_classification_multi_assises.onnx --saveEngine=model_classification_multi_assises.engine\n"
     ]
    }
   ],
   "source": [
    "!/home/jetson/TensorRT/build/trtexec --fp16 --onnx=\"modele_classification_multi_assises.onnx\" --saveEngine=\"model_classification_multi_assises.engine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
