{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b63a84d2-006f-45c7-b4bf-d9fe8ece4e62",
   "metadata": {},
   "source": [
    "### **Model Details**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd18d58b-c4c9-409b-ac8c-9bc7ab584edf",
   "metadata": {},
   "source": [
    "Convolutional neural network model that runs on RGB images and predicts human joint locations of a single person. The model is designed to be run in the browser using Tensorflow.js or on devices using TF Lite in real-time, targeting movement/fitness activities. This variant: MoveNet.SinglePose.Thunder is a higher capacity model (compared to MoveNet.SinglePose.Lightning) that performs better prediction quality while still achieving real-time (<30FPS) speed. Naturally, thunder will lag behind the lightning, but it will pack a punch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880aabba-2926-436a-ab53-11ae118e471f",
   "metadata": {},
   "source": [
    "<center><img src=\"https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Olympidades2024/Presentation-MoveNet.gif?raw=true\" width=500></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770a5180-e3ea-4ec5-acd9-5188b3abe209",
   "metadata": {},
   "source": [
    "#### **Model Architecture**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1faff1-c6c9-4a2e-8c15-3f50b879dcdf",
   "metadata": {},
   "source": [
    "MobileNetV2 image feature extractor with Feature Pyramid Network decoder (to stride of 4) followed by CenterNet prediction heads with custom post-processing logics. Lightning uses depth multiplier 1.75."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d1a510-52d8-4ef5-bb69-69b9eb6bdd6f",
   "metadata": {},
   "source": [
    "<center><img src=\"https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Olympidades2024/ArchiMoveNet.png?raw=true\" width=700></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf473a9-ef8f-496d-b669-8a4c7e9b0d38",
   "metadata": {},
   "source": [
    "https://ieeexplore.ieee.org/document/9406043  \n",
    "  \n",
    "  \n",
    "https://www.kaggle.com/models/google/movenet/frameworks/tensorFlow2/variations/multipose-lightning  \n",
    "https://www.tensorflow.org/hub/tutorials/movenet  \n",
    "https://www.tensorflow.org/lite/tutorials/pose_classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc49797b-0425-4310-afa6-75748356ef23",
   "metadata": {},
   "source": [
    "**Inputs**\n",
    "\n",
    "A frame of video or an image, represented as an int32 tensor of dynamic shape: 1xHxWx3, where H and W need to be a **multiple of 32** and the larger dimension is recommended to be 256. To prepare the input image tensor, one should resize (and pad if needed) the image such that the above conditions are hold. Please see the Usage section for more detailed explanation. Note that the size of the input image controls the tradeoff between speed vs. accuracy so choose the value that best suits your application. The channel order is RGB with values in [0, 255]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28698581-2fd9-41b9-8ef0-c958aba7c7f1",
   "metadata": {},
   "source": [
    "**Outputs**\n",
    "\n",
    "A float32 tensor of shape [1, 6, 56].\n",
    "\n",
    "* The first dimension is the batch dimension, which is always equal to 1.\n",
    "* The second dimension corresponds to the maximum number of instance detections. The model can detect up to **6** people in the image frame simultaneously.\n",
    "* The third dimension represents the predicted bounding box/keypoint locations and scores.\n",
    "  * The first **17 * 3** elements are the keypoint locations and scores in the format: [y_0, x_0, s_0, y_1, x_1, s_1, …, y_16, x_16, s_16], where y_i, x_i, s_i are the yx-coordinates (normalized to image frame, e.g. range in [0.0, 1.0]) and confidence scores of the i-th joint correspondingly.\n",
    "  * The order of the 17 keypoint joints is: [nose, left eye, right eye, left ear, right ear, left shoulder, right shoulder, left elbow, right elbow, left wrist, right wrist, left hip, right hip, left knee, right knee, left ankle, right ankle].\n",
    "  * The remaining 5 elements [ymin, xmin, ymax, xmax, score] represent the region of the bounding box (in normalized coordinates) and the confidence score of the instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63377653-28c9-4dc4-9c6c-3697ae054c91",
   "metadata": {},
   "source": [
    "<center><img src=\"https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Olympidades2024/keypoints.png?raw=true\" width=500></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad58e87-9cc2-4a64-9c2d-b0054f267ab9",
   "metadata": {},
   "source": [
    "**Usage**\n",
    "\n",
    "The following code snippet shows how to load and run the model inference on an input image in Python. We recommend to resize and pad your image to the height/width such that:\n",
    "\n",
    "* The height/width are both multiple of 32.\n",
    "* The height to width ratio is close (and enough) to cover the original image's aspect ratio.\n",
    "* Make the larger side to be 256 (one should adjust this based on the speed/accuracy requirements). For example, a 720p image (i.e. 720x1280 (HxW)) should be resized and padded to 160x256 image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04aa5748-6c78-413c-86f6-9260c198d5a4",
   "metadata": {},
   "source": [
    "### Téléchargement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c6498a-9063-4c56-8353-211a0b9ba6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --no-check-certificate -O MultiPosteLightning.tar.gz 'https://docs.google.com/uc?export=download&id=1jVkGdxpEBOFyvgmr7q9R-xCUzw2WGRGo'\n",
    "!mkdir SavedModel\n",
    "!tar -xf MultiPosteLightning.tar.gz --directory SavedModel\n",
    "!rm MultiPosteLightning.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b90eb6-7975-4a7a-b49d-cfa47d259980",
   "metadata": {},
   "source": [
    "# Utilisation du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0107a130-f105-49e2-9fcc-ba4d544dfb1c",
   "metadata": {},
   "source": [
    "### Chargement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96d3df8-ec8b-4c88-a22f-bdb0d992d458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model_path = \"SavedModel\"\n",
    "model = tf.saved_model.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c7842b-d16d-410c-9fa1-43747068c20a",
   "metadata": {},
   "source": [
    "### Chargement d'une image dans le tenseur d'entrée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4d82b2-49c1-47e3-91ba-f1bf2d27ea89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "\n",
    "img_data = tf.io.gfile.GFile('models/research/object_detection/test_images/image2.jpg', 'rb').read()\n",
    "image = Image.open(BytesIO(img_data))\n",
    "(im_width, im_height) = image.size\n",
    "image_np  = np.array(image.getdata()).reshape((im_height, im_width, 3)).astype(np.uint8)\n",
    "input_tensor = np.expand_dims(image_np, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
