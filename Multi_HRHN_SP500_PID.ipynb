{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multi_HRHN_SP500_PID.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexandreBourrieau/FICHIERS/blob/main/Multi_HRHN_SP500_PID.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Luvr5mg72jn"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "from keras import backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SngD1T5rE09j"
      },
      "source": [
        "# Initialisation TPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azq2F27MXmeS"
      },
      "source": [
        "import os\n",
        "\n",
        "use_tpu = True\n",
        "\n",
        "if use_tpu:\n",
        "    assert 'COLAB_TPU_ADDR' in os.environ, 'Missing TPU; did you request a TPU in Notebook Settings?'\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  TPU_ADDRESS = 'grpc://{}'.format(os.environ['COLAB_TPU_ADDR'])\n",
        "else:\n",
        "  TPU_ADDRESS = ''\n",
        "\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_ADDRESS)\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "print(\"All devices: \", tf.config.list_logical_devices('TPU'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArXLu7v7ZiZP"
      },
      "source": [
        "# Chargement et correction des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg8UqC4JTMqD"
      },
      "source": [
        "Le dataset utilisé est SP500..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNPjm5bA9_u8"
      },
      "source": [
        "**1. Chargement des fichiers CSV**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WwTu0bDquT2"
      },
      "source": [
        "!rm *.csv\n",
        "!curl --location --remote-header-name --remote-name \"https://github.com/AlexandreBourrieau/FICHIERS/raw/main/Series_Temporelles/Multi/Data/SPX2010_2021.csv\"\n",
        "!curl --location --remote-header-name --remote-name \"https://github.com/AlexandreBourrieau/FICHIERS/raw/main/Series_Temporelles/Multi/Data/VIX2010_2021.csv\"\n",
        "!curl --location --remote-header-name --remote-name \"https://github.com/AlexandreBourrieau/FICHIERS/raw/main/Series_Temporelles/Multi/Data/US10Y2010_2021.csv\"\n",
        "!curl --location --remote-header-name --remote-name \"https://github.com/AlexandreBourrieau/FICHIERS/raw/main/Series_Temporelles/Multi/Data/TYVIX2010_2021.csv\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z66721h8-CY1"
      },
      "source": [
        "**2. Analyse et correction des données S&P500**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffclRRHzqxYO"
      },
      "source": [
        "# Création de la série sous Pandas\n",
        "df_SP500 = pd.read_csv(\"SPX2010_2021.csv\",sep=\";\",names=[\"Date\",\"SP500_O\",\"SP500_H\",\"SP500_L\",\"SP500_C\"],decimal=\",\")\n",
        "df_SP500['Date'] = pd.to_datetime(df_SP500['Date'])\n",
        "df_SP500 = df_SP500.set_index(\"Date\")\n",
        "df_SP500 = df_SP500.asfreq(freq=\"1D\")\n",
        "df_SP500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0rshQNtq2P-"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(x=np.linspace(0,len(df_SP500),len(df_SP500)+1),y=df_SP500['SP500_C'], line=dict(color='blue', width=1),name=\"Index\"))\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2v6hCl_2yZB"
      },
      "source": [
        "**2. Analyse et correction des données VIX**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1lBSgjy2yZC"
      },
      "source": [
        "# Création de la série sous Pandas\n",
        "df_VIX = pd.read_csv(\"VIX2010_2021.csv\",sep=\";\",names=[\"Date\",\"VIX_O\",\"VIX_H\",\"VIX_L\",\"VIX_C\"],decimal=\",\")\n",
        "df_VIX['Date'] = pd.to_datetime(df_VIX['Date'])\n",
        "df_VIX = df_VIX.set_index(\"Date\")\n",
        "df_VIX = df_VIX.asfreq(freq=\"1D\")\n",
        "df_VIX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjcWJ1fw2yZD"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(x=np.linspace(0,len(df_VIX),len(df_VIX)+1),y=df_VIX['VIX_C'], line=dict(color='blue', width=1),name=\"Index\"))\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcV_ZlnD57Y5"
      },
      "source": [
        "**3. Analyse et correction des données US10Y**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NL5Ifonj57Y6"
      },
      "source": [
        "# Création de la série sous Pandas\n",
        "df_US10Y= pd.read_csv(\"US10Y2010_2021.csv\",sep=\";\",names=[\"Date\",\"US10Y_O\",\"US10Y_H\",\"US10Y_L\",\"US10Y_C\"],decimal=\",\")\n",
        "df_US10Y['Date'] = pd.to_datetime(df_US10Y['Date'])\n",
        "df_US10Y = df_US10Y.set_index(\"Date\")\n",
        "df_US10Y = df_US10Y.asfreq(freq=\"1D\")\n",
        "df_US10Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaeKBt1V57Y9"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(x=np.linspace(0,len(df_US10Y),len(df_US10Y)+1),y=df_US10Y['US10Y_C'], line=dict(color='blue', width=1),name=\"Index\"))\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIH2nnno-aXp"
      },
      "source": [
        "**4. Analyse et correction des données TYVIX**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JjB4FLL-aXq"
      },
      "source": [
        "# Création de la série sous Pandas\n",
        "df_TYVIX= pd.read_csv(\"TYVIX2010_2021.csv\",sep=\";\",names=[\"Date\",\"TYVIX_C\"],decimal=\",\")\n",
        "df_TYVIX['Date'] = pd.to_datetime(df_TYVIX['Date'])\n",
        "df_TYVIX = df_TYVIX.set_index(\"Date\")\n",
        "df_TYVIX = df_TYVIX.asfreq(freq=\"1D\")\n",
        "df_TYVIX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1hj1rle-aXr"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(x=np.linspace(0,len(df_TYVIX),len(df_TYVIX)+1),y=df_TYVIX['TYVIX_C'], line=dict(color='blue', width=1),name=\"Index\"))\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVQ772Gu_3eE"
      },
      "source": [
        "**5. Fusion des données et correction des valeurs NaN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgnvBRSvAjq3"
      },
      "source": [
        "from functools import reduce\n",
        "\n",
        "df_liste = [df_SP500,df_VIX,df_US10Y,df_TYVIX]\n",
        "\n",
        "df_etude = reduce(lambda  left,right: pd.merge(left,right,on=['Date'],how='outer'), df_liste)\n",
        "df_etude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "px1DRPuSBfXc"
      },
      "source": [
        "debut = \"2011-01-10\"\n",
        "fin = \"2020-04-24\"\n",
        "\n",
        "mask = (df_etude.index >= debut) & (df_etude.index <= fin)\n",
        "df_etude = df_etude.loc[mask]\n",
        "df_etude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_n0i_0lhCjPT"
      },
      "source": [
        "df_etude.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGO9fDfwC-FL"
      },
      "source": [
        "df_etude = df_etude.interpolate(method=\"linear\")\n",
        "df_etude.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8I2iqRsDZy7"
      },
      "source": [
        "Déplace la cible à la fin :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnq-XsMRDRUQ"
      },
      "source": [
        "col = df_etude.pop('SP500_C')\n",
        "df_etude.insert(len(df_etude.columns),\"SP500_C\",col)\n",
        "df_etude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlTj0KPxEaSO"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(x=np.linspace(0,len(df_etude),len(df_etude)+1),y=df_etude['SP500_C'], line=dict(color='blue', width=1),name=\"Index\"))\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8CVmrVUCMh5"
      },
      "source": [
        "# Séparation des données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKbWLsWRCMh6"
      },
      "source": [
        "# Sépare les données en entrainement et tests\n",
        "# 80% / 10% / 10%\n",
        "\n",
        "temps_separation = int(len(df_etude.values) * 0.8)\n",
        "temps_separation_PID = int(len(df_etude.values) * 0.9)\n",
        "date_separation = df_etude.index[temps_separation]\n",
        "date_separation_PID = df_etude.index[temps_separation_PID]\n",
        "\n",
        "serie_entrainement_X = np.array(df_etude.values[:temps_separation],dtype=np.float32)\n",
        "serie_test_X = np.array(df_etude.values[temps_separation:],dtype=np.float32)\n",
        "\n",
        "serie_entrainement_PID_X = np.array(df_etude.values[temps_separation:temps_separation_PID],dtype=np.float32)\n",
        "serie_test_PID_X = np.array(df_etude.values[temps_separation_PID:],dtype=np.float32)\n",
        "\n",
        "\n",
        "print(\"Taille de l'entrainement : %d\" %len(serie_entrainement_X))\n",
        "print(\"Taille de la validation : %d\" %len(serie_test_X))\n",
        "print(\"Taille de l'entrainement PID : %d\" %len(serie_entrainement_PID_X))\n",
        "print(\"Taille de la validation PID: %d\" %len(serie_test_PID_X))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsZWwM0-CMh7"
      },
      "source": [
        "**Normalisation des données :**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yniWB2X8CMh8"
      },
      "source": [
        "On normalise les données à l'aide de la fonction [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emf7MqosCMh8"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "# Constrution des séries\n",
        "serie_entrainement_X_norm = []\n",
        "serie_test_X_norm = []\n",
        "\n",
        "serie_entrainement_PID_X_norm = []\n",
        "serie_test_PID_X_norm = []\n",
        "\n",
        "\n",
        "for i in range(0,len(df_etude.columns)):\n",
        "  serie_entrainement_X_norm.append(serie_entrainement_X[:,i])\n",
        "  serie_test_X_norm.append(serie_test_X[:,i])\n",
        "\n",
        "  serie_entrainement_PID_X_norm.append(serie_entrainement_PID_X[:,i])\n",
        "  serie_test_PID_X_norm.append(serie_test_PID_X[:,i])\n",
        "\n",
        "serie_entrainement_X_norm = tf.convert_to_tensor(serie_entrainement_X_norm)\n",
        "serie_entrainement_X_norm = tf.transpose(serie_entrainement_X_norm)\n",
        "serie_test_X_norm = tf.convert_to_tensor(serie_test_X_norm)\n",
        "serie_test_X_norm = tf.transpose(serie_test_X_norm)\n",
        "serie_entrainement_PID_X_norm = tf.convert_to_tensor(serie_entrainement_PID_X_norm)\n",
        "serie_entrainement_PID_X_norm = tf.transpose(serie_entrainement_PID_X_norm)\n",
        "serie_test_PID_X_norm = tf.convert_to_tensor(serie_test_PID_X_norm)\n",
        "serie_test_PID_X_norm = tf.transpose(serie_test_PID_X_norm)\n",
        "\n",
        "\n",
        "# Initialisaton du MinMaxScaler\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "min_max_scaler.fit(serie_entrainement_X_norm)\n",
        "\n",
        "# Normalisation des séries\n",
        "serie_entrainement_X_norm = min_max_scaler.transform(serie_entrainement_X_norm)\n",
        "serie_test_X_norm = min_max_scaler.transform(serie_test_X_norm)\n",
        "serie_entrainement_PID_X_norm = min_max_scaler.transform(serie_entrainement_PID_X_norm)\n",
        "serie_test_PID_X_norm = min_max_scaler.transform(serie_test_PID_X_norm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoBbMQIICMh9"
      },
      "source": [
        "print(serie_entrainement_X_norm.shape)\n",
        "print(serie_test_X_norm.shape)\n",
        "print(serie_entrainement_PID_X_norm.shape)\n",
        "print(serie_test_PID_X_norm.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6THNLf2CMh-"
      },
      "source": [
        "# Affiche quelques séries\n",
        "fig, ax = plt.subplots(constrained_layout=True, figsize=(15,5))\n",
        "\n",
        "ax.plot(df_etude.index[:temps_separation].values,serie_entrainement_X_norm[:,0:1], label=\"X_Ent\")\n",
        "ax.plot(df_etude.index[temps_separation:temps_separation_PID].values,serie_entrainement_PID_X_norm[:,0:1], label=\"X_PID_Ent\")\n",
        "ax.plot(df_etude.index[temps_separation_PID:].values,serie_test_PID_X_norm[:,0:1], label=\"X_PID_Val\")\n",
        "\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNJjTisMlfgQ"
      },
      "source": [
        "# Création des datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i817Q5rwIL_x"
      },
      "source": [
        "Les datasets sont créés de la manière suivante :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwssikbyIEzc"
      },
      "source": [
        "  <img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/ConstructionDataset.png?raw=true' width=500/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLEjFxNnyWse"
      },
      "source": [
        "**1. Préparation des datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y67w_LmnpiP"
      },
      "source": [
        "# Fonction permettant de créer un dataset à partir des données de la série temporelle\n",
        "# X = {((X1_1,X1_2,...,X1_T),(X2_1,X2_2,...,X2_T),(X3_1,X3_2,...,X3_T)),\n",
        "#       (Y1,Y2,...,YT)}\n",
        "# Y = YT+1\n",
        "\n",
        "def prepare_dataset_XY(seriesX, serieY, longueur_sequence, longueur_sortie, batch_size,shift):\n",
        "  datasetX = tf.data.Dataset.from_tensor_slices(seriesX)\n",
        "  datasetX = datasetX.window(longueur_sequence+longueur_sortie, shift=shift, drop_remainder=True)\n",
        "  datasetX = datasetX.flat_map(lambda x: x.batch(longueur_sequence + longueur_sortie))\n",
        "  datasetX = datasetX.map(lambda x: (x[0:longueur_sequence][:,:]))\n",
        "  datasetX = datasetX.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "\n",
        "  datasetY = tf.data.Dataset.from_tensor_slices(serieY)\n",
        "  datasetY = datasetY.window(longueur_sequence+longueur_sortie, shift=shift, drop_remainder=True)\n",
        "  datasetY = datasetY.flat_map(lambda x: x.batch(longueur_sequence + longueur_sortie))\n",
        "  datasetY = datasetY.map(lambda x: (x[0:longueur_sequence][:,:]))\n",
        "  datasetY = datasetY.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "\n",
        "  datasetYPred = tf.data.Dataset.from_tensor_slices(serieY)\n",
        "  datasetYPred = datasetYPred.window(longueur_sequence+longueur_sortie+1, shift=shift, drop_remainder=True)\n",
        "  datasetYPred = datasetYPred.flat_map(lambda x: x.batch(longueur_sequence + longueur_sortie+1))\n",
        "  datasetYPred = datasetYPred.map(lambda x: (x[0:-1][-1:,:]))\n",
        "  datasetYPred = datasetYPred.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "\n",
        "\n",
        "  dataset = tf.data.Dataset.zip((datasetX,datasetY))\n",
        "  dataset = tf.data.Dataset.zip((dataset,datasetYPred))\n",
        "\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghhUzmxdlj0g"
      },
      "source": [
        "# Définition des caractéristiques du dataset que l'on souhaite créer\n",
        "batch_size = 128\n",
        "longueur_sequence = 10\n",
        "longueur_sortie = 1\n",
        "shift=1\n",
        "\n",
        "# Création du dataset\n",
        "dataset = prepare_dataset_XY(serie_entrainement_X_norm[:,0:-1],serie_entrainement_X_norm[:,-1:], longueur_sequence,longueur_sortie,batch_size,shift)\n",
        "dataset_val = prepare_dataset_XY(serie_test_X_norm[:,0:-1],serie_test_X_norm[:,-1:],longueur_sequence,longueur_sortie,batch_size,shift)\n",
        "\n",
        "dataset_PID = prepare_dataset_XY(serie_entrainement_PID_X_norm[:,0:-1],serie_entrainement_PID_X_norm[:,-1:], longueur_sequence,longueur_sortie,batch_size,shift)\n",
        "dataset_PID_val = prepare_dataset_XY(serie_test_PID_X_norm[:,0:-1],serie_test_PID_X_norm[:,-1:],longueur_sequence,longueur_sortie,batch_size,shift)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mJX_otLmJ7w"
      },
      "source": [
        "print(len(list(dataset.as_numpy_iterator())))\n",
        "for element in dataset.take(1):\n",
        "  print(element[0][0].shape)            # ((X1),(X2),...) = ((X1_1,X1_2,...,X1_T),(X2_1,X2_2,...,X2_T),...)\n",
        "  print(element[0][1].shape)            # (Y1,Y2,...,YT)\n",
        "  print(element[1].shape)               # YT+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qKsAMMRn2JI"
      },
      "source": [
        "print(len(list(dataset_val.as_numpy_iterator())))\n",
        "for element in dataset_val.take(1):\n",
        "  print(element[0][0].shape)            # ((X1),(X2),...) = ((X1_1,X1_2,...,X1_T),(X2_1,X2_2,...,X2_T),...)\n",
        "  print(element[0][1].shape)            # Y1,Y2,...,YT\n",
        "  print(element[1].shape)               # YT+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULb_qQmuwvYf"
      },
      "source": [
        "print(len(list(dataset_PID.as_numpy_iterator())))\n",
        "for element in dataset_PID.take(1):\n",
        "  print(element[0][0].shape)            # ((X1),(X2),...) = ((X1_1,X1_2,...,X1_T),(X2_1,X2_2,...,X2_T),...)\n",
        "  print(element[0][1].shape)            # Y1,Y2,...,YT\n",
        "  print(element[1].shape)               # YT+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNNwRyPZwypG"
      },
      "source": [
        "print(len(list(dataset_PID_val.as_numpy_iterator())))\n",
        "for element in dataset_PID_val.take(1):\n",
        "  print(element[0][0].shape)            # ((X1),(X2),...) = ((X1_1,X1_2,...,X1_T),(X2_1,X2_2,...,X2_T),...)\n",
        "  print(element[0][1].shape)            # Y1,Y2,...,YT\n",
        "  print(element[1].shape)               # YT+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuecK3H6GUeX"
      },
      "source": [
        "**3. Préparation des X/Y**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpCqWrvonaB3"
      },
      "source": [
        "X1 = []\n",
        "X2 = []\n",
        "\n",
        "# Extrait les X,Y du dataset\n",
        "x,y = tuple(zip(*dataset))              # x=43x((BS,10,3),(BS,9,1))\n",
        "                                        # y=43x(BS,1,1)\n",
        "for i in range(len(x)):\n",
        "  X1.append(x[i][0])          \n",
        "  X2.append(x[i][1])\n",
        "\n",
        "X1 = tf.convert_to_tensor(X1)           # (43,BS,10,3)\n",
        "X2 = tf.convert_to_tensor(X2)           # (43,BS,9,1)\n",
        "\n",
        "X1 = np.asarray(X1,dtype=np.float32)    # (43,BS,10,3)\n",
        "X2 = np.asarray(X2,dtype=np.float32)    # (43,BS,10,3)   \n",
        "\n",
        "# Recombine les données\n",
        "y = np.asarray(y,dtype=np.float32)      # 43x(BS,1,1) => (43xBS,1,1)\n",
        "X1 = np.reshape(X1,(X1.shape[0]*X1.shape[1],X1.shape[2],X1.shape[3]))   # (43,BS,10,3) => (43xBS,10,3)\n",
        "X2 = np.reshape(X2,(X2.shape[0]*X2.shape[1],X2.shape[2],X2.shape[3]))   # (43,BS,9,1) => (43*BS,9,1)\n",
        "\n",
        "x_train = [X1,X2]\n",
        "y_train = np.asarray(tf.reshape(y,shape=(y.shape[0]*y.shape[1],longueur_sortie,y.shape[3])))\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_train[0].shape)\n",
        "print(x_train[1].shape)\n",
        "print(y_train.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnDnemp8NujA"
      },
      "source": [
        "X1 = []\n",
        "X2 = []\n",
        "\n",
        "# Extrait les X,Y du dataset\n",
        "x,y = tuple(zip(*dataset_val))              # x=43x((BS,10,3),(BS,9,1))\n",
        "                                        # y=43x(BS,1,1)\n",
        "for i in range(len(x)):\n",
        "  X1.append(x[i][0])          \n",
        "  X2.append(x[i][1])\n",
        "\n",
        "X1 = tf.convert_to_tensor(X1)           # (43,BS,10,3)\n",
        "X2 = tf.convert_to_tensor(X2)           # (43,BS,9,1)\n",
        "\n",
        "X1 = np.asarray(X1,dtype=np.float32)    # (43,BS,10,3)\n",
        "X2 = np.asarray(X2,dtype=np.float32)    # (43,BS,10,3)   \n",
        "\n",
        "# Recombine les données\n",
        "y = np.asarray(y,dtype=np.float32)      # 43x(BS,1,1) => (43xBS,1,1)\n",
        "X1 = np.reshape(X1,(X1.shape[0]*X1.shape[1],X1.shape[2],X1.shape[3]))   # (43,BS,10,3) => (43xBS,10,3)\n",
        "X2 = np.reshape(X2,(X2.shape[0]*X2.shape[1],X2.shape[2],X2.shape[3]))   # (43,BS,9,1) => (43*BS,9,1)\n",
        "\n",
        "x_val = [X1,X2]\n",
        "y_val = np.asarray(tf.reshape(y,shape=(y.shape[0]*y.shape[1],longueur_sortie,y.shape[3])))\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_val[0].shape)\n",
        "print(x_val[1].shape)\n",
        "print(y_val.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UO9AnXmSw8SG"
      },
      "source": [
        "X1 = []\n",
        "X2 = []\n",
        "\n",
        "# Extrait les X,Y du dataset\n",
        "x,y = tuple(zip(*dataset_PID))              # x=43x((BS,10,3),(BS,9,1))\n",
        "                                        # y=43x(BS,1,1)\n",
        "for i in range(len(x)):\n",
        "  X1.append(x[i][0])          \n",
        "  X2.append(x[i][1])\n",
        "\n",
        "X1 = tf.convert_to_tensor(X1)           # (43,BS,10,3)\n",
        "X2 = tf.convert_to_tensor(X2)           # (43,BS,9,1)\n",
        "\n",
        "X1 = np.asarray(X1,dtype=np.float32)    # (43,BS,10,3)\n",
        "X2 = np.asarray(X2,dtype=np.float32)    # (43,BS,10,3)   \n",
        "\n",
        "# Recombine les données\n",
        "y = np.asarray(y,dtype=np.float32)      # 43x(BS,1,1) => (43xBS,1,1)\n",
        "X1 = np.reshape(X1,(X1.shape[0]*X1.shape[1],X1.shape[2],X1.shape[3]))   # (43,BS,10,3) => (43xBS,10,3)\n",
        "X2 = np.reshape(X2,(X2.shape[0]*X2.shape[1],X2.shape[2],X2.shape[3]))   # (43,BS,9,1) => (43*BS,9,1)\n",
        "\n",
        "x_train_PID = [X1,X2]\n",
        "y_train_PID = np.asarray(tf.reshape(y,shape=(y.shape[0]*y.shape[1],longueur_sortie,y.shape[3])))\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_train_PID[0].shape)\n",
        "print(x_train_PID[1].shape)\n",
        "print(y_train_PID.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDwuKx63xUTm"
      },
      "source": [
        "X1 = []\n",
        "X2 = []\n",
        "\n",
        "# Extrait les X,Y du dataset\n",
        "x,y = tuple(zip(*dataset_PID_val))              # x=43x((BS,10,3),(BS,9,1))\n",
        "                                        # y=43x(BS,1,1)\n",
        "for i in range(len(x)):\n",
        "  X1.append(x[i][0])          \n",
        "  X2.append(x[i][1])\n",
        "\n",
        "X1 = tf.convert_to_tensor(X1)           # (43,BS,10,3)\n",
        "X2 = tf.convert_to_tensor(X2)           # (43,BS,9,1)\n",
        "\n",
        "X1 = np.asarray(X1,dtype=np.float32)    # (43,BS,10,3)\n",
        "X2 = np.asarray(X2,dtype=np.float32)    # (43,BS,10,3)   \n",
        "\n",
        "# Recombine les données\n",
        "y = np.asarray(y,dtype=np.float32)      # 43x(BS,1,1) => (43xBS,1,1)\n",
        "X1 = np.reshape(X1,(X1.shape[0]*X1.shape[1],X1.shape[2],X1.shape[3]))   # (43,BS,10,3) => (43xBS,10,3)\n",
        "X2 = np.reshape(X2,(X2.shape[0]*X2.shape[1],X2.shape[2],X2.shape[3]))   # (43,BS,9,1) => (43*BS,9,1)\n",
        "\n",
        "x_PID_val = [X1,X2]\n",
        "y_PID_val = np.asarray(tf.reshape(y,shape=(y.shape[0]*y.shape[1],longueur_sortie,y.shape[3])))\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_PID_val[0].shape)\n",
        "print(x_PID_val[1].shape)\n",
        "print(y_PID_val.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KvuPUL1nXpX"
      },
      "source": [
        "# Création du modèle HRHN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBvAzS8KtFlp"
      },
      "source": [
        "Le modèle HRHN est décrit dans ce document de recherche : [Hierarchical Attention-Based Recurrent Highway Networks for Time Series Prediction](https://arxiv.org/pdf/1806.00685)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKq0mqg2ts2w"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/Mod%C3%A8leHRHN1.png?raw=true' width=700>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrCCnwy8nXp4"
      },
      "source": [
        "**1. Création de l'encodeur**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tn5xnfnuehY"
      },
      "source": [
        "L'encodeur a pour but de créer des représentations cachées des séries exogènes qui prennent en compte les relations spatiales entre ces séries ainsi que les relations temporelles.  \n",
        "Les relations spatiales sont extraitent à l'aide d'un ensemble de réseaux de convolution qui produisent des représentations w1, w2... w(T-1).  \n",
        "Ces représentations sont ensuites codées par un réseau RHN à 3 couches afin d'en extraire les relations temporelles. En sortie de ce réseau RHN, on extrait 3 tenseurs dont chacun contient les (T-1) états cachés de chaque couche du réseau RHN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0mq5BSauQUq"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/HRHN_Encodeur_VueEnsemble.png?raw=true'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJ-boowSGDp3"
      },
      "source": [
        "***a. Création des CNN parallèlisés***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6dF5Cp9vzWB"
      },
      "source": [
        "La structure d'un réseau de convolution est composée de trois couches CNN-1D + Max-pooling :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jga5_ZzKv6CI"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/HRHN_Encodeur_CNN1.png?raw=true'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CMXl2tgwJ76"
      },
      "source": [
        "L'intégration de caque réseau dans Keras est parallélisée :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtR-u7ZqwjTL"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/HRHN_Encodeur_CNN2.png?raw=true'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ETqqvAIGKLi"
      },
      "source": [
        "# Arguments de la méthode __init__\n",
        "#   dim_filtres_cnn   :   liste dimension des filtres ex: [3,3,3]\n",
        "#   nbr_filtres_cnn   :   liste nbr de filtre sur chaque couche ex: [16,32,64]\n",
        "#   dim_max_pooling   :   liste dimension max pooling après chaque couche ex: [3,3,3]\n",
        "\n",
        "class Encodeur_CNN(tf.keras.layers.Layer):\n",
        "  def __init__(self, dim_filtres_cnn, nbr_filtres_cnn, dim_max_pooling,dim_motif):\n",
        "    self.dim_filtres_cnn = dim_filtres_cnn\n",
        "    self.nbr_filtres_cnn = nbr_filtres_cnn\n",
        "    self.dim_max_pooling = dim_max_pooling\n",
        "    self.dim_motif = dim_motif\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  # Création de Tin réseaux de convolution + max_pooling en //\n",
        "  ############################################################\n",
        "  def build(self,input_shape):\n",
        "    convs = []\n",
        "    input_cnns = []\n",
        "\n",
        "    # Création des Tin entrées des réseaux CNN\n",
        "    for i in range(input_shape[1]):\n",
        "        input_cnns.append(tf.keras.Input(shape=(input_shape[2],1)))       # input = Tin*(batch_size,#dim,1)\n",
        "\n",
        "    # Création des Tin réseaux CNN\n",
        "    for i in range(input_shape[1]):\n",
        "      conv = tf.keras.layers.Conv1D(filters=self.nbr_filtres_cnn[0],      # conv : (batch_size,#dim,16)\n",
        "                                    kernel_size=self.dim_filtres_cnn[0],\n",
        "                                    activation='relu',\n",
        "                                    padding='same',\n",
        "                                    strides=1)(input_cnns[i])\n",
        "      conv = tf.keras.layers.MaxPool1D(pool_size=self.dim_max_pooling[0],      # conv : (batch_size,#pooling1,16)\n",
        "                                       padding='same')(conv)\n",
        "      for n in range(1,len(self.dim_filtres_cnn)):\n",
        "        conv = tf.keras.layers.Conv1D(filters=self.nbr_filtres_cnn[n],    # conv : (batch_size,#pooling_x,dim_filtres_cnn[n])\n",
        "                                      kernel_size=self.dim_filtres_cnn[n],\n",
        "                                      activation='relu',\n",
        "                                      padding='same',\n",
        "                                      strides=1)(conv)\n",
        "        conv = tf.keras.layers.MaxPool1D(pool_size=self.dim_max_pooling[n],    # conv : (batch_size,#pooling_x,dim_filtres_cnn[n])\n",
        "                                         padding='same')(conv)\n",
        "      convs.append(conv)\n",
        "    \n",
        "    # Création de la sortie concaténée des Tin réseaux CNN\n",
        "    out = tf.convert_to_tensor(convs)                                     # out : (Tin,batch_size,#pooling,64)\n",
        "    out = tf.transpose(out,perm=[1,0,2,3])                                # out : (batch_size,Tin,#pooling,64)\n",
        "    out = tf.keras.layers.Reshape(                                        # out : (batch_size,Tin,#pooling*64)\n",
        "        target_shape=(out.shape[1],out.shape[2]*out.shape[3]))(out)\n",
        "\n",
        "    if self.dim_motif == 0:\n",
        "      out = tf.keras.layers.Dense(units=out.shape[2])(out)                  # out : (batch_size,Tin,dim_motif = #pooling*64) \n",
        "    else:\n",
        "      out = tf.keras.layers.Dense(units=self.dim_motif)(out)                # out : (batch_size,Tin,dim_motif) \n",
        "\n",
        "    # Création du modèle global\n",
        "    self.conv_model = tf.keras.Model(inputs=input_cnns,outputs=out)\n",
        "\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "    \n",
        "  # Entrées :\n",
        "  #     input:  Entrée séries exogènes  : (batch_size,Tin,#dim)\n",
        "  # Sorties :\n",
        "  #     w:      Sorties des motifs CNN  : (batch_size,Tin,#dim_motif)\n",
        "  #                                       (taille dernier filtre=64)\n",
        "  def call(self, input):\n",
        "    # Coupes temporelles sur les séries exogènes\n",
        "    # au format : Tin*(batch_size,#dim,1)\n",
        "    input_list = []\n",
        "    for i in range(input.shape[1]):\n",
        "      input_list.append(tf.transpose(input[:,i:i+1,:],perm=[0,2,1]))      # (batch_size,#dim,1)\n",
        "    # Convolutions spatiales des séries exogènes\n",
        "    w = self.conv_model(input_list)                                       # (batch_size,Tin,dim_motif)\n",
        "    return w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_y68mmiRpR1"
      },
      "source": [
        "***b. Création des cellules RHN***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FQ47zOsxHpx"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/HRHN_Encodeur_RHN.png?raw=true'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvNrS-wQ1B5C"
      },
      "source": [
        "On crée une cellule RHN en reprenant le code précédent auquel :  \n",
        "- On ajoute la possibilité de retourner tous les états cachés de chaque couche\n",
        "- On ajoute la prise en compte de la dimension d'entrée correspondant à la dimension des motifs en sortie des réseaux CNN (dim_motif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CygD9DXbBTDJ"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/Structure_RHN4.png?raw=true'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HldCwl9z3fY"
      },
      "source": [
        "class Cellule_RHN(tf.keras.layers.Layer):\n",
        "  def __init__(self, dim_RHN, nbr_couches, return_all_states = False, dim_input=1):\n",
        "    self.dim_RHN = dim_RHN\n",
        "    self.nbr_couches = nbr_couches\n",
        "    self.dim_input = dim_input\n",
        "    self.return_all_states = return_all_states\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.Wh = self.add_weight(shape=(input_shape[2],self.dim_RHN),initializer=\"normal\",name=\"Wh\")       # (#dim, #RHN)\n",
        "    self.Wt = self.add_weight(shape=(input_shape[2],self.dim_RHN),initializer=\"normal\",name=\"Wt\")       # (#dim, #RHN)\n",
        "    self.Wc = self.add_weight(shape=(input_shape[2],self.dim_RHN),initializer=\"normal\",name=\"Wc\")       # (#dim, #RHN)\n",
        "\n",
        "    self.Rh = self.add_weight(shape=(self.nbr_couches,self.dim_RHN,self.dim_RHN),initializer=\"normal\",name=\"Rh\")      # (n_couches,#RHN, #RHN)\n",
        "    self.Rt = self.add_weight(shape=(self.nbr_couches,self.dim_RHN,self.dim_RHN),initializer=\"normal\",name=\"Rt\")      # (n_couches,#RHN, #RHN)\n",
        "    self.Rc = self.add_weight(shape=(self.nbr_couches,self.dim_RHN,self.dim_RHN),initializer=\"normal\",name=\"Rc\")      # (n_couches,#RHN, #RHN)\n",
        "\n",
        "    self.bh = self.add_weight(shape=(self.nbr_couches,self.dim_RHN,1),initializer=\"normal\",name=\"bh\")        # (n_couches,#RHN, 1)\n",
        "    self.bt = self.add_weight(shape=(self.nbr_couches,self.dim_RHN,1),initializer=\"normal\",name=\"bt\")        # (n_couches,#RHN, 1)\n",
        "    self.bc = self.add_weight(shape=(self.nbr_couches,self.dim_RHN,1),initializer=\"normal\",name=\"bc\")        # (n_couches,#RHN, 1)\n",
        "\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "    # Initialisation des masques de dropout\n",
        "  def InitMasquesDropout(self,drop=0.0):\n",
        "    self.Wh_ = tf.convert_to_tensor(np.random.binomial(n=1,p=1.0-drop,size=(self.dim_input,1)),dtype=tf.float32)                 # (#dim,1)\n",
        "    self.Wt_ = tf.convert_to_tensor(np.random.binomial(n=1,p=1.0-drop,size=(self.dim_input,1)),dtype=tf.float32)                 # (#dim,1)\n",
        "    self.Wc_ = tf.convert_to_tensor(np.random.binomial(n=1,p=1.0-drop,size=(self.dim_input,1)),dtype=tf.float32)                 # (#dim,1)\n",
        "    self.Rh_ = tf.convert_to_tensor(np.random.binomial(n=1,p=1.0-drop,size=(self.nbr_couches,self.dim_RHN,1)),dtype=tf.float32)  # (n_couches,#RHN,1)\n",
        "    self.Rt_ = tf.convert_to_tensor(np.random.binomial(n=1,p=1.0-drop,size=(self.nbr_couches,self.dim_RHN,1)),dtype=tf.float32)  # (n_couches,#RHN,1)\n",
        "    self.Rc_ = tf.convert_to_tensor(np.random.binomial(n=1,p=1.0-drop,size=(self.nbr_couches,self.dim_RHN,1)),dtype=tf.float32)  # (n_couches,#RHN,1)\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:          Entrées X[t]        : (batch_size,1,#dim)\n",
        "  #     init_hidden:    Etat caché Init.    : (batch_size,#RHN)\n",
        "  # Sorties :\n",
        "  #     sL:             Etat caché de la dernière couche       : (batch_size,#RHN) \n",
        "  #           ou        Etats cachés de chaque couche SL[t]    : (batch_size,nbr_couches,#RHN)\n",
        "  def call(self, input, init_hidden=None):\n",
        "    # Construction d'un vecteur d'état nul si besoin\n",
        "    if init_hidden == None:\n",
        "      init_hidden = tf.matmul(tf.zeros(shape=(self.dim_RHN,input.shape[2])), # (#RHN,#dim)X(batch_size,#dim,1) = (batch_size,#RHN,1)\n",
        "                              tf.transpose(input,perm=[0,2,1]))\n",
        "      init_hidden = tf.squeeze(init_hidden,-1)                               # (batch_size,#RHN,1) => (batch_size,#RHN)\n",
        "  \n",
        "    liste_sl = []                                                            # Liste pour  enregistrer les états cachés de chaque couche\n",
        "    # Calcul de hl, tl et cl\n",
        "    for i in range(self.nbr_couches):\n",
        "      if i==0:\n",
        "        # Applique le masque aux poids\n",
        "        Rh = tf.multiply(self.Rh_[0,:,:],self.Rh[0,:,:])                      # (#RHN,1)_x_(#RHN,#RHN) = (#RHN,#RHN)\n",
        "        Rt = tf.multiply(self.Rt_[0,:,:],self.Rt[0,:,:])\n",
        "        Rc = tf.multiply(self.Rc_[0,:,:],self.Rc[0,:,:])\n",
        "\n",
        "        Wh = tf.multiply(self.Wh_,self.Wh)                                    # (#dim,1)_x_(#dim,#RHN) = (#dim,#RHN)\n",
        "        Wt = tf.multiply(self.Wt_,self.Wt)\n",
        "        Wc = tf.multiply(self.Wc_,self.Wc)\n",
        "   \n",
        "        # Calcul de hl\n",
        "        hl = tf.matmul(Rh,tf.expand_dims(init_hidden,-1))                   # (#RHN,#RHN)X(batch_size,#RHN,1) = (batch_size,#RHN,1)\n",
        "        hl = hl + self.bh[0,:,:]                                            # (batch_size,#RHN,1) + (#RHN,1) = (batch_size,#RHN,1)\n",
        "        hl = hl + tf.matmul(tf.transpose(Wh),\n",
        "                            tf.transpose(input,perm=[0,2,1]))               # (#RHN,#dim)X(batch_size,#dim,1) = (batch_size,#RHN,1)\n",
        "        hl = tf.squeeze(hl,-1)                                              # (batch_size,#RHN)\n",
        "        hl = K.tanh(hl)\n",
        "\n",
        "        # Calcul de tl\n",
        "        tl = tf.matmul(Rt,tf.expand_dims(init_hidden,-1))                   # (#RHN,#RHN)X(batch_size,#RHN,1) = (batch_size,#RHN,1)\n",
        "        tl = tl + self.bt[0,:,:]                                            # (batch_size,#RHN,1) + (#RHN,1) = (batch_size,#RHN,1)\n",
        "        tl = tl + tf.matmul(tf.transpose(Wt),\n",
        "                            tf.transpose(input,perm=[0,2,1]))               # (#RHN,#dim)X(batch_size,#dim,1) = (batch_size,#RHN,1)\n",
        "        tl = tf.squeeze(tl,-1)                                              # (batch_size,#RHN)\n",
        "        tl = tf.keras.activations.sigmoid(tl)\n",
        "\n",
        "        # Calcul de cl\n",
        "        cl = tf.matmul(Rc,tf.expand_dims(init_hidden,-1))                   # (#RHN,#RHN)X(batch_size,#RHN,1) = (batch_size,#RHN,1)\n",
        "        cl = cl + self.bc[0,:,:]                                            # (batch_size,#RHN,1) + (#RHN,1) = (batch_size,#RHN,1)\n",
        "        cl = cl + tf.matmul(tf.transpose(Wc),\n",
        "                            tf.transpose(input,perm=[0,2,1]))               # (#RHN,#dim)X(batch_size,#dim,1) = (batch_size,#RHN,1)\n",
        "        cl = tf.squeeze(cl,-1)                                              # (batch_size,#RHN)\n",
        "        cl = tf.keras.activations.sigmoid(cl)\n",
        "\n",
        "      else:\n",
        "        # Applique le masque aux poids\n",
        "        Rh = tf.multiply(self.Rh_[i,:,:],self.Rh[i,:,:])\n",
        "        Rt = tf.multiply(self.Rt_[i,:,:],self.Rt[i,:,:])\n",
        "        Rc = tf.multiply(self.Rc_[i,:,:],self.Rc[i,:,:])\n",
        "\n",
        "        # Calcul de hl\n",
        "        hl = tf.matmul(Rh,tf.expand_dims(init_hidden,-1))                   # (#RHN,#RHN)X(batch_size,#RHN,1) = (batch_size,#RHN,1)\n",
        "        hl = hl + self.bh[i,:,:]                                            # (batch_size,#RHN,1) + (#RHN,1) = (batch_size,#RHN,1)\n",
        "        hl = tf.squeeze(hl,-1)                                              # (batch_size,#RHN)\n",
        "        hl = K.tanh(hl)\n",
        "\n",
        "        # Calcul de tl\n",
        "        tl = tf.matmul(Rt,tf.expand_dims(init_hidden,-1))                   # (#RHN,#RHN)X(batch_size,#RHN,1) = (batch_size,#RHN,1)\n",
        "        tl = tl + self.bt[i,:,:]                                            # (batch_size,#RHN,1) + (#RHN,1) = (batch_size,#RHN,1)\n",
        "        tl = tf.squeeze(tl,-1)                                              # (batch_size,#RHN)\n",
        "        tl = tf.keras.activations.sigmoid(tl)\n",
        "\n",
        "        # Calcul de cl\n",
        "        cl = tf.matmul(Rc,tf.expand_dims(init_hidden,-1))                   # (#RHN,#RHN)X(batch_size,#RHN,1) = (batch_size,#RHN,1)\n",
        "        cl = cl + self.bc[i,:,:]                                            # (batch_size,#RHN,1) + (#RHN,1) = (batch_size,#RHN,1)\n",
        "        cl = tf.squeeze(cl,-1)                                              # (batch_size,#RHN)\n",
        "        cl = tf.keras.activations.sigmoid(cl)\n",
        "      \n",
        "      # Calcul de sl\n",
        "      sl = tf.keras.layers.multiply([hl,tl])                                # (batch_size,#RHN)\n",
        "      sl = sl + tf.keras.layers.multiply([init_hidden,cl])                  # (batch_size,#RHN)\n",
        "      liste_sl.append(sl)       # Sauvegarde l'état caché de la couche courante\n",
        "      init_hidden = sl\n",
        "    if self.return_all_states == False:\n",
        "      return sl\n",
        "    else:\n",
        "      liste_sl = tf.convert_to_tensor(liste_sl)                             # (nbr_couches,batch_size,#RHN)\n",
        "      liste_sl = tf.transpose(liste_sl,perm=[1,0,2])                        # (batch_size,nbr_couches,#RHN)\n",
        "      return liste_sl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJY-TY7W55ZF"
      },
      "source": [
        "***c. Création de l'encodeur : Convolutions + RHN***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMT8C8-UVe2O"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/HRHN_Encodeur_VueEnsemble.png?raw=true'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYQu67IfBdel"
      },
      "source": [
        "# Arguments de la méthode __init__\n",
        "#   dim_filtres_cnn   :   liste dimension des filtres ex: [3,3,3]\n",
        "#   nbr_filtres_cnn   :   liste nbr de filtre sur chaque couche ex: [16,32,64]\n",
        "#   dim_max_pooling   :   liste dimension max pooling après chaque couche ex: [3,3,3]\n",
        "#   dim_motif         :   dimension du motif en sortie du CNN\n",
        "#   dim_RHN           :   dimension du vecteur caché RHN\n",
        "#   nbr_couches_RHN   :   nombre de couches du RHN\n",
        "#   dropout           :   dropout variationnel pour le RHN ex: [0.1]\n",
        "\n",
        "class Encodeur(tf.keras.layers.Layer):\n",
        "  def __init__(self, dim_filtres_cnn, nbr_filtres_cnn, dim_max_pooling, dim_motif,dim_RHN,nbr_couches_RHN, dropout=0.0):\n",
        "    self.dim_filtres_cnn = dim_filtres_cnn\n",
        "    self.nbr_filtres_cnn = nbr_filtres_cnn\n",
        "    self.dim_max_pooling = dim_max_pooling\n",
        "    self.dim_motif = dim_motif\n",
        "    self.dim_RHN = dim_RHN\n",
        "    self.nbr_couches_RHN = nbr_couches_RHN\n",
        "    self.dropout = dropout\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.encodeur_cnn = Encodeur_CNN(dim_filtres_cnn=self.dim_filtres_cnn,nbr_filtres_cnn=self.nbr_filtres_cnn,dim_max_pooling=self.dim_max_pooling,dim_motif=self.dim_motif)\n",
        "    self.RHN = Cellule_RHN(dim_RHN=self.dim_RHN,nbr_couches=self.nbr_couches_RHN,return_all_states=True,dim_input=self.dim_motif)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "    \n",
        "  # Entrées :\n",
        "  #     input:          Entrées X         : (batch_size,Tin,#dim)\n",
        "  # Sorties :\n",
        "  #     hidden_states   Vecteurs cachés   : (batch_size,nbr_couches,Tin,#RHN)\n",
        "  def call(self, input):\n",
        "    # Convolutions spatiales des séries exogènes\n",
        "    w = self.encodeur_cnn(input)      #  (batch_size,Tin,dim_motif)\n",
        "\n",
        "    # Encodage des motifs CNN avec les cellules RHN\n",
        "    sequence = []\n",
        "    hidden = None\n",
        "\n",
        "    # Initialisation des masques de dropout pour tous les pas de temps\n",
        "    self.RHN.InitMasquesDropout(self.dropout)\n",
        "\n",
        "    # Applique la cellule RHN à chaque pas de temps\n",
        "    for i in range(input.shape[1]):\n",
        "      hidden = self.RHN(w[:,i:i+1,:],hidden)          # Envoie (batch_size,1,dim_motif)\n",
        "      sequence.append(hidden)                         # Sauve (batch_size,nbr_couches,#RHN)\n",
        "\n",
        "      # Le premier état caché du prochain instant\n",
        "      # est l'état caché de la dernière couche précédente\n",
        "      hidden = hidden[:,self.nbr_couches_RHN-1,:]       # (batch_size,#RHN)\n",
        "\n",
        "    # Traite le format des vecteurs cachés de l'encodeur\n",
        "    sequence = tf.convert_to_tensor(sequence)               # (Tin,batch_size,nbr_couches,#RHN)\n",
        "    hidden_states = tf.transpose(sequence,perm=[1,2,0,3])   # (batch_size,nbr_couches,Tin,#RHN)  \n",
        "\n",
        "    return hidden_states"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOHqRH7oASLh"
      },
      "source": [
        "**Test de l'encodeur**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbasFfpdAUkA"
      },
      "source": [
        "x_train[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvygu1kuD9lz"
      },
      "source": [
        "x_train[0][0:5,:,:].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8v427DL7AW5p"
      },
      "source": [
        "s = Encodeur(dim_filtres_cnn=[16,32,64],nbr_filtres_cnn=[3,3,3],dim_max_pooling=[3,3,3],dim_motif=3,dim_RHN=128,nbr_couches_RHN=3,dropout=0.0)(x_train[0][0:5,:,:])\n",
        "s.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMqHZDOD7VM1"
      },
      "source": [
        "entrees_sequences = tf.keras.layers.Input(shape=(10,81))\n",
        "sortie = Encodeur(dim_filtres_cnn=[16,32,64],nbr_filtres_cnn=[3,3,3],dim_max_pooling=[3,3,3],dim_motif=3,dim_RHN=128,nbr_couches_RHN=3,dropout=0.0)(entrees_sequences)\n",
        "model = tf.keras.Model(entrees_sequences,sortie)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__CJ4O7yJne3"
      },
      "source": [
        "**2. Création du décodeur**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lt2yWQeaJwNn"
      },
      "source": [
        "Le décodeur prend en entrée et à chaque pas de temps :  \n",
        "- Le tenseur en sortie de l'encodeur RHN qui contient l'ensemble des vecteurs cachés des différentes couches : (batch_size,Nbr_couches,Tin,#RHN)\n",
        "- L'état caché de la dernière couche du décodeur RHN précédent : (batch_size,#RHN)\n",
        "- La valeur de la série cible à l'instant courant : (batch_size,1,1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtYLxAoIK8Xn"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/HRHN_VueEnsembleDecodeur2.png?raw=true'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjHiRZZbLief"
      },
      "source": [
        "**a. Création de la couche d'attention hiérarchique**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JX5hGeWNN8w"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/HRHN_AttentionHierarchique.png?raw=true'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9p7ylHmY6gS"
      },
      "source": [
        "On commence par créer la fonction permettant de calculer les scores. Cette fonction sera appelée avec la méthode TimeDistributed de Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvaGAb0uY1XL"
      },
      "source": [
        "class CalculScore(tf.keras.layers.Layer):\n",
        "  def __init__(self):\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.T = self.add_weight(shape=(input_shape[1],input_shape[1]),initializer=\"normal\",name=\"T\")            # (#RHN, #RHN)\n",
        "    self.U = self.add_weight(shape=(input_shape[1],input_shape[1]),initializer=\"normal\",name=\"U\")            # (#RHN, #RHN)\n",
        "    self.b = self.add_weight(shape=(input_shape[1],1),initializer=\"normal\",name=\"b\")                         # (#RHN, 1)\n",
        "    self.v = self.add_weight(shape=(input_shape[1],1),initializer=\"normal\",name=\"v\")                         # (#RHN, 1)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  #     hid_state:  Etat initial RHN          : (batch_size,#RHN)\n",
        "  def SetInitState(self,hid_state):\n",
        "    self.hid_state = hid_state\n",
        "\n",
        "  def compute_output_shape(self,input_shape):\n",
        "    return(input_shape[0],1)\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:      1 sortie encodeur RHN     : (batch_size,#RHN)\n",
        "  # Sorties :\n",
        "  #     score:      score                     : (batch_size,1,1)\n",
        "  def call(self, input):\n",
        "    score = tf.matmul(self.U,tf.expand_dims(input,-1))                      # (#RHN,#RHN)x(batch_size,#RHN,1) = (batch_size,#RHN,1)\n",
        "    score = score + tf.matmul(self.T,tf.expand_dims(self.hid_state,-1))     # (batch_size,#RHN,1)\n",
        "    score = score + self.b                                                  # (batch_size,#RHN,1)\n",
        "    score = K.tanh(score)\n",
        "    score = tf.matmul(tf.transpose(self.v),score)                           # (1,#RHN)x(batch_size,#RHN,1) = (batch_size,1,1)\n",
        "    return tf.squeeze(score,-1)                                             # (batch_size,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pF02ysdbxWU"
      },
      "source": [
        "On crée maintenant la couche d'attention hiérarchique :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kxnR9fSVXDC"
      },
      "source": [
        "class AttentionHierarchique(tf.keras.layers.Layer):\n",
        "  def __init__(self):\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.couche_score = CalculScore()\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "    \n",
        "  # Entrées :\n",
        "  #     input:      Sorties d'une couche encodeur RHN       : (batch_size,Tin,#RHN)\n",
        "  #     hid_state:  Etat initial RHN                        : (batch_size,#RHN)\n",
        "  # Sorties :\n",
        "  #     vc:         SousVecteur contexte                    : (batch_size,1,RHN)\n",
        "  def call(self, input, hid_state):\n",
        "    # Calcul des scores\n",
        "    self.couche_score.SetInitState(hid_state)\n",
        "    scores = tf.keras.layers.TimeDistributed(self.couche_score)(input)        # (batch_size,Tin,#RHN) : Timestep = Tin\n",
        "                                                                              # (batch_size,#RHN) envoyé Tin fois\n",
        "                                                                              # (batch_size,Tin,1) retourné\n",
        "    scores = tf.keras.activations.softmax(scores,axis=1)                      # (batch_size,Tin,1)\n",
        "\n",
        "    # Applique les scores aux sorties de la couche RHN\n",
        "    poids = tf.multiply(input,scores)             # (batch_size,Tin,#RHN)_x_(batch_size,Tin,1) = (batch_size,Tin,#RHN)\n",
        "\n",
        "    # Calcul le sous-vecteur contexte\n",
        "    vc = K.sum(poids,axis=1)                      # (batch_size,#RHN)\n",
        "    return tf.expand_dims(vc,1)                   # (batch_size,1,#RHN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slCSUTmyifEY"
      },
      "source": [
        "**b. Création du décodeur**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_efbOikfRwt"
      },
      "source": [
        "Dans le décodeur, on parallélise autant de couches d'attention que nécessaire afin de créer un modèle d'attention multi-entrées."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCElCxBcnUHj"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/ParaDecodeur.png?raw=true'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2nZG3Rrjv3O"
      },
      "source": [
        "class Decodeur(tf.keras.layers.Layer):\n",
        "  def __init__(self,dim_RHN,nbr_couches_RHN,dropout=0.0):\n",
        "    self.dim_RHN = dim_RHN\n",
        "    self.nbr_couches_RHN = nbr_couches_RHN\n",
        "    self.dropout = dropout\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    attentions = []\n",
        "    inputs_attention = []\n",
        "\n",
        "    # Création des \"nbr_couches\" entrées des attentions\n",
        "    # Chaque entrée est une liste : [input,init_state] = [((batch_size,Tin,#RHN)),((batch_size,#RHN))]\n",
        "    for i in range(input_shape[1]):\n",
        "      inputs_attention.append([tf.keras.Input(shape=(input_shape[2],input_shape[3])),          # input = \"nbr_couches\"*(batch_size,Tin,#RHN)\n",
        "                                 tf.keras.Input(shape=(input_shape[3]))])                      # init_state = \"nbr_couches\"*(batch_size,#RHN)\n",
        "\n",
        "    # Création des \"nbr_couches\" couches d'attentions hierarchiques\n",
        "    for i in range(input_shape[1]):\n",
        "      att = AttentionHierarchique()(inputs_attention[i][0],                 # inputs_attention[i][0] : (batch_size,Tin,#RHN)\n",
        "                                    inputs_attention[i][1])                 # inputs_attention[i][1] : (batch_size,#RHN)\n",
        "      attentions.append(att)\n",
        "\n",
        "    # Création de la sortie concaténée des \"nbr_couches\" couches d'attentions\n",
        "    out = tf.convert_to_tensor(attentions)                                # out : (nbr_couches,batch_size,1,#RHN)\n",
        "    out = tf.transpose(out,perm=[1,0,2,3])                                # out : (batch_size,nbr_couches,1,#RHN)\n",
        "\n",
        "    # Création du modèle global\n",
        "    self.att_model = tf.keras.Model(inputs=inputs_attention,outputs=out)\n",
        "\n",
        "    # Création des poids\n",
        "    self.Wtilda = tf.keras.layers.Dense(units=1,activation=None,use_bias=None)\n",
        "    self.Vtilda = tf.keras.layers.Dense(units=1,activation=None,use_bias=True)\n",
        "\n",
        "    # Création du décodeur RHN\n",
        "    self.dec_RHN = Cellule_RHN(dim_RHN=self.dim_RHN,nbr_couches=self.nbr_couches_RHN,return_all_states=False,dim_input=1)\n",
        "   \n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "    \n",
        "  # Entrées :\n",
        "  #     input:      Sorties des couches de l'encodeur RHN   : (batch_size,nbr_couches,Tin,#RHN)\n",
        "  #     hid_state:  Etat initial RHN                        : (batch_size,#RHN)\n",
        "  #     Y:          Valeur de la série cible                : (batch_size,1)\n",
        "  #     only_att    Si =True ne calcul que le vecteur ctx   : True/False\n",
        "  # Sorties :\n",
        "  #     d:          Vecteur contexte                        : (batch_size,nbr_couches*RHN)\n",
        "  #     s:          Vecteur caché décodeur RHN              : (batch_size,#RHN)\n",
        "  def call(self, input, hid_state, Y,only_att):\n",
        "    # Initialisation de l'état caché à 0 si besoin\n",
        "    # Construit le tenseur nul au format (batch_size,#RHN)\n",
        "    if hid_state == None:\n",
        "      coef = tf.expand_dims(input[:,0,0,0],-1)                        # (batch_size,1)\n",
        "      coef = tf.expand_dims(coef,-1)                                  # (batch_size,1,1)\n",
        "      hid_state = tf.matmul(coef,tf.zeros(shape=(1,input.shape[3])))  # (batch_size,1,1)X(1,#RHN) = (batch_size,1,#RHN)\n",
        "      hid_state = tf.squeeze(hid_state,axis=1)                        # (batch_size,#RHN)\n",
        "\n",
        "    # Construction de l'entrée du modèle\n",
        "    # nbr_couches*[((batch_size,Tin,#RHN)),((batch_size,#RHN))]\n",
        "    input_model = []\n",
        "    for i in range(input.shape[1]):\n",
        "      input_model.append([input[:,i,:,:],hid_state])    # [((batch_size,Tin,#RHN)),((batch_size,#RHN))]\n",
        "    \n",
        "    # Calcul des sous-vecteurs contextes\n",
        "    # avec le modèle d'attention hiérarchique parallélisé\n",
        "    d = self.att_model(input_model)                     # d : (batch_size,nbr_couches,1,#RHN)\n",
        "\n",
        "    # Concaténation des sous-vecteurs contextes\n",
        "    d = tf.squeeze(d,axis=2)                            # (batch_size,nbr_couches,#RHN)\n",
        "    d = tf.keras.layers.Flatten()(d)                    # (batch_size,nbr_couches*RHN)\n",
        "\n",
        "    if only_att == False :\n",
        "      # Calcul de y_tilda\n",
        "      ytilda = self.Wtilda(Y)                             # (batch_size,1)\n",
        "      ytilda = ytilda + self.Vtilda(d)                    # (batch_size,1)\n",
        "\n",
        "      # Initialisation des masques de dropout pour tous les pas de temps\n",
        "      self.dec_RHN.InitMasquesDropout(self.dropout)\n",
        "\n",
        "      # Décodage avec le réseau RHN\n",
        "      s = self.dec_RHN(tf.expand_dims(ytilda,-1),hid_state)                  # (batch_size,#RHN)\n",
        "      return d,s\n",
        "    else:\n",
        "      return d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYOTdM7fZT65"
      },
      "source": [
        "**3. Création de la couche HRHN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhML2b5bFsZB"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/Gene_HRHN.png?raw=true'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PCEHUDEZ1bt"
      },
      "source": [
        "class Net_HRHN(tf.keras.layers.Layer):\n",
        "  def __init__(self,encodeur,decodeur,longueur_sequence, dim_RHN, regul=0.0, drop = 0.0):\n",
        "    self.encodeur = encodeur\n",
        "    self.decodeur = decodeur\n",
        "    self.longueur_sequence = longueur_sequence\n",
        "    self.regul = regul\n",
        "    self.drop = drop\n",
        "    self.dim_RHN = dim_RHN\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.W = tf.keras.layers.Dense(units=1,activation=None,use_bias=None)\n",
        "    self.V = tf.keras.layers.Dense(units=1,activation=None,use_bias=True)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:          Entrées X           : (batch_size,Tin,#dim)\n",
        "  #     output_seq:     Sortie séquence Y   : (batch_size,Tin,1)\n",
        "  # Sorties :\n",
        "  #     sortie:         Prédiction Y        : (batch_size,1,1)\n",
        "  def call(self,input,output_seq):\n",
        "    # Appel de l'encodeur\n",
        "    # Récupère l'ensemble des états cachés de l'encodeur RHN\n",
        "    H = self.encodeur(input)                # (batch_size,nbr_couches,Tin,#RHN)\n",
        "\n",
        "    # Décodage\n",
        "    hidden_state = None\n",
        "    for t in range(input.shape[1]):\n",
        "      vc, hidden_state = self.decodeur(H,hidden_state,output_seq[:,t:t+1,0],only_att = False)\n",
        "    \n",
        "    # Couche d'attention finale\n",
        "    vc = self.decodeur(H,hidden_state,output_seq[:,0,0],only_att=True)\n",
        "\n",
        "    # Génération de la prédiction\n",
        "    sortie = self.W(hidden_state) + self.V(vc)        # (batch_size,1)\n",
        "    return tf.expand_dims(sortie,-1)                  # (batch_size,1,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8_PgjEpdC8z"
      },
      "source": [
        "#Mise en place du modèle HRHN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMgLi7JPdFTq"
      },
      "source": [
        "dim_RHN = 128\n",
        "nbr_filtres_cnn = [16,32,64]\n",
        "dim_filtres_cnn = [3,3,3]\n",
        "dim_max_pooling = [3,3,3]\n",
        "nbr_couches_RHN = 3\n",
        "#dim_motif = 3\n",
        "dim_motif = Encodeur_CNN(dim_filtres_cnn=dim_filtres_cnn,nbr_filtres_cnn=nbr_filtres_cnn,dim_max_pooling=dim_max_pooling,dim_motif=0)(x_train[0][0:1,:,:]).shape[2]\n",
        "drop=0.0\n",
        "\n",
        "def get_model_HRHN():\n",
        "  entrees_sequences = tf.keras.layers.Input(shape=(longueur_sequence,x_train[0].shape[2]))\n",
        "  sorties_sequence = tf.keras.layers.Input(shape=(longueur_sequence,1))\n",
        "\n",
        "  encodeur = Encodeur(dim_filtres_cnn=dim_filtres_cnn,nbr_filtres_cnn=nbr_filtres_cnn,dim_max_pooling=dim_max_pooling,dim_motif=dim_motif,dim_RHN=dim_RHN,nbr_couches_RHN=nbr_couches_RHN,dropout=drop)\n",
        "  decodeur = Decodeur(dim_RHN=dim_RHN,nbr_couches_RHN=nbr_couches_RHN,dropout=drop)\n",
        "\n",
        "  sortie = Net_HRHN(encodeur,decodeur,longueur_sequence=longueur_sequence,dim_RHN=dim_RHN,drop=drop)(entrees_sequences,sorties_sequence)\n",
        "\n",
        "  model = tf.keras.Model([entrees_sequences,sorties_sequence],sortie)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1Xeyk8B-dlm"
      },
      "source": [
        "# Chargement du modèle pré-entrainé"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fOahfnJ-yX1"
      },
      "source": [
        "**HRHN SP500 #1**  \n",
        "  - longueur_sequence = 64      \n",
        "  - dim_RHN = 128  \n",
        "  - dim_filtres_cnn = [16,32,64]  \n",
        "  - nbr_filtres_cnn = [2,2,2]  \n",
        "  - dim_max_pooling = [2,2,2]  \n",
        "  - nbr_couches_RHN = 3  \n",
        "  - dim_motif = auto  \n",
        "  - drop=0.00  \n",
        "\t=> mse : 4.2054e-06/0.0043   \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_05bqnRr-pyf"
      },
      "source": [
        "!rm *.hdf5\n",
        "!curl --location --remote-header-name --remote-name \"https://github.com/AlexandreBourrieau/FICHIERS/raw/main/Series_Temporelles/Multi/Models/Multi_HRHN_SP500_3.hdf5\"\n",
        "!curl --location --remote-header-name --remote-name \"https://github.com/AlexandreBourrieau/FICHIERS/raw/main/Series_Temporelles/Multi/Models/Multi_HRHN_SP500_1.hdf5\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAvoqGqeVroL"
      },
      "source": [
        "strategy = tf.distribute.TPUStrategy(resolver)\n",
        "with strategy.scope():\n",
        "  model_HRHN = get_model_HRHN()\n",
        "  model_HRHN.compile(loss=\"mse\")\n",
        "  model_HRHN.load_weights(\"Multi_HRHN_SP500_1.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfIkATObr8cV"
      },
      "source": [
        "# Prédictions single-step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfA2H5J4S35o"
      },
      "source": [
        "pred_ent = model_HRHN.predict([x_train[0],x_train[1]],verbose=1)\n",
        "pred_val = model_HRHN.predict([x_val[0],x_val[1]],verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Q1NA36BFZ4N"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "decalage = 1\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Courbes originales\n",
        "fig.add_trace(go.Scatter(x=df_etude.index,y=serie_entrainement_X_norm[:,-1],line=dict(color='blue', width=1)))\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[temps_separation:],y=serie_test_X_norm[:,-1],line=dict(color='red', width=1)))\n",
        "\n",
        "#Affiche les prédictions sur l'entrainement\n",
        "pred = []\n",
        "\n",
        "max = len(pred_ent)\n",
        "max = max\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_ent[i,0:decalage,:],1))\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[longueur_sequence:],y=pred, mode='lines', line=dict(color='green', width=1)))\n",
        "\n",
        "#Affiche les prédictions sur les validations\n",
        "pred = []\n",
        "max = len(pred_val)\n",
        "max = max\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_val[i,0:decalage,:],1))\n",
        "\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[temps_separation+longueur_sequence:],y=pred, mode='lines', line=dict(color='green', width=1)))\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9wCeoEnMEYv"
      },
      "source": [
        "**Zone d'entrainement du modèle HRHN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2_ceD69Mju6"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "decalage = 1\n",
        "pred = []\n",
        "\n",
        "max = len(pred_ent)\n",
        "max = max\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_ent[i,0:decalage,:],1))\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[longueur_sequence:],y=serie_entrainement_X_norm[longueur_sequence:-(serie_entrainement_X_norm[longueur_sequence:,:].shape[0]-pred.shape[0]),-1],line=dict(color='blue', width=1)))\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[longueur_sequence:],y=pred,line=dict(color='green', width=1)))\n",
        "\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1VVQShj3Bcs"
      },
      "source": [
        "**Zone de validation du modèle HRHN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WV5JTpnnNApH"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "decalage = 1\n",
        "pred = []\n",
        "\n",
        "max = len(pred_val)\n",
        "max = max\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_val[i,0:decalage,:],1))\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "\n",
        "# Calcul les valeurs non prédites dans l'intervalle de validation\n",
        "manque_pred = serie_test_X_norm[longueur_sequence:,:].shape[0]-pred.shape[0]\n",
        "\n",
        "# Affiche les valeurs originales de l'intervalle de validation\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[temps_separation+longueur_sequence:-manque_pred:],y=serie_test_X_norm[longueur_sequence:-manque_pred,-1],line=dict(color='blue', width=1)))\n",
        "\n",
        "# Affiche les prédictions sur l'intervalle d'entrainement PID\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[temps_separation+longueur_sequence:temps_separation_PID],y=pred[:temps_separation_PID-temps_separation-longueur_sequence],line=dict(color='green', width=1)))\n",
        "\n",
        "# Affiche les prédictions sur l'intervalle de validation PID\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[temps_separation_PID:-manque_pred],y=pred[temps_separation_PID-temps_separation-longueur_sequence:],line=dict(color='red', width=1)))\n",
        "\n",
        "\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyFQ-G2I3Gi2"
      },
      "source": [
        "**Erreurs sur les zones PID**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UPolG5m3RPE"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "decalage = 1\n",
        "pred = []\n",
        "\n",
        "max = len(pred_val)\n",
        "max = max\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_val[i,0:decalage,:],1))\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "\n",
        "# Calcul les valeurs non prédites dans l'intervalle de validation\n",
        "manque_pred = serie_test_X_norm[longueur_sequence:,:].shape[0]-pred.shape[0]\n",
        "\n",
        "# Affiche les valeurs originales de l'intervalle d'entrainement PID\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[temps_separation+longueur_sequence:temps_separation_PID],y=serie_entrainement_PID_X_norm[longueur_sequence:,-1],line=dict(color='blue', width=1)))\n",
        "\n",
        "# Affiche les prédictions sur l'intervalle d'entrainement PID\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[temps_separation+longueur_sequence:temps_separation_PID],y=pred[:temps_separation_PID-temps_separation-longueur_sequence],line=dict(color='green', width=1)))\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()\n",
        "\n",
        "mse_PID_ent = tf.keras.losses.mse(serie_entrainement_PID_X_norm[longueur_sequence:,-1],pred[:temps_separation_PID-temps_separation-longueur_sequence])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx8dsAhGMxhA"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "decalage = 1\n",
        "pred = []\n",
        "\n",
        "max = len(pred_val)\n",
        "max = max\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_val[i,0:decalage,:],1))\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "\n",
        "# Calcul les valeurs non prédites dans l'intervalle de validation\n",
        "manque_pred = serie_test_X_norm[longueur_sequence:,:].shape[0]-pred.shape[0]\n",
        "\n",
        "# Affiche les valeurs originales de l'intervalle de validation PID\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[temps_separation_PID:-manque_pred],y=serie_test_PID_X_norm[:-manque_pred,-1],line=dict(color='blue', width=1)))\n",
        "\n",
        "# Affiche les prédictions sur l'intervalle de validation PID\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[temps_separation_PID:-manque_pred],y=pred[temps_separation_PID-temps_separation-longueur_sequence:],line=dict(color='red', width=1)))\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()\n",
        "\n",
        "mse_PID_val = tf.keras.losses.mse(serie_test_PID_X_norm[:-manque_pred,-1],pred[temps_separation_PID-temps_separation-longueur_sequence:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tm8qbezaOr8-"
      },
      "source": [
        "print(mse_PID_ent)\n",
        "print(mse_PID_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgFFtQ_SDDmI"
      },
      "source": [
        "# Prédictions naïves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fs9qKFYDJRx"
      },
      "source": [
        "Mettons en place un modèle qui effectue des prédictions naïves sur le NASDAQ :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clV7wF6UDmla"
      },
      "source": [
        "class Net_Naif(tf.keras.layers.Layer):\n",
        "  def __init__(self,longueur_sequence):\n",
        "    self.longueur_sequence = longueur_sequence\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:    Sortie séquence Y   : (batch_size,1,1)\n",
        "  # Sorties :\n",
        "  #     sortie:   Prédiction Y        : (batch_size,1,1)\n",
        "  def call(self,input):\n",
        "    return tf.expand_dims(input[:,-1:,:],-1)                  # (batch_size,1,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69wJ62cFEKzp"
      },
      "source": [
        "def get_model_naif():\n",
        "  sorties_sequence = tf.keras.layers.Input(shape=(longueur_sequence,1))\n",
        "  sortie = Net_Naif(longueur_sequence=longueur_sequence)(sorties_sequence)\n",
        "  model = tf.keras.Model(sorties_sequence,sortie)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rapisky5Ert4"
      },
      "source": [
        "model_naif = get_model_naif()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7KgbADxEq75"
      },
      "source": [
        "pred_ent_naif = model_naif.predict(x_train[1],verbose=1)\n",
        "pred_val_naif = model_naif.predict(x_val[1],verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kT7jLNNCFV9J"
      },
      "source": [
        "model_naif.compile(loss=\"mse\")\n",
        "model_naif.evaluate(x=x_train[1],y=y_train)\n",
        "model_naif.evaluate(x=x_val[1],y=y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uipadQWdFBaK"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "decalage = 1\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Courbes originales\n",
        "fig.add_trace(go.Scatter(x=df_etude.index,y=serie_entrainement_X_norm[:,-1],line=dict(color='blue', width=1)))\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[temps_separation:],y=serie_test_X_norm[:,-1],line=dict(color='red', width=1)))\n",
        "\n",
        "#Affiche les prédictions sur l'entrainement\n",
        "pred = []\n",
        "\n",
        "max = len(pred_ent_naif)\n",
        "max = max\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_ent_naif[i,0:decalage,:],1))\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[longueur_sequence:],y=pred, mode='lines', line=dict(color='green', width=1)))\n",
        "\n",
        "#Affiche les prédictions sur les validations\n",
        "pred = []\n",
        "max = len(pred_val_naif)\n",
        "max = max\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_val_naif[i,0:decalage,:],1))\n",
        "\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[temps_separation+longueur_sequence:],y=pred, mode='lines', line=dict(color='green', width=1)))\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Nnj-8Sm31Ro"
      },
      "source": [
        "**Erreurs en single step**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QirDorgP31Rp"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "decalage = 1\n",
        "pred = []\n",
        "\n",
        "max = len(pred_val_naif)\n",
        "max = max\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_val_naif[i,0:decalage,:],1))\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "\n",
        "# Calcul les valeurs non prédites dans l'intervalle de validation\n",
        "manque_pred = serie_test_X_norm[longueur_sequence:,:].shape[0]-pred.shape[0]\n",
        "\n",
        "# Affiche les valeurs originales de l'intervalle d'entrainement PID\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[temps_separation+longueur_sequence:temps_separation_PID],y=serie_entrainement_PID_X_norm[longueur_sequence:,-1],line=dict(color='blue', width=1)))\n",
        "\n",
        "# Affiche les prédictions sur l'intervalle d'entrainement PID\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[temps_separation+longueur_sequence:temps_separation_PID],y=pred[:temps_separation_PID-temps_separation-longueur_sequence],line=dict(color='green', width=1)))\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()\n",
        "\n",
        "mse_PID_ent_naif = tf.keras.losses.mse(serie_entrainement_PID_X_norm[longueur_sequence:,-1],pred[:temps_separation_PID-temps_separation-longueur_sequence])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bb5ivRqh31Rp"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "decalage = 1\n",
        "pred = []\n",
        "\n",
        "max = len(pred_val_naif)\n",
        "max = max\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_val_naif[i,0:decalage,:],1))\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "\n",
        "# Calcul les valeurs non prédites dans l'intervalle de validation\n",
        "manque_pred = serie_test_X_norm[longueur_sequence:,:].shape[0]-pred.shape[0]\n",
        "\n",
        "# Affiche les valeurs originales de l'intervalle de validation PID\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[temps_separation_PID:-manque_pred],y=serie_test_PID_X_norm[:-manque_pred,-1],line=dict(color='blue', width=1)))\n",
        "\n",
        "# Affiche les prédictions sur l'intervalle de validation PID\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[temps_separation_PID:-manque_pred],y=pred[temps_separation_PID-temps_separation-longueur_sequence:],line=dict(color='red', width=1)))\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()\n",
        "\n",
        "mse_PID_val_naif = tf.keras.losses.mse(serie_test_PID_X_norm[:-manque_pred,-1],pred[temps_separation_PID-temps_separation-longueur_sequence:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nA8NqFUP31Rp"
      },
      "source": [
        "print(mse_PID_ent_naif)\n",
        "print(mse_PID_val_naif)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jl3P7VPpQbb3"
      },
      "source": [
        "# Création du modèle HRHN + CORRECTEUR PID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJtZC2irv_qQ"
      },
      "source": [
        "**1. Couche du correcteur**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbL_Y5KkwCCA"
      },
      "source": [
        "class Correcteur_PID(tf.keras.layers.Layer):\n",
        "  def __init__(self):\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.Couche_Dense = tf.keras.layers.Dense(units=1)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "  \n",
        "  # y_reel :    Valeur cible réelle à (t-1)   :   (batch_size,1,1)\n",
        "  # y_pred :    Prédiction à (t-1)            :   (batch_size,1,1)\n",
        "  def call(self, y_reel,y_pred):\n",
        "    return self.Couche_Dense(y_reel-y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBA02IIm3c1n"
      },
      "source": [
        "**2. Couche du prédicteur**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0pFOiwy3fUE"
      },
      "source": [
        "class Predicteur(tf.keras.layers.Layer):\n",
        "  def __init__(self,predicteur):\n",
        "    self.predicteur = predicteur\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "  \n",
        "  # input:    input[0] :  Séries exogènes     : (batch_size,Tin,#dim)\n",
        "  #           input[1] :  Cible               : (batch_size,Tin,1)\n",
        "  def call(self, input):\n",
        "    return self.predicteur(input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkKn8Dm8xEWl"
      },
      "source": [
        "**2. Création du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnVrkkzdJeWL"
      },
      "source": [
        "class CustomModel(keras.Model):\n",
        "    def __init__(self,predicteur,correcteur):\n",
        "      super(CustomModel, self).__init__()\n",
        "      self.predicteur = predicteur\n",
        "      self.Correcteur = correcteur\n",
        "      self.y_pred_ent_1 = tf.Variable(shape=(1,1,1),trainable=False,initial_value=tf.zeros(shape=(1,1,1)))          # (1, 1)\n",
        "\n",
        "    # inputs:   inputs[0] : Séries exogènes     : (Tin,#dim)\n",
        "    #           inputs[1] : Cible               : (Tin,1)\n",
        "    def call(self, inputs, training=True):\n",
        "      y_pred = self.predicteur([inputs[0],inputs[1]])\n",
        "      y_1 = inputs[1][:,-1:,:]                                # Vraie valeur précédente : (batch_size,1,1)\n",
        "      y_pred_ent_1 = self.y_pred_ent_1\n",
        "      offset = self.Correcteur(y_1,y_pred_ent_1)              # Calcul de l'offset : (batch_size,1,1)\n",
        "      y_pred = y_pred + offset                                # Ajoute l'offset à la prédiction\n",
        "      self.y_pred_ent_1.assign(y_pred)                        # Enregistre la prédiction\n",
        "      return y_pred\n",
        "\n",
        "assert tf.distribute.get_replica_context() is not None  # default"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iE3RIwaAiwoI"
      },
      "source": [
        "model_HRHN = get_model_HRHN()\n",
        "model_HRHN.compile(loss=\"mse\")\n",
        "model_HRHN.load_weights(\"Multi_HRHN_SP500_1.hdf5\")\n",
        "model_HRHN.trainable = False\n",
        "model_HRHN.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFEqDbSM7hTW"
      },
      "source": [
        "with strategy.scope():\n",
        "  predicteur = Predicteur(model_HRHN)\n",
        "  correcteur = Correcteur_PID()\n",
        "  model = CustomModel(predicteur,correcteur)\n",
        "\n",
        "  periodes = 10\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "  loss_fn = keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM)\n",
        "  loss_metric = tf.keras.metrics.Mean()\n",
        "\n",
        "\n",
        "  def train_step():\n",
        "    with tf.GradientTape() as tape:\n",
        "      pred = model([x_train_PID[0][i:i+1,:,:],x_train_PID[1][i:i+1,:,:]])\n",
        "      loss = loss_fn(pred, y_train_PID[i:i+1,:,:])\n",
        "    grads = tape.gradient(loss, model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    return loss\n",
        "\n",
        "  @tf.function\n",
        "  def distributed_train_step():\n",
        "#    assert tf.distribute.get_replica_context() is None\n",
        "    per_replica_losses = strategy.run(train_step)\n",
        "    return per_replica_losses\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5QAcNZORamI"
      },
      "source": [
        "for periode in range(periodes):\n",
        "  print(periode)\n",
        "  for i in range(x_train_PID[0].shape[0]):\n",
        "    loss = distributed_train_step()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzoa-qq_Omuw"
      },
      "source": [
        "strategy = tf.distribute.TPUStrategy(resolver)\n",
        "strategy.run(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZWHVAO0auHZ"
      },
      "source": [
        "# Entrainement avec TPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liyiRZvRa8As"
      },
      "source": [
        "x_train_PID[1][:,-1:,:].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkAWNR0jq-90"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "max_periodes = 100\n",
        "\n",
        "strategy = tf.distribute.TPUStrategy(resolver)\n",
        "with strategy.scope():\n",
        "  # Création du modèle\n",
        "  model_HRHN = get_model_HRHN()\n",
        "  model_HRHN.compile(loss=\"mse\")\n",
        "  model_HRHN.load_weights(\"Multi_HRHN_SP500_1.hdf5\")\n",
        "  model_HRHN.trainable = False\n",
        "\n",
        "  cor_erreur = tf.keras.Sequential(\n",
        "      [\n",
        "      tf.keras.layers.InputLayer(input_shape=(1,1)),\n",
        "      tf.keras.layers.Dense(units=1,trainable=True),\n",
        "      ],\n",
        "      name=\"cor_erreur\"\n",
        "  )\n",
        "  entree = tf.keras.layers.Input(input_shape=(longueur_sequence,))\n",
        "\n",
        "\n",
        "  model = CustomModel(model_HRHN,cor_erreur,batch_size)\n",
        "\n",
        "  # Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "  lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "      initial_learning_rate=0.001,\n",
        "      decay_steps=20,\n",
        "      decay_rate=0.01)\n",
        "\n",
        "  optimiseur=tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "  # Utilisation de la méthode ModelCheckPoint\n",
        "  CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_train.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "  # Compile le modèle\n",
        "  model.compile(loss=\"mse\", optimizer=optimiseur)\n",
        "\n",
        "  # Entraine le modèle\n",
        "  historique = model.fit(x=[x_train_PID[0],x_train_PID[1]],y=y_train_PID,validation_data=([x_PID_val[0],x_PID_val[1]],y_PID_val), epochs=max_periodes,verbose=1, callbacks=[CheckPoint,tf.keras.callbacks.EarlyStopping(monitor='loss', patience=100)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJfv3rZfryIO"
      },
      "source": [
        "model.load_weights(\"poids_train.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkK74TexF-Ou"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvQ6UIyG5_Zy"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement[40:100])),erreur_entrainement[40:100], label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement[40:100])),erreur_validation[40:100], label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGI1__8KseSt"
      },
      "source": [
        "# Prédictions single-step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSK4gnb0s-mf"
      },
      "source": [
        "pred_ent_PID = model.predict([x_train[0],x_train[1]],verbose=1)\n",
        "pred_val_PID = model.predict([x_val[0],x_val[1]],verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nBhShniseSu"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "decalage = 1\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Courbes originales\n",
        "fig.add_trace(go.Scatter(x=df_etude.index,y=serie_entrainement_X_norm[:,-1],line=dict(color='blue', width=1)))\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[temps_separation:],y=serie_test_X_norm[:,-1],line=dict(color='red', width=1)))\n",
        "\n",
        "#Affiche les prédictions sur l'entrainement\n",
        "pred = []\n",
        "\n",
        "max = len(pred_ent_PID)\n",
        "max = max\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_ent_PID[i,0:decalage,:],1))\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[longueur_sequence:],y=pred, mode='lines', line=dict(color='green', width=1)))\n",
        "\n",
        "#Affiche les prédictions sur les validations\n",
        "pred = []\n",
        "max = len(pred_val_PID)\n",
        "max = max\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_val_PID[i,0:decalage,:],1))\n",
        "\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[temps_separation+longueur_sequence:],y=pred, mode='lines', line=dict(color='green', width=1)))\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGCwJr2mseSv"
      },
      "source": [
        "**Zone d'entrainement du modèle HRHN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlaBn7sqseSv"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "decalage = 1\n",
        "pred = []\n",
        "\n",
        "max = len(pred_ent_PID)\n",
        "max = max\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_ent_PID[i,0:decalage,:],1))\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[longueur_sequence:],y=serie_entrainement_X_norm[longueur_sequence:-(serie_entrainement_X_norm[longueur_sequence:,:].shape[0]-pred.shape[0]),-1],line=dict(color='blue', width=1)))\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[longueur_sequence:],y=pred,line=dict(color='green', width=1)))\n",
        "\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6FB3NTKseSw"
      },
      "source": [
        "**Zone de validation du modèle HRHN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GL4JG6NcseSx"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "decalage = 1\n",
        "pred = []\n",
        "\n",
        "max = len(pred_val_PID)\n",
        "max = max\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_val_PID[i,0:decalage,:],1))\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "\n",
        "# Calcul les valeurs non prédites dans l'intervalle de validation\n",
        "manque_pred = serie_test_X_norm[longueur_sequence:,:].shape[0]-pred.shape[0]\n",
        "\n",
        "# Affiche les valeurs originales de l'intervalle de validation\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[temps_separation+longueur_sequence:-manque_pred:],y=serie_test_X_norm[longueur_sequence:-manque_pred,-1],line=dict(color='blue', width=1)))\n",
        "\n",
        "# Affiche les prédictions sur l'intervalle d'entrainement PID\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[temps_separation+longueur_sequence:temps_separation_PID],y=pred[:temps_separation_PID-temps_separation-longueur_sequence],line=dict(color='green', width=1)))\n",
        "\n",
        "# Affiche les prédictions sur l'intervalle de validation PID\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[temps_separation_PID:-manque_pred],y=pred[temps_separation_PID-temps_separation-longueur_sequence:],line=dict(color='red', width=1)))\n",
        "\n",
        "\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSK7juEUseSx"
      },
      "source": [
        "**Erreurs sur les zones PID**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Gh4TjxEseSx"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "decalage = 1\n",
        "pred = []\n",
        "\n",
        "max = len(pred_val_PID)\n",
        "max = max\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_val_PID[i,0:decalage,:],1))\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "\n",
        "# Calcul les valeurs non prédites dans l'intervalle de validation\n",
        "manque_pred = serie_test_X_norm[longueur_sequence:,:].shape[0]-pred.shape[0]\n",
        "\n",
        "# Affiche les valeurs originales de l'intervalle d'entrainement PID\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[temps_separation+longueur_sequence:temps_separation_PID],y=serie_entrainement_PID_X_norm[longueur_sequence:,-1],line=dict(color='blue', width=1)))\n",
        "\n",
        "# Affiche les prédictions sur l'intervalle d'entrainement PID\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[temps_separation+longueur_sequence:temps_separation_PID],y=pred[:temps_separation_PID-temps_separation-longueur_sequence],line=dict(color='green', width=1)))\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()\n",
        "\n",
        "mse_PID_ent = tf.keras.losses.mse(serie_entrainement_PID_X_norm[longueur_sequence:,-1],pred[:temps_separation_PID-temps_separation-longueur_sequence])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgG0ogotseSy"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "decalage = 1\n",
        "pred = []\n",
        "\n",
        "max = len(pred_val_PID)\n",
        "max = max\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_val_PID[i,0:decalage,:],1))\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "\n",
        "# Calcul les valeurs non prédites dans l'intervalle de validation\n",
        "manque_pred = serie_test_X_norm[longueur_sequence:,:].shape[0]-pred.shape[0]\n",
        "\n",
        "# Affiche les valeurs originales de l'intervalle de validation PID\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[temps_separation_PID:-manque_pred],y=serie_test_PID_X_norm[:-manque_pred,-1],line=dict(color='blue', width=1)))\n",
        "\n",
        "# Affiche les prédictions sur l'intervalle de validation PID\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[temps_separation_PID:-manque_pred],y=pred[temps_separation_PID-temps_separation-longueur_sequence:],line=dict(color='red', width=1)))\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()\n",
        "\n",
        "mse_PID_val = tf.keras.losses.mse(serie_test_PID_X_norm[:-manque_pred,-1],pred[temps_separation_PID-temps_separation-longueur_sequence:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLUQ52cqseSy"
      },
      "source": [
        "print(mse_PID_ent)\n",
        "print(mse_PID_val)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
